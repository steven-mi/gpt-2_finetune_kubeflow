{"id": "1", "url": "https://de.wikipedia.org/wiki?curid=1", "title": "Alan Smithee", "text": "Alan Smithee\n\nAlan Smithee steht als Pseudonym für einen fiktiven Regisseur, der Filme verantwortet, bei denen der eigentliche Regisseur seinen Namen nicht mit dem Werk in Verbindung gebracht haben möchte. Von 1968 bis 2000 wurde es von der Directors Guild of America (DGA) für solche Situationen empfohlen, seither ist es Thomas Lee. \"Alan Smithee\" ist jedoch weiterhin in Gebrauch.\n\nAlternative Schreibweisen sind unter anderem die Ursprungsvariante \"Allen Smithee\" sowie \"Alan Smythee\" und \"Adam Smithee\". Auch zwei teilweise asiatisch anmutende Schreibweisen \"Alan Smi Thee\" und \"Sumishii Aran\" gehören – so die Internet Movie Database – dazu.\n\n\nGeschichte.\n\nEntstehung.\nDas Pseudonym entstand 1968 infolge der Arbeiten am Western-Film \"Death of a Gunfighter\" (deutscher Titel \"Frank Patch – Deine Stunden sind gezählt\"). Regisseur Robert Totten und Hauptdarsteller Richard Widmark gerieten in einen Streit, woraufhin Don Siegel als neuer Regisseur eingesetzt wurde.\n\nDer Film trug nach Abschluss der Arbeiten noch deutlich Tottens Handschrift, der auch mehr Drehtage als Siegel daran gearbeitet hatte, weshalb dieser die Nennung seines Namens als Regisseur ablehnte. Totten selbst lehnte aber ebenfalls ab. Als Lösung wurde \"Allen Smithee\" als ein möglichst einzigartiger Name gewählt.\n\nIn den zeitgenössischen Kritiken wurde der Regisseur u.&nbsp;a. von Roger Ebert mit den Worten gelobt: \n\n\nAufdeckung und Abkehr.\n1997 kam die Parodie \"An Alan Smithee Film: Burn Hollywood Burn\" (deutscher Titel \"Fahr zur Hölle Hollywood\") in die Kinos, was das Pseudonym einem größeren Publikum bekannt machte, nicht zuletzt weil Arthur Hiller, der eigentliche Regisseur des Films, selbst seinen Namen zurückzog und analog zum Filmtitel das Pseudonym \"Alan Smithee\" benutzte. Der Film gilt als einer der schlechtesten Filme der 1990er Jahre und gewann fünf Goldene Himbeeren.\n\nDer Film \"Supernova\" ist der erste Post-Smithee-Film, dort führte ein gewisser \"Thomas Lee\" alias Walter Hill die Regie.\n\nVerwendung.\nDie Verwendung dieses oder eines anderen Pseudonyms ist für Mitglieder der DGA streng reglementiert. Ein Regisseur, der für einen von ihm gedrehten Film seinen Namen nicht hergeben möchte, hat nach Sichtung des fertigen Films drei Tage Zeit, anzuzeigen, dass er ein Pseudonym verwenden möchte. Der Rat der DGA entscheidet binnen zwei Tagen über das Anliegen. Erhebt die Produktionsfirma Einspruch, entscheidet ein Komitee aus Mitgliedern der DGA und der Vereinigung der Film- und Fernsehproduzenten, ob der Regisseur ein Pseudonym angeben darf. Über die Beantragung muss der Regisseur Stillschweigen halten, ebenso darf er den fertigen Film nicht öffentlich kritisieren, wenn die DGA ihm die Verwendung eines Pseudonyms zugesteht. Ein Antrag des Regisseurs auf Pseudonymisierung kann abgelehnt werden, so durfte Tony Kaye den Namen Smithee bei dem Film \"American History X\" nicht einsetzen, obwohl er den Antrag stellte.\n\nAuch bei nicht-US-amerikanischen Produktionen wird der Name verwendet, wie etwa beim Pilotfilm der Fernsehserie \"Schulmädchen\". 2007 sendete die ARD am 8. und 9. August den zweiteiligen TV-Film \"Paparazzo\". Auch in diesem Werk erscheint anstatt des eigentlichen Regisseurs Stephan Wagner Alan Smithee im Abspann.\n\nRegisseure, die das Pseudonym benutzt haben:\n\nDer Pilotfilm der Serie \"MacGyver\" und die fünfte Folge der ersten Staffel führen einen Alan Smithee als Regisseur. Auf der TV-Serien-Seite \"TV Rage\" wird Jerrold Freedman als Regisseur des Pilotfilms angegeben. Der Regisseur der fünften Folge ist unbekannt.\n\nZu den Drehbuchautoren, die das Pseudonym benutzt haben, gehören Sam Raimi und Ivan Raimi, die das Drehbuch zu \"Die total beknackte Nuß\" als \"Alan Smithee, Jr.\" und \"Alan Smithee, Sr.\" schrieben.\n\nAuch in Computerspielen wird dieses Pseudonym angegeben: Im Abspann des Ego-Shooters \"Marine Sharpshooter IV\" aus dem Jahr 2008 wird als Art Director des Spiels \"Alan Smithee\" genannt.\n\n2014 produzierte die New Yorker Performance-Kompanie Big Dance Theater \"Alan Smithee Directed this Play\", das im August des Jahres auch in Berlin bei Tanz im August aufgeführt wurde.\n\n\n"}
{"id": "3", "url": "https://de.wikipedia.org/wiki?curid=3", "title": "Actinium", "text": "Actinium\n\nActinium ist ein radioaktives chemisches Element mit dem Elementsymbol Ac und der Ordnungszahl 89. Im Periodensystem der Elemente steht es in der 3.&nbsp;IUPAC-Gruppe, der Scandiumgruppe. Das Element ist ein Metall und gehört zur 7.&nbsp;Periode, d-Block. Es ist der Namensgeber der Gruppe der Actinoide, der ihm folgenden 14 Elemente.\n\n\nGeschichte.\nDas Actinium wurde im Jahr 1899 von dem französischen Chemiker André-Louis Debierne entdeckt, der es aus Pechblende isolierte und ihm zunächst Ähnlichkeiten mit dem Titan oder dem Thorium zuschrieb; seine Bezeichnung leitete er wegen der Radioaktivität von griechisch ἀκτίς \"aktís\" ‚Strahl‘ ab. Friedrich Giesel entdeckte das Element unabhängig davon im Jahr 1902 und beschrieb eine Ähnlichkeit zum Lanthan; er gab ihm den Namen Emanium, eine Bildung zu lateinisch \"emano\" ‚ausfließen‘, ebenfalls mit Bezug zur abgegebenen Strahlung. Nachdem Actinium und Emanium im Jahre 1904 als identisch erkannt worden waren, wurde Debiernes Namensgebung der Vorzug gegeben, da er es zuerst entdeckt hatte.\n\nDie Geschichte der Entdeckung wurde in Publikationen von 1971 und später im Jahr 2000 immer noch als fraglich beschrieben. Sie zeigen, dass die Publikationen von 1904 einerseits und die von 1899 und 1900 andererseits Widersprüche aufweisen.\n\n\nGewinnung und Darstellung.\nDa in Uranerzen nur wenig Actinium vorhanden ist, spielt diese Quelle keine Rolle für die Gewinnung. Technisch wird das Isotop Ac durch Bestrahlung von Ra mit Neutronen in Kernreaktoren hergestellt.\n\nDurch den schnellen Zerfall des Actiniums waren stets nur geringe Mengen verfügbar. Die erste künstliche Herstellung von Actinium wurde im Argonne National Laboratory in Chicago durchgeführt.\n\n\nEigenschaften.\n\nPhysikalische Eigenschaften.\nDas Metall ist silberweiß glänzend und relativ weich. Aufgrund seiner starken Radioaktivität leuchtet Actinium im Dunkeln in einem hellblauen Licht.\n\nActinium ist das namensgebende Element der Actinoiden, ähnlich wie Lanthan für die Lanthanoiden. Die Gruppe der Elemente zeigt deutlichere Unterschiede als die Lanthanoide; daher dauerte es bis 1945, bis Glenn T. Seaborg die wichtigsten Änderungen zum Periodensystem von Mendelejew vorschlagen konnte: die Einführung der Actinoide.\n\n\nChemische Eigenschaften.\nEs ist sehr reaktionsfähig und wird von Luft und Wasser angegriffen, überzieht sich aber mit einer Schicht von Actiniumoxid, wodurch es vor weiterer Oxidation geschützt ist. Das Ac-Ion ist farblos. Das chemische Verhalten von Actinium ähnelt sehr dem Lanthan. Actinium ist in allen zehn bekannten Verbindungen dreiwertig.\n\n\nIsotope.\nBekannt sind 26 Isotope, wovon nur zwei natürlich vorkommen. Das langlebigste Isotop Ac (Halbwertszeit 21,8&nbsp;Jahre) hat zwei Zerfallskanäle: es ist ein Alpha- und Betastrahler. Ac ist ein Zerfallsprodukt des Uranisotops U und kommt zu einem kleinen Teil in Uranerzen vor. Daraus lassen sich wägbare Mengen Ac gewinnen, die somit ein verhältnismäßig einfaches Studium dieses Elementes ermöglichen. Da sich unter den radioaktiven Zerfallsprodukten einige Gammastrahler befinden, sind aber aufwändige Strahlenschutzvorkehrungen nötig.\n\n\nVerwendung.\nActinium wird zur Erzeugung von Neutronen eingesetzt, die bei Aktivierungsanalysen eine Rolle spielen. Außerdem wird es für die thermoionische Energieumwandlung genutzt.\n\nBeim dualen Zerfall des Ac geht der größte Teil unter Emission von Beta-Teilchen in das Thoriumisotop Th, aber ca. 1 % zerfällt durch Alpha-Emission zu Francium Fr. Eine Lösung von Ac ist daher als Quelle für das kurzlebige Fr verwendbar. Letzteres kann dann regelmäßig abgetrennt und untersucht werden.\n\n\nSicherheitshinweise.\nEinstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.\n\n\nVerbindungen.\nNur eine geringe Anzahl von Actiniumverbindungen ist bekannt. Mit Ausnahme von AcPO sind sie alle den entsprechenden Lanthanverbindungen ähnlich und enthalten Actinium in der Oxidationsstufe +3. Insbesondere unterscheiden sich die Gitterkonstanten der jeweiligen Lanthan- und Actinium-Verbindungen nur in wenigen Prozent.\n\n\nOxide.\nActinium(III)-oxid&nbsp;(AcO) kann durch Erhitzen des Hydroxids bei 500&nbsp;°C oder des Oxalats bei 1100&nbsp;°C im Vakuum erhalten werden. Das Kristallgitter ist isotyp mit den Oxiden der meisten dreiwertigen Seltenerdmetalle.\n\n\nHalogenide.\nActinium(III)-fluorid&nbsp;(AcF) kann entweder in Lösung oder durch Feststoffreaktion dargestellt werden. Im ersten Fall gibt man bei Raumtemperatur Flusssäure zu einer Ac-Lösung und fällt das Produkt aus. im anderen Fall wird Actinium-Metall mit Fluorwasserstoff bei 700&nbsp;°C in einer Platinapparatur behandelt.\n\nActinium(III)-chlorid&nbsp;(AcCl) wird durch Umsetzung von Actiniumhydroxid oder -oxalat mit Tetrachlormethan bei Temperaturen oberhalb von 960&nbsp;°C erhalten.\n\nDie Reaktion von Aluminiumbromid und Actinium(III)-oxid führt zum Actinium(III)-bromid&nbsp;(AcBr) und Behandlung mit feuchtem Ammoniak bei 500&nbsp;°C führt zum Oxibromid AcOBr.\n\n\nWeitere Verbindungen.\nGibt man Natriumdihydrogenphosphat&nbsp;(NaHPO) zu einer Lösung von Actinium in Salzsäure, erhält man weiß gefärbtes Actiniumphosphat&nbsp;(AcPO&nbsp;·&nbsp;0,5&nbsp;HO); ein Erhitzen von Actinium(III)-oxalat mit Schwefelwasserstoff bei 1400&nbsp;°C für ein paar Minuten führt zu schwarzem Actinium(III)-sulfid&nbsp;(AcS).\n\n\n"}
{"id": "5", "url": "https://de.wikipedia.org/wiki?curid=5", "title": "Ang Lee", "text": "Ang Lee\n\nAng Lee (; * 23. Oktober 1954 in Chaozhou, Landkreis Pingtung, Republik China (Taiwan)) ist ein taiwanischer Filmregisseur, Drehbuchautor und Produzent. Er ist als vielfach ausgezeichneter Regisseur bekannt für so unterschiedliche Filme wie \"Eat Drink Man Woman\", die Jane-Austen-Adaption \"Sinn und Sinnlichkeit\" und den Martial Arts-Film \"Tiger and Dragon\". Für seine Filme \"Brokeback Mountain\" (2005) und \"\" (2012) wurde er jeweils mit dem Oscar in der Kategorie \"Beste Regie\" ausgezeichnet.\n\n\nLeben.\nAng Lee wurde 1954 in Taiwan geboren. Seine Eltern, Emigranten aus China, lernten sich in Taiwan kennen, Lee ist ihr ältester Sohn. Die Großeltern väterlicher- und mütterlicherseits sind im Zuge der kommunistischen Revolution in China ums Leben gekommen. Da sein Vater als Lehrer häufiger die Arbeitsstelle wechselte, wuchs Ang Lee in verschiedenen Städten Taiwans auf.\n\nEntgegen den Wünschen seiner Eltern, wie sein Vater eine klassische akademische Laufbahn einzuschlagen, interessierte sich Lee für das Schauspiel und absolvierte mit ihrem Einverständnis zunächst ein Theater- und Filmstudium in Taipeh. Im Anschluss daran ging er 1978 in die USA, um an der Universität von Illinois in Urbana-Champaign Theaterwissenschaft und -regie zu studieren. Nach dem Erwerb seines B.A. in Illinois verlegte er sich ganz auf das Studium der Film- und Theaterproduktion an der Universität von New York, das er 1985 mit einem Master abschloss. Danach entschloss er sich, mit seiner ebenfalls aus Taiwan stammenden Ehefrau zusammen in den USA zu bleiben.\n\nSein Interesse verschob sich trotz erster Erfahrungen mit dem Super-8-Film in Taiwan erst spät ganz auf Filmregie und -produktion – auch weil Lee seinen Berufswunsch seiner Familie und insbesondere seinem Vater gegenüber lange Zeit nicht eingestehen wollte.\nNach dem Studium konnte er zunächst keine eigenen Projekte umsetzen. Erst ab 1992, als er seinen ersten Langfilm fertigstellte, zeichnete sich eine kontinuierliche Karriere als Regisseur ab.\n\nAls seine bisher größte Erfolge – sowohl beim Publikum als auch bei der Kritik – gelten das Martial Arts-Drama \"Tiger and Dragon\" mit einer pan-asiatischen Starbesetzung und der Post-Western-Liebesfilm \"Brokeback Mountain\" mit Heath Ledger und Jake Gyllenhaal. Für Letzteren bekam Lee 2006 als erster asiatisch-stämmiger und nicht-weißer Regisseur den Oscar für die beste Regie. Außerdem wurden Lees Filme, neben vielen weiteren Preisen, mit mittlerweile zwei Goldenen Bären der Berlinale und zwei Goldenen Löwen der Filmfestspiele von Venedig ausgezeichnet.\n\nLee ist seit 1983 mit der Mikrobiologin Jane Lin verheiratet. Sie leben in White Plains, Westchester County, im Bundesstaat New York. Aus der Ehe stammen die Söhne Haan (* 1984) und Mason (* 1990). Ang Lee besitzt eine United States Permanent Resident Card.\n\n\nFilmisches Werk.\nNach seinen ersten Filmerfahrungen in Taiwan setzte sich Lee erst wieder während seines Studiums in den USA ernsthaft mit dem Filmemachen auseinander. Im Rahmen seines Studiums in New York drehte er einige Kurzfilme und wirkte unter anderem beim Abschlussdreh seines Studienkollegen Spike Lee als Regieassistent mit. Sein eigener Abschlussfilm \"Fine Line\" gewann 1985 zwei Preise beim renommierten Filmfest seiner Universität. Erst 1992 gelang es ihm, nach dem Gewinn eines hochdotierten Drehbuchwettbewerbs in Taiwan, den ersten einer Reihe von drei Filmen zu drehen, die west-östliche Konflikte taiwanischer Familien zum Thema haben.\n\n\n1992–1994: Die „Father-Knows-Best“-Trilogie.\nDiese ersten drei Langfilme, die Lee realisieren konnte, werden im Allgemeinen unter dem Begriff \"Father Knows Best\" gefasst. Diese Bezeichnung geht auf die wiederkehrende Figur des chinesischen Familienoberhaupts, gespielt jeweils vom taiwanischen Schauspieler Sihung Lung, zurück. Die drei Filme thematisieren, wie später noch öfter bei Ang Lee, familiäre Probleme, die aus dem Konflikt zwischen Selbstbestimmung und Tradition, zwischen Innen und Außen, zwischen Ost und West sowie zwischen den Generationen herrühren. Die Filme sind allesamt US-amerikanisch-taiwanische Koproduktionen. Anders als bei allen bislang folgenden Projekten handelt es sich bei den ersten Filmen Lees nicht um Adaptionen, sondern um Filme nach von ihm selbst geschriebenen Originaldrehbüchern.\n\nDer erste Film, \"Schiebende Hände\" (1992), handelt vom Einzug eines chinesischen Vaters bei seinem erwachsenen Sohn und der US-amerikanischen Schwiegertochter in New York und den interkulturellen Problemen, die in der neuen Wohngemeinschaft entstehen. Dies war die erste Zusammenarbeit zwischen Lee und dem Drehbuchautor und Produzenten James Schamus – seitdem bildeten die beiden bei jedem Film Lees eine enge Arbeitsgemeinschaft. Wie in den beiden folgenden Filmen schrieben sie auch gemeinsam das Drehbuch. In allen weiteren Filmen Lees (mit Ausnahme des Kurzfilms \"The Hire: Chosen\") hat Schamus seither entscheidende Funktionen ausgeübt.\n\nAuch die regelmäßige Zusammenarbeit mit dem Filmeditor Tim Squyres nahm in Lees Erstling ihren Anfang. Mit Ausnahme des Erfolgsfilms \"Brokeback Mountain\" von 2005 hat Squires jeden Film, den Ang Lee gedreht hat, geschnitten.\n\nNach dem Erfolg seines Erstlings konnte Lee als Nächstes \"Das Hochzeitsbankett\" (1993) drehen, eine Komödie über die fingierte Eheschließung eines homosexuellen Exil-Taiwaners in den USA. Erneut taucht hier die Figur des strengen, aber weisen Familienoberhaupts auf. Hatte \"Schiebende Hände\" zunächst vor allem in Taiwan für Aufmerksamkeit (und Preise) gesorgt, wurde mit dem zweiten Langfilm Lees auch Europa auf den aufstrebenden Regisseur aufmerksam: Der Film erhielt bei der Berlinale 1993 den \"Goldenen Bären\" als \"Bester fremdsprachiger Film\" und war zudem für einen Oscar nominiert. Er gilt darüber hinaus als einer der profitabelsten Low-Budget-Filme des Jahres 1993. Mit nur einer Million US-Dollar Produktionskosten erzielte er ein Einspielergebnis von über 23&nbsp;Millionen US-Dollar.\n\nSihung Lung ist auch im letzten Teil der Trilogie, \"Eat Drink Man Woman\" (1994), die „kongeniale Verkörperung des chinesischen Familienoberhaupts“, das „Zentrum dieser Maskeraden, in denen es darum geht, ein altes Gesicht zu wahren und dann zu lernen, es zu verlieren, um ein neues, lebenstauglicheres zu gewinnen.“ Dieses Mal ist er der verwitwete Vater dreier Töchter, die ihr Leben und ihre Lieben auf unterschiedliche Art angehen und dabei ebenfalls innerfamiliäre Konflikte klären müssen. \"Eat Drink Man Woman\" wurde, anders als seine Vorgänger, in Taipeh gedreht. Im Mittelpunkt des Films stehen (der Titel deutet es an) die Liebe und das Essen. Ang Lee, privat ein passionierter Koch, legte hierbei besonders großen Wert auf die kulinarische Komponente als Stilmittel und konzipierte die Hauptfigur des älteren Witwers als berühmten Koch.\n\n\n1995–1999: Dreimal anglo-amerikanische Geschichte.\nMit dem Angebot der Produzentin Lindsay Doran, die von der britischen Schauspielerin Emma Thompson verfasste Adaption des Romans \"Verstand und Gefühl\" von Jane Austen in Großbritannien zu drehen, eröffnete sich Lee eine lange ersehnte neue Perspektive jenseits asiatisch geprägter Stoffe.\n\nIn einer neuen Trilogie setzt er sich mit unterschiedlichen Kulturen auseinander:\n\n\n2000–heute: Pendeln zwischen West und Ost.\n\"Tiger and Dragon\" sowie \"Hulk\" sind sehr unterschiedliche Action-Filme. Mit \"Tiger and Dragon\" gewann Lee zwei Golden Globes. Das Werk wurde außerdem mit vier Academy Awards (Oscars) prämiert, darunter der Trophäe für den besten fremdsprachigen Film. Für diesen Film wurde er 2001 auch mit einem Chlotrudis Award ausgezeichnet, seinen zweiten Chlotrudis erhielt er 2006 für \"Brokeback Mountain\".\n\nFür \"Brokeback Mountain\" wurde Lee mit einer Vielzahl von Filmpreisen geehrt, darunter mit dem Oscar für die beste Regie, dem Goldene Löwen der Filmfestspiele von Venedig sowie der Auszeichnung der Hollywood Foreign Press Association als bester Regisseur des Jahres. 2007 verfilmte er mit \"Gefahr und Begierde\" eine Kurzgeschichte von Eileen Chang. Der Thriller spielt zur Zeit des Zweiten Weltkriegs in Shanghai und handelt von einer jungen chinesischen Agentin (gespielt von Tang Wei), die beauftragt wird, einen hochrangigen Verräter (Tony Leung Chiu Wai) zu liquidieren. Lees erste chinesischsprachige Spielfilmproduktion seit \"Tiger and Dragon\" war 2007 im offiziellen Wettbewerb der 64.&nbsp;Filmfestspiele von Venedig vertreten und brachte ihm erneut den Goldenen Löwen ein. Im selben Jahr wurde \"Gefahr und Begierde\" als offizieller taiwanischer Beitrag für die Nominierung um den besten fremdsprachigen Film bei der Oscar-Verleihung 2008 ausgewählt, später aber auf Empfehlung der Academy of Motion Picture Arts and Sciences wieder zurückgezogen und durch Chen Huai-Ens \"Lian xi qu\" ersetzt.\n\nEnde Februar 2009 wurde bekannt gegeben, dass Lee die Jury der 66.&nbsp;Filmfestspiele von Venedig leiten werde. Zwei Monate später erhielt er für seine Komödie \"Taking Woodstock\" eine Einladung in den Wettbewerb der 62.&nbsp;Internationalen Filmfestspiele von Cannes.\n\n2013 wurde er in die Wettbewerbsjury des 66.&nbsp;Filmfestivals von Cannes berufen.\n\n\nStil.\nAng Lee ist ein international anerkannter und erfolgreicher Regisseur und gilt als einer der vielseitigsten Filmemacher der letzten Jahre. Häufig behandelt Lee in seinen Filmen das Thema Familie auf eine Art und Weise, die autobiographische Züge seines eigenen Lebens trägt. Er lässt seine Umgebung ganz bewusst auf sich einwirken und bringt diese in seine Filme ein.\n\nKennzeichnend für die meisten seiner Filme ist eine wenig geradlinige Erzählstruktur, die die Charaktere und die Geschichte aus verschiedenen Blickwinkeln darstellt. Er verknüpft die Konflikte des menschlichen Lebens mit traditionellen und innovativen Stilelementen.\n\nFür Ang Lee sind die klassisch-soliden Erzählstrukturen zu langweilig, daher kombiniert er verschiedene Genres und Epochen. Er selbst sagte einmal:\n\n\n\n\n\n\n\n\n"}
{"id": "7", "url": "https://de.wikipedia.org/wiki?curid=7", "title": "Anschluss (Luhmann)", "text": "Anschluss (Luhmann)\n\nAnschluss ist in der Soziologie ein Fachbegriff aus der Systemtheorie von Niklas Luhmann und bezeichnet die in einer sozialen Begegnung auf eine Selektion der anderen Seite folgende, selbst gewählte Selektion. Diese Selektionen beziehen sich aufeinander.\n\nDie Anschlussfähigkeit ist die Kapazität von Systemen zu gewährleisten, dass sich an die Selektionen eines Systems weitere anschließen können. Alle sozialen Systeme reproduzieren sich über Kommunikation (z.&nbsp;B. Wirtschaftssystem oder Politik) oder Handlungen (Medizin und Erziehungssystem). Dies gelingt nur, wenn die einzelnen Einheiten aneinander anschlussfähig sind, was durch einen systemspezifischen Code geleistet wird, der als zentrale Logik (Leitunterscheidung) aller Kommunikation zugrunde liegt und sie als systemzugehörig erkennbar macht. Im Wirtschaftssystem beispielsweise sorgt der Code \"zahlen/nicht zahlen\" dafür, dass die Kommunikationen sich auf sich selbst beziehen und sich selbst reproduzieren können, also dass auf jede Zahlung eine neue erfolgt. Dies funktioniert über das generalisierte Kommunikationsmedium Geld, das die letzte Zahlung mit der jetzigen verknüpft. Würde das Geld nicht mehr akzeptiert, folgt der Zahlung keine weitere Zahlung mehr und das System hätte seine Anschlussfähigkeit verloren. Die Anschlussfähigkeit innerhalb eines Systems wird als Selbstreferenz bezeichnet, im Gegensatz zum fremdreferentiellen Bezug auf die Umwelt (Welt, andere Systeme).\n\nDen Begriff hat Luhmann auf eine Anregung eines Bielefelder Kollegen, des Philosophen Jürgen Frese entwickelt. Frese zeigte in einem Sektionsreferat des Achten Deutschen Kongresses für Philosophie in Heidelberg (1966, gedruckt 1967) mit dem Titel „Sprechen als Metapher für Handeln“, dass es fruchtbar ist, von den dominanten Handlungsmodellen Arbeit und Konsum abzurücken und ergänzend Sprechen als Modell für Handeln zu nutzen. Frese schreibt: „Die wichtigste Errungenschaft, die die Sprachmetapher für die Aufhellung des nicht-sprachlichen Handelns einbringt, ist ihre Leistung, Reihenbildung erklärbar zu machen. Fassen wir Satz und Handlung zum neutralen und an andere Philosopheme anschließbaren Begriff des Aktes zusammen, so können wir ... sagen: Der Sinn eines Aktes ist das als eine bestimmte Situation gegebene Ensemble der Möglichkeiten, an diesen Akt weitere Akte anzuschließen; d.&nbsp;h. der Sinn eines Aktes ist die Mannigfaltigkeit der Anschließbarkeiten, die er eröffnet.“ Diese Idee wurde von Luhmann aufgegriffen und im Rahmen seiner Systemtheorie weiterentwickelt. Frese selbst baute sie im Rahmen seiner Lehre von den Formularen weiter aus.\n\n"}
{"id": "10", "url": "https://de.wikipedia.org/wiki?curid=10", "title": "Aussagenlogik", "text": "Aussagenlogik\n\nDie Aussagenlogik ist ein Teilgebiet der Logik, das sich mit Aussagen und deren Verknüpfung durch Junktoren befasst, ausgehend von strukturlosen Elementaraussagen (Atomen), denen ein Wahrheitswert zugeordnet wird. In der \"klassischen Aussagenlogik\" wird jeder Aussage genau einer der zwei Wahrheitswerte „wahr“ und „falsch“ zugeordnet. Der Wahrheitswert einer zusammengesetzten Aussage lässt sich ohne zusätzliche Informationen aus den Wahrheitswerten ihrer Teilaussagen bestimmen.\n\n\nGeschichte.\nHistorisch geht die Aussagenlogik zurück bis zu Aristoteles, der erstmals aussagenlogische Grundsätze diskutierte, nämlich in seiner Metaphysik den Satz vom Widerspruch und den Satz vom ausgeschlossenen Dritten, und der in seiner ersten Analytik den indirekten Beweis thematisierte. Die zweiwertige aussagenlogische Semantik entwickelten etwas später die megarischen Philosophen Diodoros Kronos und Philon. Die Aussagensemantik und -axiomatik kombinierte der Stoiker Chrysippos von Soli, der den ersten aussagenlogischen Kalkül formulierte. Die Weiterentwicklung der Aussagenlogik der Stoa durch das Mittelalter wird oft übersehen. Eine erste vollständige und entscheidbare Formalisierung für aussagenlogische Tautologien –&nbsp;allerdings noch nicht für das aussagenlogische Schließen&nbsp;– schuf George Boole 1847 mit seinem algebraischen Logikkalkül. Den ersten aussagenlogischen Kalkül mit Schlussregeln formulierte Gottlob Frege im Rahmen seiner Begriffsschrift 1879. Er war die Vorlage für den Aussagenkalkül von Bertrand Russell 1908, der sich später durchsetzte (s.&nbsp;u.).\n\nAbgrenzung zu anderen Logiken.\nDa in der heutigen Mathematik die klassische Aussagenlogik maßgeblich wurde, wird in diesem Artikel dieser moderne Haupttypus der Aussagenlogik behandelt. Allgemein ist die klassische Logik durch zwei Eigenschaften charakterisiert:\n\n\nDas Prinzip der Zweiwertigkeit wird oft mit dem Satz vom ausgeschlossenen Dritten verwechselt.\n\nDie \"klassische Aussagenlogik\" ist jenes Gebiet der klassischen Logik, das die innere Struktur von Sätzen (Aussagen) daraufhin untersucht, aus welchen anderen Sätzen (Teilsätzen) sie zusammengesetzt sind und wie diese Teilsätze miteinander verknüpft sind. Die innere Struktur von Sätzen, die ihrerseits nicht in weitere Teilsätze zerlegt werden können, wird von der Aussagenlogik nicht betrachtet. Ein Beispiel: Die Aussage „Alle Katzen sind Hunde, und die Erde ist eine Scheibe“ ist mit dem Bindewort „und“ aus den beiden kürzeren Aussagen „Alle Katzen sind Hunde“ und „Die Erde ist eine Scheibe“ zusammengesetzt. Diese beiden Aussagen lassen sich ihrerseits nicht mehr in weitere Aussagen zerlegen, sind aus aussagenlogischer Sicht also elementar oder atomar. Andere, auf die Aussagenlogik aufbauende logische Systeme betrachten die innere Struktur solcher atomaren Aussagen; ein wichtiges Beispiel ist die Prädikatenlogik.\n\nIn Abgrenzung zur klassischen Logik entstehen \"nichtklassische Logiksysteme\", wenn man das Prinzip der Zweiwertigkeit, das Prinzip der Extensionalität oder sogar beide Prinzipien aufhebt. Nichtklassische Logiken, die durch die Aufhebung des Prinzips der Zweiwertigkeit entstehen, heißen mehrwertige Logik. Die Zahl der Wahrheitswerte (in diesem Falle üblicher: Pseudowahrheitswerte) kann dabei endlich sein (z.&nbsp;B. dreiwertige Logik), ist aber oft auch unendlich (z.&nbsp;B. Fuzzy-Logik). Hingegen verwenden Logiken, die durch die Aufhebung der Extensionalität entstehen, Junktoren (Konnektive), bei denen sich der Wahrheitswert des zusammengesetzten Satzes nicht mehr eindeutig aus dem Wahrheitswert seiner Teile bestimmen lässt. Ein Beispiel für nichtextensionale Logik ist die Modallogik, die die einstelligen nichtextensionalen Operatoren „es ist notwendig, dass“ und „es ist möglich, dass“ einführt.\n\nLogische Systeme stehen innerhalb der Logik nicht in einem Konkurrenzverhältnis um Wahrheit oder Richtigkeit. Die Frage, welches logische System für einen bestimmten Zweck genutzt werden soll, ist eher eine pragmatische.\n\nOft werden logische Systeme und logische Fragestellungen mit außerlogischen Fragen verwechselt oder vermischt, z.&nbsp;B. mit der metaphysischen Frage, welches logische System „richtig“ sei, d.&nbsp;h. die Wirklichkeit beschreibe. Zu dieser Frage gibt es unterschiedliche Standpunkte einschließlich des positivistischen Standpunkts, dass diese Frage sinnlos sei. Diese Fragen fallen aber in andere Gebiete, z.&nbsp;B. Philosophie, Wissenschaftstheorie und Sprachwissenschaft.\n\nWenn in diesem Artikel die klassische Aussagenlogik behandelt wird, so ist das also nicht als metaphysische Festlegung zu verstehen oder gar als Behauptung, dass „alle Aussagen wahr oder falsch sind“. Es ist lediglich so, dass die klassische Aussagenlogik einfach nur solche Aussagen behandelt, die wahr oder falsch sind. Das ist eine große formale Vereinfachung, die dieses System relativ leicht erlernbar sein lässt. Braucht man aus metaphysischen oder pragmatischen Gründen mehr als zwei Wahrheitswerte, kann die klassische Aussagenlogik als Ausgangspunkt dienen, um ein geeignetes logisches System aufzustellen.\n\n\nUmgangssprachliche Einleitung.\n\nEinfache Aussage (Elementaraussage).\nEine Aussage A ist ein Satz, der entweder wahr (w, wahr, true, 1) oder nicht wahr (f, falsch, false, 0) ist. Das gilt sowohl für einfache als auch für verknüpfte Aussagen. „Halbwahrheiten“ gibt es nicht. Eine Aussage kann sowohl der gewöhnlichen Sprache entstammen als auch der Sprache der Mathematik. Eine Elementaraussage ist eine Aussage, die keine aussagenlogischen Verknüpfungen (\"nicht, und, oder, wenn … dann, genau dann wenn\") enthält.\n\nBeispiele für Elementaraussagen:\nformula_2 ist offensichtlich wahr, formula_4 dagegen ist falsch. formula_1 muss man zunächst prüfen,\nbevor man entscheiden kann, ob formula_1 wahr oder falsch ist. Ob formula_3 wahr ist, kann man derzeit nicht entscheiden. Das wird sich erst am Ende der nächsten Fußballsaison\nherausstellen.\n\nIn der klassischen Aussagenlogik ist eine Aussage entweder wahr oder nicht wahr, auch wenn man den Wahrheitsgehalt nicht kennt. Das ist zum Beispiel bei den ungelösten mathematischen Problemen der Fall.\n\n\"Anmerkung:\" formula_4 ist eine All-Aussage; die Struktur solcher Aussagen ist Gegenstand der Prädikatenlogik. Im Sinne der Aussagenlogik ist es eine Elementaraussage.\n\n\nVerneinte Aussage – Negation.\nDie \"Verneinung\" bzw. \"Negation\" (auch: \"Satzverneinung\", \"äußere Verneinung\", \"kontradiktorisches Gegenteil\") einer Aussage A ist diejenige Aussage ¬A, die genau dann wahr ist, wenn A falsch ist, und die genau dann falsch ist, wenn A wahr ist. Einfacher: Die Verneinung einer Aussage A dreht den Wahrheitswert von A in sein Gegenteil um.\n\nMan erhält die Verneinung einer Aussage A immer dadurch, dass man ihr die Formulierung „Es ist nicht der Fall, dass“ voranstellt. Zwar lässt sich ein natürlichsprachlicher Satz auch verneinen, indem man das Wort „nicht“ oder eine andere negative Formulierung an geeigneter Stelle einfügt – es ist aber nicht immer ganz einfach, zu erkennen, welche Formulierung zu verwenden und an welcher Stelle einzufügen ist. Formal schreibt man für „nicht A“ in der gebräuchlichsten Notation (Schreibweise) ¬A, auf Englisch und in der Schaltalgebra auch „NOT A“, gelegentlich auch „~A“.\nWir verneinen die obigen Beispiele:\n\nAllgemein gilt für die Verneinung:\n\n\nUnd-verknüpfte Aussagen – Konjunktion.\nEine \"Konjunktion\" ist eine aus zwei Aussagen zusammengesetzte Aussage, die die Wahrheit all ihrer Teilaussagen behauptet. Umgangssprachlich verbindet man zwei Aussagen A und B durch das Bindewort „und“ zu einer Konjunktion „A und B“, in der logischen Sprache verwendet man meist das Zeichen formula_22 (Schreibweise: formula_23), gelegentlich auch das kaufmännische Und, den Ampersand (&).\n\nDie Aussage formula_23 ist immer dann wahr, wenn sowohl A als auch B jeweils wahr sind.\nAndernfalls ist formula_23 falsch, nämlich dann, wenn entweder A oder B oder beide Aussagen falsch sind.\n\nBeispiele für eine \"Und\"-Verknüpfung:\n\nDiese Teilaussagen und ihre Negationen werden nun durch formula_22 miteinander verknüpft:\n\nNur formula_32 ist wahr, weil formula_15 wahr ist und auch formula_34 wahr ist.\nformula_35 ist falsch, weil formula_16 falsch ist.\nformula_37 ist falsch, weil formula_38 falsch ist.\nformula_39 ist falsch, weil sowohl formula_16 als auch formula_38 falsch\nist.\n\n\nNichtausschließendes Oder – Disjunktion.\nEine \"Disjunktion\" ist eine zusammengesetzte Aussage, die behauptet, dass mindestens eine ihrer Teilaussagen wahr ist. Die Disjunktion in diesem Sinn wird auch \"nichtausschließendes Oder\" genannt. (Aber Achtung: Die Bezeichnung „Disjunktion“ wurde und wird oft auch für das ausschließende Oder, „entweder … oder“, verwendet – man denke an das Konzept der disjunkten Mengen. Einige Autoren verwenden daher für das Nichtausschließende Oder den Begriff \"Adjunktion\".)\nDas Formelzeichen „formula_42“ stammt von dem lateinischen Wort „vel“, was auf deutsch „oder“ bedeutet.\n\nDie Aussage formula_43 ist immer dann wahr, wenn mindestens eine der Teilaussagen A oder B wahr ist, bzw. wenn beide Teilaussagen wahr sind. Andernfalls ist formula_43 falsch, nämlich dann, wenn sowohl A als auch B falsch sind.\n\nBeispiel für eine \"Oder\"-Verknüpfung:\n\nDiese Teilaussagen und ihre Negationen werden nun durch formula_42 miteinander verknüpft:\n\nformula_51 ist wahr, weil sowohl formula_15 als auch formula_34 wahr sind.\nformula_54 ist wahr, weil formula_34 wahr ist.\nformula_56 ist wahr, weil formula_15 wahr ist.\nNur formula_58 ist falsch, weil sowohl formula_16 als auch formula_38 falsch sind.\n\n\nMateriale Implikation.\nDie \"materiale Implikation\", auch \"Konditional\" oder \"Subjunktion\" genannt, drückt die \"hinreichende Bedingung\" aus: Sie sagt, dass die Wahrheit des einen Satzes eine hinreichende Bedingung für die Wahrheit des anderen Satzes ist. Man schreibt\n\noder auch\n\noder auch nur\n\nIn einem Konditional nennt man A das \"Antezedens\", B das \"Konsequens\" oder \"Sukzedens\".\n\nBeispiele:\n\nDie Lesart „wenn … dann“ ist insofern problematisch, als mit dem natürlichsprachlichen „wenn … dann“ vor allem inhaltliche Zusammenhänge wie Kausalität oder zeitliche Nähe ausgedrückt werden. All das macht die materiale Implikation nicht, sie nennt nur den formalen Zusammenhang: „Dass es regnet, ist eine hinreichende Bedingung dafür, dass die Straße nass ist“. Zur Frage, \"warum\" das eine hinreichende Bedingung ist – ob auf Grund eines kausalen Zusammenhangs oder auch nur rein zufällig –, nimmt die materiale Implikation nicht Stellung.\nAls \"Umkehrschluss\" bezeichnet man den Schluss von formula_61 auf formula_64. Für die Beispiele bedeutet das:\n\nUmgangssprachlich lässt man sich gelegentlich zu weiteren&nbsp;– \"falschen\"&nbsp;– Aussagen\nverleiten:\n\nDas bedeutet: Wenn die Folgerung formula_61 wahr ist, dann erhält man aus der\nAussage ¬A keine Aussage über B; B kann wahr oder falsch sein. („Ex falso sequitur quodlibet“ – „Aus Falschem folgt Beliebiges“)\n\nDie Implikation ist ein wichtiges Mittel in der Mathematik. Die meisten mathematischen Beweise verwenden das Konzept der Implikation.\n\n\nBikonditional.\nDas \"Bikonditional\", oft auch \"objektsprachliche Äquivalenz\" oder \"materiale Äquivalenz\" genannt, drückt die \"hinreichende und notwendige Bedingung\" aus, sagt also, dass eine Aussage A genau dann zutrifft, wenn eine Aussage B zutrifft. Man schreibt:\n\nund liest\n\nAuch beim Bikonditional wird eine rein formale Aussage getroffen, die nichts über einen allfälligen inhaltlichen Zusammenhang von A und B aussagt.\n\nStatt formula_66 zu sagen, kann man auch sagen, dass A eine hinreichende Bedingung für B und dass B eine hinreichende Bedingung für A ist, also formula_68. Tatsächlich sind diese beiden Aussagen logisch äquivalent.\nBeispiel:\n\nDas Bikonditional als zusammengesetzte Aussage innerhalb der logischen Sprache (siehe Objektsprache) wird oft mit dem Konzept der logischen Äquivalenz verwechselt oder vermischt. Die logische Äquivalenz ist eine metasprachliche, meist natürlichsprachlich formulierte Eigenschaft zweier Aussagen der logischen Sprache. Ein Zusammenhang zwischen logischer Äquivalenz und Bikonditional besteht nur insofern, als das Metatheorem gilt, dass ein Bikonditional formula_66 genau dann eine Tautologie ist, wenn die beiden Aussagen A und B logisch äquivalent sind.\n\n\nAusschließendes Oder.\nDas ausschließende Oder (Kontravalenz oder Antivalenz), „entweder A oder&nbsp;B“, besagt, dass genau eine der beiden von ihm verknüpften Aussagen wahr ist. Entsprechend ist ein ausschließendes Oder nicht nur dann falsch, wenn sowohl A als auch B falsch sind, sondern auch, wenn \"beide\" wahr sind. (Einige Autoren verwenden für das Ausschließende Oder den Begriff \"Alternative\".)\n\nObwohl das ausschließende Oder ein Konzept ist, mit dem man in der natürlichen Sprache immer wieder zu tun hat, wird es in den meisten logischen Sprachen nicht als eigenständiger Junktor eingeführt. Stattdessen wird das ausschließende Oder zum Beispiel als verneintes Bikonditional ausgedrückt, also als formula_70.\n\nGroße Bedeutung genießt das ausschließende Oder hingegen in der Schaltalgebra, wo es meist als XOR \"(eXclusive OR)\" aufgeschrieben wird.\n\n\nVerneinung einer verknüpften Aussage (De Morgansche Gesetze).\n\nVerneinung einer Konjunktion.\nDie Verneinung der Konjunktion „A und B“ (in der logischen Schreibweise: formula_23) lautet „Es ist nicht der Fall, dass A und B zutreffen“ (in der logischen Schreibweise: formula_72).\nDiese ist logisch äquivalent mit der Aussage\n„A ist nicht der Fall, oder B ist nicht der Fall (oder beides)“ (in logischer Schreibweise: formula_73).\n\nEin Beispiel:\n\nWenn man die Aussage\nverneinen möchte, dann kann man entweder sagen\noder man sagt\n\nIn der Schaltalgebra wird sehr oft der Junktor NAND verwendet, wobei „A NAND B“ denselben Wahrheitswertverlauf hat wie der Ausdruck formula_72.\n\n\nVerneinung einer Disjunktion.\nDie Verneinung der Disjunktion „A oder B (oder beides)“ (in der logischen Schreibweise: formula_75) lautet „Es ist nicht der Fall, dass A oder B zutrifft“ (in logischer Schreibweise: formula_76).\nDiese ist logisch äquivalent mit der Aussage\n„A ist nicht der Fall, und B ist nicht der Fall“ (in logischer Schreibweise: formula_77).\n\nEin Beispiel:\n\nWenn man die Aussage\nverneinen möchte, so sagt man\nNach dem Gesetz von De Morgan kann man nun aber auch sagen:\noder in schönerem Deutsch\n\nIn der Schaltalgebra wird das Konnektiv NOR verwendet, das denselben Wahrheitswertverlauf hat wie die Aussage formula_76.\n\n\nHinreichende und notwendige Bedingung.\nDieser Abschnitt soll den zunächst oft als kontraintuitiv empfundenen Zusammenhang zwischen hinreichender und notwendiger Bedingung, wie er im Abschnitt über die materiale Implikation angesprochen wurde, wiederaufgreifen und näher ausführen.\n\nBetrachten wir noch einmal die materiale Implikation formula_79.\n\nMan sagt: A ist \"hinreichend\" für B: Schon wenn A der Fall ist, ist auch B der Fall.\n\nUmgekehrt kann man aber auch sagen: B ist \"notwendig\" für A. Ohne B kann A nicht erfüllt sein.\n\nWie kommt dieser Zusammenhang zustande?\n\nWir wissen, dass die Wahrheit von A die Wahrheit von B nach sich zieht, denn A ist ja hinreichende Bedingung für B. Somit ist es einfach nicht möglich, dass A eintritt, ohne dass B damit ebenfalls eintreten würde: B ist also gezwungenermaßen der Fall, wenn A der Fall ist. B ist „notwendig“ für A.\n\nDieser Zusammenhang ist in Wahrheit also ziemlich einfach; Hauptgrund dafür, dass er anfangs oft als kontraintuitiv empfunden wird, ist wahrscheinlich die Schwierigkeit, zwischen den vielen Bedeutungen des umgangssprachlichen „wenn … dann“ einerseits und der rein formalen hinreichenden und notwendigen Bedingung andererseits strikt zu trennen.\n\nMit dem umgangssprachlichen „wenn … dann“ möchte man fast immer einen inhaltlichen (kausalen oder auch temporalen) Zusammenhang zwischen Antecedens und Konsequens ausdrücken: „Regen verursacht Straßennässe“, „Zuerst fällt der Regen, erst nachher wird die Straße nass“. Wenn man die hinreichende Bedingung in diesem Sinn missversteht, dann ist es klar, dass die in umgekehrter Reihenfolge formulierte notwendige Bedingung „Nur wenn die Straße nass ist, regnet es“ seltsam aussieht: „Regen verursacht doch Straßennässe. Wie kann daraus je gefolgert werden, dass Straßennässe Regen verursacht?“\n\nAll dies sagt die materiale Implikation aber nicht aus. „A ist eine hinreichende Bedingung für B“ meint schlicht, dass wenn die Aussage A wahr ist, auch die Aussage B wahr ist – zeitlos und zusammenhanglos, nicht etwa „später“ oder „weil“.\n\nAnalog sagt die notwendige Bedingung, „B ist eine notwendige Bedingung für A“, lediglich das aus, dass B wahr ist, sofern A es ist. Genau das ist aber die Definition des Konditionals A → B.\n\n\nFormaler Zugang.\n\nEinleitung.\nSpätestens beim \"lauten\" Lesen von Sätzen wie:\n\nwird der selbstbewusste Laie verlangen, dass ihm erklärt wird, was das soll.\n\nDie Antwort des Logikers: Es soll versucht werden, Sicherheit in die Regeln des logischen Schließens zu bringen. Seit den Sophisten ist dem Abendland klar, dass scheinbar zwingende Schlüsse zu offensichtlich absurden Ergebnissen führen können. Immer wieder wurden Paradoxien formuliert und von großen Denkern als Herausforderung empfunden. Logiker versuchen deshalb, die Regeln des Argumentierens so streng wie möglich zu fassen.\n\nDas einleitende Beispiel macht klar, dass dazu eine \"Trennung der Sprachebenen\" unerlässlich ist: Die formale Aussage A∧B soll dadurch erklärt werden, dass auf einer metasprachlichen Ebene über die Aussage A wie auch über die Aussage B geredet wird.\n\n\"Ein\" Versuch dies durchzuführen, besteht darin, die Aussagenlogik als formales System, konkret als Kalkül (eine bestimmte Art eines formalen Systems) zu definieren. Die Begriffe „wahr“ und „falsch“ kommen in diesem System zunächst überhaupt nicht vor. Stattdessen werden Axiome gesetzt, die einfach als Zeichenketten angesehen werden, aus denen weitere ableitbare Zeichenketten aufgrund von bestimmten Schlussregeln hergeleitet werden.\n\nDas Ziel dabei ist einerseits, dass in einem formalen System nur Zeichenketten (Sätze) hergeleitet werden können, die bei einer plausiblen Interpretation auch wahr sind. Andererseits sollen alle Sätze, die als „wahr“ interpretierbar sind, auch hergeleitet werden können. Das erste ist die Forderung nach \"Korrektheit\", das zweite die nach \"Vollständigkeit\" des formalen Systems; beide Eigenschaften sind unter Kalkül: Der Begriff Kalkül in der Logik beschrieben.\n\nFür die klassische Aussagenlogik, mit der wir es hier zu tun haben, gibt es Kalküle (formale Systeme), die sowohl korrekt als auch vollständig sind. Für komplexere logische Systeme (z.&nbsp;B. Mengenlehre) ist es aber \"unmöglich\", einen vollständigen Kalkül aufzustellen, der auch korrekt ist – diese Erkenntnis wurde 1931 von Kurt Gödel bewiesen (Gödelscher Unvollständigkeitssatz).\n\n\nSyntax.\nEs gibt viele verschiedene Möglichkeiten, die Syntax („Grammatik“) einer logischen Sprache formal zu definieren; meist geschieht das im Rahmen eines Kalküls. Die folgende Definition ist daher nur als Beispiel dafür zu verstehen, wie ein Kalkül für die klassische Aussagenlogik aussehen kann. Weitere Beispiele für konkrete Kalküle finden sich unter Baumkalkül, Begriffsschrift, Systeme natürlichen Schließens, Sequenzenkalkül oder Resolutionskalkül. Ein weiterer axiomatischer Kalkül ist als Beispiel im Artikel Hilbert-Kalkül angegeben, ein graphischer Kalkül im Artikel Existential Graphs.\n\n\nBausteine der aussagenlogischen Sprache.\nAls \"Bausteine\" der aussagenlogischen Sprache sollen \"Satzbuchstaben\" („atomare Formeln“, Satzkonstanten), \"Junktoren\" und \"Gliederungszeichen\" verwendet werden. Satzbuchstaben sollen die Zeichen P, P, P, … sein. Junktoren sollen die Zeichen ¬, ∧, ∨, → und ↔ sein. Als Gliederungszeichen sollen die runden Klammern dienen.\n\nFormal lässt sich das z.&nbsp;B. auf folgende Weise ausdrücken:\n\nSei V die (abzählbar unendliche) Menge der \"atomaren Formeln\" (Satzbuchstaben):\n\nSei J die Menge der Junktoren und Gliederungszeichen:\n\nDas \"Alphabet\" der logischen Sprache sei die Menge V ∪ J, also die Vereinigungsmenge von atomaren Formeln, Junktoren und Gliederungszeichen.\n\n\nFormationsregeln.\nDie \"Formationsregeln\" legen fest, wie man aus den Bausteinen der aussagenlogischen Sprache Sätze (Formeln) bilden kann. Hier sollen \"aussagenlogische Formeln\" als Worte über dem Alphabet der logischen Sprache, also über V ∪ J wie folgt induktiv definiert werden:\n\n\n\nSchlussregeln.\n\"Schlussregeln\" sind allgemein Transformationsregeln (Umformungsregeln), die auf bestehende Formeln angewandt werden und aus ihnen neue Formeln erzeugen. Wenn man einen Kalkül für ein logisches System aufstellt, dann wählt man die Transformationsregeln so, dass sie aus bestehenden Formeln solche Formeln erzeugen, die aus den Ausgangsformeln semantisch \"folgen\" – deshalb die Bezeichnung „Schlussregel“ (\"eine Schlussfolgerung ziehen\").\n\nInnerhalb der Syntax sind die Schlussregeln allerdings rein formale Transformationsregeln, denen für sich keinerlei inhaltliche Bedeutung zukommt.\n\nAn konkreten Schlussregeln sollen hier nur zwei angegeben werden: Der Modus ponendo ponens und die Substitutionsregel.\n\n\n\n\nAxiome.\n\"Axiome\" sind ausgezeichnete (im Sinn von: hervorgehobene) Formeln der aussagenlogischen Sprache. Die Auszeichnung besteht darin, dass sie innerhalb eines Beweises oder einer Herleitung (siehe unten) ohne weitere Rechtfertigung verwendet werden.\n\nPragmatisch wählt man solche Formeln als Axiome, die semantisch gesehen Tautologien sind, also immer zutreffen, und die dabei helfen, Beweise zu verkürzen. Innerhalb der Syntax sind die Axiome allerdings rein formale Objekte, denen keinerlei inhaltliche Bedeutung oder Rechtfertigung zukommt.\n\nAxiome sind im Allgemeinen optional, d.&nbsp;h. ein Kalkül kann auch ganz ohne Axiome auskommen, wenn er ausreichend viele bzw. mächtige Schlussregeln hat. Axiomfreie Kalküle sind zum Beispiel die Systeme natürlichen Schließens oder Baumkalküle.\n\nHier soll exemplarisch ein axiomatischer Kalkül gezeigt werden, und zwar Russells Aussagenkalkül aus seiner Typentheorie 1908, den er 1910 in die Principia Mathematica übernahm. Dieser Kalkül umfasst die folgenden Axiome (von denen das vierte redundant, d.&nbsp;h. nicht unbedingt erforderlich, weil aus den anderen Axiomen herleitbar ist):\n\n\nUm aus diesen Axiomen auch solche gültigen Sätze herleiten zu können, die andere als die in den Axiomen vorkommende Junktoren enthalten, werden diese durch folgende Festlegung auf die vorhandenen Junktoren zurückgeführt:\n\n\nAlternativ zu – wie hier – konkreten Axiomen kann man auch \"Axiomenschemata\" angeben, in welchem Fall man auch ohne Substitutionsregel auskommt. Interpretiert man die obigen Axiome als Axiomenschemata, dann stünde z.&nbsp;B. das erste Axiomenschema, formula_91, für unendlich viele Axiome, nämlich alle Ersetzungsinstanzen dieses Schemas.\n\n\nHerleitung und Beweis.\nEine Herleitung ist eine Liste von aufsteigend nummerierten Sätzen, die mit einer oder mehreren Annahmen (den Prämissen der Herleitung) oder Axiomen beginnt. Alle auf diese folgenden Sätze sind entweder ebenfalls Axiome (bei manchen Kalkülen sind auch weitere Annahmen zulässig) oder sind aus einer oder mehreren der vorangehenden Zeilen durch Anwendung von Schlussregeln entstanden. Der letzte Satz in der Liste ist die Konklusion der Herleitung.\n\nEine Herleitung ohne Prämissen heißt \"Beweis\". Oft werden aber die Wörter „Herleitung“ und „Beweis“ synonym gebraucht.\n\nWenn es gelingt, aus einer Menge von Annahmen (Prämissen) Δ eine Konklusion P herzuleiten, dann schreibt man auch formula_99.\n\nGelingt es, einen Satz P ohne die Verwendung von Annahmen herzuleiten (zu beweisen), dann schreibt man auch: formula_100. In diesem Fall wird P \"Theorem\" genannt.\n\nDas Zeichen formula_101 geht auf die Begriffsschrift zurück, jenes Werk, in dem Gottlob Frege 1879 die erste Formalisierung der Prädikatenlogik angegeben hat.\n\nIn der klassischen Aussagenlogik wählt man die Schlussregeln so, dass sich mit ihrer Hilfe \"alle\" gültigen Argumente (und \"nur\" gültige Argumente) herleiten lassen; die Frage der Gültigkeit wird im folgenden Abschnitt, „Semantik“, behandelt.\n\n\nSemantik.\nAußerhalb der Logik bezeichnet Semantik ein Forschungsgebiet, das sich mit der Bedeutung von Sprache und deren Teilen befasst. Oft wird auch das Wort \"Semantik\" gleichbedeutend mit dem Wort \"Bedeutung\" verwendet.\n\nAuch innerhalb der Logik geht es bei Semantik um Bedeutung: Darum nämlich, den Ausdrücken einer formalen Sprache – zum Beispiel der hier behandelten Sprache der Aussagenlogik – eine Bedeutung zuzuordnen. In der Logik wird auch das meist sehr formal unternommen.\n\nIm Zentrum der (formalen) Semantik steht eine Auswertungsfunktion (andere Bezeichnungen lauten Bewertungsfunktion, Denotationsfunktion, Wahrheitswertefunktion), die den Formeln der logischen Sprache eine Bedeutung zuordnet. Formal gesprochen ist die Auswertungsfunktion eine Abbildung von der Menge der Formeln der Sprache in die Menge der Wahrheitswerte. Oft wird die Auswertungsfunktion mit dem Großbuchstaben V bezeichnet.\n\nIn der klassischen Aussagenlogik ist die Auswertungsfunktion sehr einfach: Das Prinzip der Zweiwertigkeit fordert, dass sie für jede zu bewertende Formel genau einen von genau zwei Wahrheitswerten liefern muss; und das Prinzip der Extensionalität fordert, dass die Bewertungsfunktion beim Bewerten eines komplexen Satzes nur die Bewertung von dessen Teilsätzen berücksichtigen muss.\n\nJedem Atom, also jedem Satzbuchstaben (Atom) wird durch Festsetzung ein Wahrheitswert zugeordnet. Man sagt: Die Atome werden interpretiert. Es wird also z.&nbsp;B. festgelegt, dass P wahr ist, dass P falsch ist und dass P ebenfalls falsch ist. Damit ist der Bewertung der Bausteine der logischen Sprache Genüge getan. Formal ist eine solche Bewertung –&nbsp;\"Interpretation\" genannt und oft mit dem Kleinbuchstaben v bezeichnet&nbsp;– eine Funktion im mathematischen Sinn, d.&nbsp;h. eine Abbildung von der Menge der Atome in die Menge der Wahrheitswerte.\n\nWenn die Auswertungsfunktion V auf ein Atom angewandt wird, d.&nbsp;h. wenn sie ein Atom bewerten soll, liefert sie die Interpretation dieses Atoms im Sinn des obigen Absatzes. Mit anderen Worten, sie liefert den Wert, den die Bewertung v dem Atom zuordnet.\n\nUm die zusammengesetzten Formeln bewerten zu können, muss für jeden Junktor definiert werden, welchen Wahrheitswert die Bewertungsfunktion für die unterschiedlichen Wahrheitswertkombinationen liefert, den seine Argumente annehmen können. In der klassischen Aussagenlogik geschieht das meist mittels Wahrheitstabellen, weil es nur überschaubar wenige Möglichkeiten gibt.\n\nDer einstellige Junktor ¬, die Negation, ist in der klassischen Aussagenlogik so definiert, dass er den Wahrheitswert seines Arguments ins Gegenteil umkehrt, also „verneint“: Ist die Bewertung einer Formel X wahr, dann liefert die Bewertungsfunktion für ¬X falsch; wird aber X falsch bewertet, dann liefert die Bewertungsfunktion für ¬X wahr. Die Wahrheitstabelle sieht folgendermaßen aus:\n\nDie Wahrheitswertverläufe der verwendeten zweistelligen Konnektive sind in der klassischen Aussagenlogik wie folgt definiert:\n\nAllgemein gibt es für die klassische Aussagenlogik vier einstellige und sechzehn zweistellige Junktoren. Die hier behandelte logische Sprache beschränkt sich nur deshalb auf die Junktoren ¬, ∧, ∨, → und&nbsp;↔, weil diese am gebräuchlichsten sind und weil sie auch inhaltlich noch am ehesten aus der Alltagssprache bekannt sind. Aus formaler Sicht ist die einzige Bedingung, die man bei der Wahl von Junktoren erfüllen möchte, die, dass sich mit den gewählten Junktoren auch alle anderen theoretisch möglichen Junktoren ausdrücken lassen; man sagt: Dass die Menge der gewählten Junktoren funktional vollständig ist. Diese Anforderung ist bei der hier getroffenen Wahl erfüllt.\n\nNäheres zur Frage, wie viele und welche Junktoren es gibt und wie viele Junktoren man benötigt, um funktionale Vollständigkeit zu erreichen, ist im Kapitel Junktor beschrieben.\n\n\nSemantische Gültigkeit, Tautologien.\n\"Semantische Gültigkeit\" ist eine Eigenschaft von Formeln oder von Argumenten. (Ein Argument ist die Behauptung, dass aus einigen Aussagen –&nbsp;den Prämissen&nbsp;– eine bestimmte Aussage –&nbsp;die Konklusion&nbsp;– folgt.)\n\nEine \"Formel\" der aussagenlogischen Sprache heißt genau dann semantisch gültig, wenn die Formel unter allen Interpretationen –&nbsp;d.&nbsp;h. unter allen Zuordnungen von Wahrheitswerten zu den in ihr vorkommenden Atomen&nbsp;– wahr ist; wenn sie sozusagen allgemeingültig ist; mit anderen Worten: Wenn die Wahrheitstabelle für diese Aussage in jeder Zeile das Ergebnis \"wahr\" zeigt. Man nennt semantisch gültige Formeln auch Tautologien und schreibt, wenn formula_102 eine Tautologie ist, formal wie folgt:\n\nEin \"Argument\" heißt genau dann semantisch gültig, wenn unter der Voraussetzung, dass alle Prämissen wahr sind, auch die Konklusion wahr ist. In der Formulierung von Gottfried Wilhelm Leibniz: \"Aus Wahrem folgt nur Wahres.\" Diese Definition muss natürlich ebenfalls formal gefasst werden, und das geschieht wie folgt: Ein Argument ist genau dann semantisch gültig, wenn alle Zuordnungen von Wahrheitswerten zu den in Prämissen und Konklusion vorkommenden Atomen, unter denen die Bewertungsfunktion für alle Prämissen den Wert \"wahr\" liefert, auch für die Konklusion den Wert \"wahr\" liefert.\n\nUm auszudrücken, dass aus einer Menge formula_104 von Formeln (der Prämissenmenge) eine Formel formula_102 (die Konklusion) semantisch folgt, schreibt man formal wie folgt:\n\nBeachte die graphische Ähnlichkeit und die inhaltliche Verschiedenheit zwischen formula_99 (Kapitel „Herleitung und Beweis“) und formula_106 (\"Siehe:\" Semantische Folgerung): Die erste Formulierung –&nbsp;formula_99&nbsp;– drückt die \"syntaktische\" Gültigkeit des Arguments aus, sagt also, dass aus den Formeln in formula_104 mit den Schlussregeln des gewählten Kalküls die Formel formula_102 \"hergeleitet\" werden kann. formula_106 hingegen behauptet die \"semantische\" Gültigkeit, die in der klassischen Aussagenlogik wie in den vorangegangenen Absätzen als das Leibniz’sche \"Aus Wahrem folgt nur Wahres\" definiert ist.\n\n\nWichtige semantische Eigenschaften: Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit.\nNeben der Eigenschaft der Gültigkeit (Allgemeingültigkeit) gibt es einige andere wichtige Eigenschaften: Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit. Im Gegensatz zur Gültigkeit, die Eigenschaft von Formeln oder von Argumenten sein kann, sind Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit Eigenschaften von Sätzen oder von Satzmengen.\n\n\nDie Frage, ob eine Formel (oder eine Formelmenge) eine der genannten Eigenschaften hat, ist ebenso wie die Frage, ob eine Formel allgemeingültig, d.&nbsp;h. eine Tautologie ist, für allgemeine Formeln nicht effizient lösbar: Zwar ist die Wahrheitstafel ein Entscheidungsverfahren für jede dieser Fragen, doch umfasst eine Wahrheitstafel für eine Aussage bzw. eine Aussagemenge in n Atomen formula_113 Zeilen; das Wahrheitstafelverfahren ist nichts anderes als ein Brute-Force-Verfahren.\n\nJede dieser Fragestellungen kann auf die Frage zurückgeführt werden, ob eine bestimmte Formel erfüllbar ist:\n\nDie Frage, ob eine Aussage erfüllbar ist, wird Erfüllbarkeitsproblem oder \"SAT-Problem\" (nach dem englischen Wort für Erfüllbarkeit, \"satisfiability\") genannt. Das SAT-Problem spielt eine wichtige Rolle in der theoretischen Informatik und Komplexitätstheorie. Das Erfüllbarkeitsproblem für allgemeine (beliebige) Formeln ist NP-vollständig, d.&nbsp;h. (unter der Voraussetzung, dass P ungleich NP) nicht in polynomialer Laufzeit lösbar.\n\nFür bestimmte echte Teilmengen der Formeln der aussagenlogischen Sprache ist das SAT-Problem dennoch schneller, d.&nbsp;h. in polynomial beschränkter Rechenzeit lösbar. Eine solche Teilmenge sind die Horn-Formeln, das sind Konjunktionen von Disjunktionen, deren Disjunkte verneinte oder unverneinte Atome sind, wobei innerhalb einer solchen Disjunktion allerdings höchstens ein Atom unverneint sein darf.\n\n\nAlgebraische Sicht.\nWenn man die Semantik betrachtet, die hier für die klassische Aussagenlogik aufgestellt wurde, dann erkennt man gewisse Gesetzmäßigkeiten. Wird z.&nbsp;B. die Auswertungsfunktion auf eine Aussage der Form X ∧ W angewendet, wobei W eine beliebige wahre Aussage sein soll, dann stellt man fest, dass die Auswertungsfunktion für X ∧ W immer den Wahrheitswert \"wahr\" liefert, wenn V(X)=wahr ist (das heißt V(X∧W)=V(X)). Von der Struktur her gleichwertige Gesetzmäßigkeiten gelten auch in anderen Semantiken, auch in solchen, die für ganz andere, nichtlogische Systeme aufgestellt werden. Für die Arithmetik gilt z.&nbsp;B., dass die dortige Bewertungsfunktion (hier V genannt) für einen Ausdruck der Form X + Y immer den Wert von X liefert, sofern der Wert von Y null ist: V(X+Y)=V(X), wenn V(Y) = null ist.\n\nEine formale Wissenschaft, die solche strukturellen Gesetzmäßigkeiten untersucht, ist die abstrakte Algebra (meist Teilgebiet der Mathematik, aber auch der Informatik). In der abstrakten Algebra wird zum Beispiel untersucht, für welche Verknüpfungen es ein neutrales Element gibt, d.&nbsp;h. ein Element \"N\", das für eine Verknüpfung \"op\" dazu führt, dass (für beliebiges X) gilt: X \"op\" \"N\" = X. So würde man aus algebraischer Sicht sagen, dass es für die klassische aussagenlogische Konjunktion genau ein neutrales Element gibt, nämlich \"wahr\", und dass es für die Addition in der Arithmetik ebenfalls genau ein neutrales Element gibt, nämlich die Zahl Null. Nur am Rande sei erwähnt, dass es auch für andere Junktoren neutrale Elemente gibt; das neutrale Element für die Disjunktion ist \"falsch\": V(X ∨ F) = V(X), wenn V(F)=falsch ist.\n\nDie formale Algebra betrachtet formale Semantiken rein nach ihren strukturellen Eigenschaften. Sind diese identisch, dann besteht zwischen ihnen aus algebraischer Sicht kein Unterschied. Aus algebraischer Sicht, genauer: Aus Sicht der formalen Algebra ist die Semantik für die klassische Aussagenlogik eine zweiwertige Boolesche Algebra. Andere formale Systeme, deren Semantiken jeweils eine Boolesche Algebra bilden, sind die Schaltalgebra und die elementare Mengenlehre. Aus algebraischer Sicht besteht daher zwischen diesen Disziplinen kein Unterschied.\n\nNormalformen.\nJede aussagenlogische Formel lässt sich in eine äquivalente Formel in\nkonjunktiver Normalform und eine äquivalente Formel in\ndisjunktiver Normalform umformen.\n\n\nMetatheorie.\nIn der Metatheorie werden die Eigenschaften von logischen Systemen untersucht: Das logische System ist in der Metatheorie der Untersuchungsgegenstand.\n\nEine metatheoretische Fragestellung ist zum Beispiel die, ob in einem Kalkül ein Widerspruch hergeleitet werden kann.\n\nDer vorliegende Abschnitt soll einige wichtige metatheoretische Fragestellungen aus dem Blickwinkel der Aussagenlogik betrachten.\n\n\n\n\n\nEin metatheoretisches Resultat ist zum Beispiel die Feststellung, dass alle korrekten Kalküle auch konsistent sind. Ein anderes metatheoretisches Resultat ist die Feststellung, dass ein konsistenter Kalkül nicht automatisch korrekt sein muss: Es ist ohne weiteres möglich, einen Kalkül aufzustellen, in dem zwar kein Widerspruch hergeleitet werden kann, in dem aber z.&nbsp;B. die nicht allgemeingültige Aussage der Form „A ∨ B“ hergeleitet werden kann. Ein solcher Kalkül wäre aus ersterem Grund konsistent, aus letzterem Grund aber nicht korrekt.\n\nEin weiteres, sehr einfaches Resultat ist die Feststellung, dass ein vollständiger Kalkül nicht automatisch auch korrekt oder nur konsistent sein muss. Das einfachste Beispiel wäre ein Kalkül, in dem \"jede\" Formel der aussagenlogischen Sprache herleitbar ist. Da jede Formel herleitbar ist, sind alle Tautologien herleitbar, die ja Formeln sind: Das macht den Kalkül vollständig. Da aber jede Formel herleitbar ist, ist insbesondere auch die Formel P ∧ ¬ P und die Formel A ∨ B herleitbar: Ersteres macht den Kalkül inkonsistent, letzteres inkorrekt.\n\nDas Ideal, das ein Kalkül erfüllen sollte, ist Korrektheit und Vollständigkeit: Wenn das der Fall ist, dann ist er der ideale Kalkül für ein logisches System, weil er alle semantisch gültigen Sätze (und nur diese) herleiten kann. So sind die beiden Fragen, ob ein konkreter Kalkül korrekt und/oder vollständig ist und ob es für ein bestimmtes logisches System überhaupt möglich ist, einen korrekten und vollständigen Kalkül anzugeben, zwei besonders wichtige metatheoretische Fragestellungen.\n\n\nAbgrenzung und Philosophie.\nDie klassische Aussagenlogik, wie sie hier ausgeführt wurde, ist ein formales logisches System. Als solches ist sie eines unter vielen, die aus formaler Sicht gleichwertig nebeneinander stehen und die ganz bestimmte Eigenschaften haben: Die meisten sind konsistent, die meisten sind korrekt, etliche sind vollständig, und einige sind sogar entscheidbar. Aus formaler Sicht stehen die logischen Systeme in keinem Konkurrenzverhalten hinsichtlich Wahrheit oder Richtigkeit.\n\nVon formalen, innerlogischen Fragen klar unterschieden sind außerlogische Fragen: Solche nach der Nützlichkeit (Anwendbarkeit) einzelner Systeme für einen bestimmten Zweck und solche nach dem philosophischen, speziell metaphysischen Status einzelner Systeme.\n\nDie Nützlichkeitserwägung ist die einfachere, bezüglich deren Meinungsunterschiede weniger tiefgehend bzw. weniger schwerwiegend sind. Klassische Aussagenlogik zum Beispiel bewährt sich in der Beschreibung elektronischer Schaltungen (Schaltalgebra) oder zur Formulierung und Vereinfachung logischer Ausdrücke in Programmiersprachen. Prädikatenlogik wird gerne angewandt, wenn es darum geht, Faktenwissen zu formalisieren und automatisiert Schlüsse daraus zu ziehen, wie das unter anderem im Rahmen der Programmiersprache Prolog geschieht. Fuzzy-Logiken, nonmonotone, mehrwertige und auch parakonsistente Logiken sind hochwillkommen, wenn es darum geht, mit Wissensbeständen umzugehen, in denen Aussagen mit unterschiedlich starkem Gewissheitsgrad oder gar einander widersprechende Aussagen abgelegt werden sollen und dennoch sinnvolle Schlüsse aus dem Gesamtbestand gezogen werden sollen. Auch wenn es je nach Anwendungsfall sehr große Meinungsunterschiede geben kann, welches logisches System besser geeignet ist, ist die Natur des Problems für alle Beteiligten unmittelbar und in gleicher Weise greifbar. Einzelwissenschaftliche Überlegungen und Fragestellungen spielen sich überwiegend in diesem Bereich ab.\n\n(Noch) kontroverser als solche pragmatischen Überlegungen sind Fragestellungen philosophischer und metaphysischer Natur. Geradezu paradigmatisch ist die Frage, „welches logische System richtig ist“, wobei „richtig“ hier gemeint ist als: Welches logische System nicht nur einen Teilaspekt der Wirklichkeit modellhaft vereinfacht, sondern die Wirklichkeit, das Sein als Ganzes adäquat beschreibt. Zu dieser Fragestellung gibt es viele unterschiedliche Meinungen einschließlich der vom philosophischen Positivismus eingeführten Meinung, dass die Fragestellung als Ganzes sinnlos ist.\n\nIn den Bereich metaphysischer Fragestellungen fällt auch die Frage, ob es so etwas wie ein \"metaphysisches\" Prinzip der Zweiwertigkeit gebe, ob also Aussagen über die Wirklichkeit durchgehend ins Schema wahr/falsch passen oder nicht. Diese Frage ist unabhängig von der Frage, ob die Beschäftigung mit zwei- oder mehrwertigen Logiken praktisch sinnvoll ist: Selbst wenn ein metaphysisches Prinzip der Zweiwertigkeit herrscht, könnte man anwendungspraktisch mehrwertige Logiken nützen, etwa dazu, epistemische Sachverhalte zu fassen, zum Beispiel aus Aussagen zu schließen, die zwar metaphysisch wahr oder falsch sind, von denen aber nicht oder noch nicht bekannt ist, welches von beidem der Fall ist. Umgekehrt kann man auch dann, wenn ein solches metaphysisches Prinzip nicht gilt, zweiwertige Logik wegen ihrer Einfachheit für solche Anwendungen bevorzugen, bei denen nur mit solchen Sätzen umgegangen werden muss, die tatsächlich wahr oder falsch sind.\n\nDie Frage nach einem metaphysischen Prinzip der Zweiwertigkeit ist wie die meisten metaphysischen Fragen nicht endgültig zufriedenstellend beantwortet. Ein früher Einwand gegen ein solches Prinzip, den Aristoteles zur Diskussion stellte, war das Thema der Aussagen über zukünftige Sachverhalte („Morgen wird es regnen“). Wenn Aussagen über Zukünftiges schon heute wahr oder falsch wären, so wird argumentiert, dann müsse die Zukunft bis ins letzte Detail vorbestimmt sein. Ein anderer Einwand, der vorgebracht wird, ist, dass es Aussagen gibt, deren Wahrheit praktisch oder theoretisch nicht festgestellt werden kann – zum Beispiel lässt sich die Wahrheit von „Der Rasen vor dem Weißen Haus bestand am 1. Februar 1870 aus genau 6.120.375,4 Grashalmen“ einfach nicht feststellen.\n\nBefürworter eines metaphysischen Zweiwertigkeitsprinzips berufen sich oft auf das Verhalten von Metatheoretikern, also von Mathematikern oder Logikern, die Aussagen \"über\" formale Systeme treffen: Egal wie mehrwertig oder nichtklassisch das untersuchte System ist, die dabei getroffenen Metavermutungen, Metabehauptungen und Metafeststellungen sind immer zweiwertig: Ein Kalkül, auch ein parakonsistenter oder nonmonotoner, wird immer als \"entweder\" konsistent \"oder\" inkonsistent betrachtet, und ein logisches System ist immer \"entweder\" korrekt oder inkorrekt, vollständig oder nicht vollständig, entscheidbar oder unentscheidbar, niemals „ein bisschen“ von beidem. Befürworter deuten das als Hinweis darauf, dass es in der Wirklichkeit tatsächlich eine strenge Unterscheidung nach wahr und falsch gebe oder dass es zumindest sinnvoll ist, eine solche anzunehmen.\n\nEine andere philosophische Fragestellung ist die nach dem metaphysischen Status des Untersuchungsgegenstands der Logik, also danach, was logische Systeme, Kalküle, Wahrheitswerte eigentlich „sind“.\n\nDer platonische Standpunkt besteht darin, dass die in der Logik verwendeten Zeichen und Konstrukte eine außerlogische Bedeutung haben, dass sie Namen für real existierende (wenn auch natürlich nicht-physikalische) Gegenstände sind. In diesem Sinn gäbe es so etwas wie \"das Wahre\" und \"das Falsche\", abstrakte Gegenstände, die von den Zeichen „wahr“ und „falsch“ benannt werden.\n\nDer Gegenpol zum Platonismus wäre der Nominalismus, der Existenz nur den Zeichen zuspricht, die in der Logik manipuliert werden. Gegenstand der Logik sind Zeichen, und die Tätigkeit der Logiker ist die Manipulation von Zeichen. Die Zeichen bezeichnen aber nichts, so etwas wie das Wahre oder das Falsche gibt es also nicht. Im Grundlagenstreit der Mathematik entspräche der nominalistischen Position die formalistische Richtung.\n\nEine Mittelstellung nähme der philosophische Konstruktivismus ein, demzufolge die Zeichen zwar keine unabhängig existierenden Gegenstände bezeichnen, durch den Umgang mit den Zeichen aber Gegenstände konstruiert werden.\n\n\n"}
{"id": "13", "url": "https://de.wikipedia.org/wiki?curid=13", "title": "Liste von Autoren/A", "text": "Liste von Autoren/A\n"}
{"id": "14", "url": "https://de.wikipedia.org/wiki?curid=14", "title": "Liste von Autoren/H", "text": "Liste von Autoren/H\n"}
{"id": "15", "url": "https://de.wikipedia.org/wiki?curid=15", "title": "Liste von Autoren/C", "text": "Liste von Autoren/C\n"}
{"id": "16", "url": "https://de.wikipedia.org/wiki?curid=16", "title": "Liste von Autoren/I", "text": "Liste von Autoren/I\n"}
{"id": "17", "url": "https://de.wikipedia.org/wiki?curid=17", "title": "Liste von Autoren/K", "text": "Liste von Autoren/K\n\n"}
{"id": "18", "url": "https://de.wikipedia.org/wiki?curid=18", "title": "Liste von Autoren/J", "text": "Liste von Autoren/J\n"}
{"id": "19", "url": "https://de.wikipedia.org/wiki?curid=19", "title": "Liste von Autoren/V", "text": "Liste von Autoren/V\n"}
{"id": "20", "url": "https://de.wikipedia.org/wiki?curid=20", "title": "Liste von Autoren/G", "text": "Liste von Autoren/G\n"}
{"id": "21", "url": "https://de.wikipedia.org/wiki?curid=21", "title": "Liste von Autoren/W", "text": "Liste von Autoren/W\n"}
{"id": "22", "url": "https://de.wikipedia.org/wiki?curid=22", "title": "Liste von Autoren/B", "text": "Liste von Autoren/B\n"}
{"id": "23", "url": "https://de.wikipedia.org/wiki?curid=23", "title": "Liste von Autoren/D", "text": "Liste von Autoren/D\n"}
{"id": "24", "url": "https://de.wikipedia.org/wiki?curid=24", "title": "Liste von Autoren/S", "text": "Liste von Autoren/S\n"}
{"id": "25", "url": "https://de.wikipedia.org/wiki?curid=25", "title": "Liste von Autoren/T", "text": "Liste von Autoren/T\n"}
{"id": "26", "url": "https://de.wikipedia.org/wiki?curid=26", "title": "Liste von Autoren/M", "text": "Liste von Autoren/M\n\n"}
{"id": "27", "url": "https://de.wikipedia.org/wiki?curid=27", "title": "Liste von Autoren/O", "text": "Liste von Autoren/O\n"}
{"id": "28", "url": "https://de.wikipedia.org/wiki?curid=28", "title": "Liste von Autoren/F", "text": "Liste von Autoren/F\n"}
{"id": "29", "url": "https://de.wikipedia.org/wiki?curid=29", "title": "Liste von Autoren/E", "text": "Liste von Autoren/E\n"}
{"id": "30", "url": "https://de.wikipedia.org/wiki?curid=30", "title": "Liste von Autoren/L", "text": "Liste von Autoren/L\n"}
{"id": "31", "url": "https://de.wikipedia.org/wiki?curid=31", "title": "Liste von Autoren/N", "text": "Liste von Autoren/N\n"}
{"id": "32", "url": "https://de.wikipedia.org/wiki?curid=32", "title": "Liste von Autoren/P", "text": "Liste von Autoren/P\n"}
{"id": "33", "url": "https://de.wikipedia.org/wiki?curid=33", "title": "Liste von Autoren/Q", "text": "Liste von Autoren/Q\n\n"}
{"id": "34", "url": "https://de.wikipedia.org/wiki?curid=34", "title": "Liste von Autoren/R", "text": "Liste von Autoren/R\n"}
{"id": "35", "url": "https://de.wikipedia.org/wiki?curid=35", "title": "Liste von Autoren/U", "text": "Liste von Autoren/U\n"}
{"id": "36", "url": "https://de.wikipedia.org/wiki?curid=36", "title": "Liste von Autoren/Y", "text": "Liste von Autoren/Y\n"}
{"id": "37", "url": "https://de.wikipedia.org/wiki?curid=37", "title": "Liste von Autoren/Z", "text": "Liste von Autoren/Z\n"}
{"id": "38", "url": "https://de.wikipedia.org/wiki?curid=38", "title": "Anthony Minghella", "text": "Anthony Minghella\n\nAnthony Minghella, CBE (* 6. Januar 1954 auf der Isle of Wight; † 18. März 2008 in London) war ein britischer Filmregisseur, Filmproduzent, Drehbuchautor, Dramatiker, Hörspielautor, Theater- und Opernregisseur.\n\n\nLeben.\nMinghella war der Sohn italienisch-schottischer Eltern, die auf der Isle of Wight eine Fabrik für Eiscreme betrieben. Nach seinem Schulabschluss studierte er an der Universität Hull, wo er eine Zeit lang als Dozent tätig war. 1978 drehte er einen ersten Kurzfilm. Seit 1981 war er als Autor und Story Editor tätig. Er wurde mit Theaterstücken, Rundfunkhörspielen, der Fernsehserie \"Inspector Morse\" und vielen Drehbüchern für Film und Fernsehen bekannt. Er entwickelte die Drehbücher für die 1988 erfolgreich ausgestrahlte Fernsehserie The Storyteller von Muppets-Erfinder Jim Henson.\n\nAuch als Produzent war er erfolgreich, darunter für die Filme \"Der stille Amerikaner\", \"Die Dolmetscherin\" und \"Der Vorleser\", für den er 2008 posthum für den Oscar (Kategorie „Bester Film“) nominiert wurde. Gemeinsam mit seinem Freund und Kollegen Sydney Pollack gründete er die Produktionsfirma Mirage Enterprises. Der Regisseur Minghella galt als ein guter Schauspielerführer: Unter seiner Regie brachten es zahlreiche Darsteller zu Oscar-Nominierungen, zwei Schauspielerinnen erhielten die Auszeichnung als „Beste Nebendarstellerin“: Juliette Binoche (\"Der englische Patient\") und Renée Zellweger (\"Unterwegs nach Cold Mountain\").\n\nGegen Ende seines Lebens kehrte Minghella zu seinen Anfängen im Radio und auf der Bühne zurück: 2006 wurde sein Hörspiel \"Eyes Down Looking\" mit Jude Law zu Ehren von Samuel Beckett auf BBC Radio 3 ausgestrahlt, ein Jahr zuvor hatte seine Inszenierung der Puccini-Oper Madame Butterfly in der English National Opera in London Premiere und wurde auch in der Nationaloper von Vilnius, in der Metropolitan Opera in New York und in der Wiener Staatsoper gezeigt. Am Ende des Films \"Abbitte\" von Joe Wright (2007) hat er einen Kurzauftritt als Talkshow-Moderator neben Vanessa Redgrave. Seine letzte Arbeit als Drehbuchautor war das Skript für den Musical-Film \"Nine\" (gemeinsam mit Michael Tolkin). Zu seinen letzten Regiearbeiten zählt der Pilotfilm zur Krimiserie \"Eine Detektivin für Botswana\" (Originaltitel: ), den die BBC fünf Tage nach seinem Tod erstmals ausstrahlte.\n\nMinghella war mit der aus Hongkong stammenden Choreographin, Produzentin und Schauspielerin Carolyn Choa (\"Wie verrückt und aus tiefstem Herzen\") verheiratet. Der Ehe entstammen zwei Kinder, die in der Filmbranche tätig sind: Tochter Hannah Minghella in der Produktion und Sohn Max Minghella als Schauspieler (\"Agora – Die Säulen des Himmels\"). Die Tante Edana Minghella und der Onkel Dominic Minghella (u.&nbsp;a. für die deutsche Fernsehserie \"Doktor Martin\") sind Drehbuchautoren.\n\nMinghella starb im Alter von 54 Jahren in einem Londoner Krankenhaus an inneren Blutungen infolge der Operation eines Tonsillenkarzinoms und eines Karzinoms im Nacken.\n\n\nAuszeichnungen.\n1984 erhielt Minghella den Londoner Kritikerpreis als meistversprechender junger Dramatiker, 1986 den Kritikerpreis für sein Stück \"Made in Bangkok\" als bestes Stück der Saison. 1997 erhielt er für \"Der englische Patient\" den Oscar in der Rubrik \"Beste Regie\", 1999 eine Oscar-Nominierung in der Kategorie „Bestes adaptiertes Drehbuch“ für \"Der talentierte Mr. Ripley\", bei dem er auch Regie führte.\n\n2001 wurde Minghella zum Commander of the British Empire (CBE) ernannt. Von 2003 bis 2007 war er Präsident des British Film Institute. Seit 1997 trägt das Anthony Minghella Theatre auf der Isle of Wight seinen Namen.\n\n\n"}
{"id": "39", "url": "https://de.wikipedia.org/wiki?curid=39", "title": "US-amerikanischer Film", "text": "US-amerikanischer Film\n\nDie Geschichte des US-amerikanischen Films ist ein Kapitel der Filmgeschichte, das gerade wegen der hervorgehobenen Stellung der Vereinigten Staaten als Filmnation sowohl für die Filmkunst als auch für die Ökonomie des Films relevant ist. Weltruhm erlangte Hollywood, ein Stadtteil von Los Angeles, als Zentrum der US-amerikanischen Filmindustrie, weshalb der Name oft auch als Synonym für die gesamte amerikanische Film-Branche steht. Synonym für Hollywoods Filmindustrie wird wiederum der Begriff \"Traumfabrik\" ( \"Dreamfactory\") verwendet.\n\nDer Aufbau des Filmmarktes (1910 bis 1918).\n\nInternationale Entwicklung.\nBis 1912 konzentrierten sich die US-amerikanischen Filmunternehmen auf den inneramerikanischen Filmwettbewerb. Erst danach stieg ihr Einfluss auf dem Weltmarkt. Und zwar so rapide, dass sie bereits 1914, zu Beginn des Ersten Weltkriegs, die Hälfte der Welt-Filmproduktion stellten.\n\nDer harte Wettkampf zwischen dem Edison Trust und den von Carl Laemmle angeführten „Independents“ hatte wirksame Instrumente geschaffen, die, am nationalen Konkurrenten erprobt und verfeinert, nun mit zunehmender Härte die internationalen Mitbewerber trafen. Dennoch war die Vormachtstellung Hollywoods längst nicht unangreifbar, erst eine politische Entwicklung verschaffte ihr die nötige Ruhe zur Restrukturierung: Der Krieg in Europa.\n\nDie französische Filmproduktion, Hauptkonkurrent der US-Amerikaner, kam mit dem Ausbruch des Krieges sofort und vollständig zum Erliegen, denn Pathé wandelte seine Rohfilm-Fabrik in eine Munitionsfabrik um und seine Studios in Kasernen. Ähnlich, und doch weniger extrem, brach die italienische Produktion beim Kriegseintritt des Landes 1916 ein.\n\nNachdem absehbar war, dass der Krieg sehr lange dauern konnte, bemühten sich die Franzosen, wieder ins Geschäft zu kommen. Die Position, die sie vor Ausbruch des Krieges innehatten, erreichten sie nicht mehr. Zudem beschloss das Deutsche Reich 1916 das generelle Filmeinfuhrverbot, was die europäischen Filmnationen ihres wichtigsten Absatzmarktes beraubte. Auch der Export nach Übersee gestaltete sich zunehmend schwierig, denn die Militärs beanspruchten viele Transportkapazitäten für sich. Außerdem führten deutsche U-Boote und kleinere Kreuzer einen Handelskrieg gegen die Entente-Mächte, wobei auch zivile Frachter versenkt wurden, da man die Entente verdächtigte, sie für Waffenlieferungen zu missbrauchen (z.&nbsp;B. die Versenkung der RMS Lusitania).\n\n\nNationale Entwicklung.\nDie Macht der Motion Picture Patents Company (MPPC) war 1914 bereits weitgehend gebrochen, die später folgenden Gerichtsurteile waren nur noch Formalitäten. Sowohl die nationale als auch die internationale Konkurrenz der Independents waren also ausgeschaltet. Die US-Filmwirtschaft verlor zwar einen Teil des europäischen Absatzmarktes, doch der Bedarf an frischen Filmen innerhalb der Vereinigten Staaten war höher als in ganz Europa zusammen, so gab es beispielsweise 1916 bereits ca. 28.000 Kinos in ganz Amerika.\nAuch in der übrigen Welt nahmen die Hollywood-Unternehmen eine dominierende Stellung ein, sie stellten zum Beispiel einen Großteil der in Australien und Südamerika gezeigten Filme, die ab ca. 1916 direkt vertrieben wurden (früher war es üblich, an lokale Zwischenhändler zu verkaufen).\n\n\nOligopolisierung.\nNach Robert C. Allen und Douglas Gomery basiert der freie Wettbewerb zwischen Unternehmen auf vier Punkten:\n\n\n\nDas Oligopol der MPPC.\nDer erste Versuch, den freien Wettbewerb zu zerstören und ein Oligopol zu bilden, wurde mittels der Patente betrieben. MPPC versuchte, den Zugang fremder Unternehmen zu behindern, indem sie diesen durch Lizenzgebühren den Wettbewerb erschwerte. Um das System durchzusetzen, sollte zudem eine hohe Marktdurchdringung erfolgen. Auf ihrem Höhepunkt kontrollierte die MPPC via Lizenz den Großteil der Kinos. Auch der Zugang zu Filmmaterial war nicht ohne Lizenz möglich, da Eastman Kodak einen Exklusivvertrag mit der MPPC geschlossen hatte.\n\nDer Edison-Trust attackierte also vor allem die Punkte 2–4. Das System scheiterte endgültig mit der Annullierung der Edison-Patente durch den Obersten Gerichtshof der Vereinigten Staaten, sein Niedergang jedoch hatte schon wesentlich früher begonnen.\n\n\nReaktionen der „Independents“.\nDen freien Zugang zum Filmmaterial erlangten die Independents durch den Bau eigener Kameras und durch die Aufhebung des Patents auf Rohfilme 1912. Und um mit dem Trust konkurrieren zu können, begannen sie, ihre Filme von denen der MPPC unterscheidbar zu machen. Hierbei entstanden der Feature Film und das „Starsystem“.\n\nDie MPPC war zwar nicht blind gegenüber diesen Neuerungen, auch sie drehte Feature Films, durch ihre Struktur und vor allem durch ihre Kundenstruktur, war sie dennoch nicht in der Lage, mit diesen neuen Instrumenten zu experimentieren. Der Trust wollte Massenware verkaufen um eine bestimmte Marge zu erwirtschaften. Teure Stars hätten nur die Kosten hochgetrieben, und Feature Films bargen ein nicht zu unterschätzendes Risiko, für das die Kunden des Trusts nicht aufkommen wollten. So konnten die „Independents“ den ersten Punkt des freien Wettbewerbs unterhöhlen und einzigartige Filmerlebnisse statt austauschbarer Produkte bieten, was dem Publikumsinteresse deutlich entgegenkam und vor allem finanzkräftigere Mittelschichten erschloss.\n\nDer Feature Film kommt ca. 1909 auf und wird nur von den Independents ernsthaft weiterentwickelt, beispielsweise von Famous Players, die später nur noch Features produzieren. Famous Players sind auch die erste Gesellschaft, die das Starsystem konsequent nutzt, nach früheren Versuchen, z.&nbsp;B. von I.M.P.\n\n\nDistribution.\nDurch die oben genannten Schritte schaffen es die Independents, sich eine Position im Markt zu sichern und immer weiter auszubauen. Für nationales und internationales Wachstum fehlen ihnen effiziente Strukturen, zum Beispiel in der Distribution. Noch bis in die Mitte der 1910er Jahre hält sich das alte States-Rights-System, in dem der Produzent lokale Franchise-Rechte seines Films an einen Distributor verkauft, der diese dann innerhalb seines festgelegten Gebiets an Kinos weiter verleiht.\n\nDiese Situation ändert sich erstmals 1914 mit der Fusion von elf regionalen Distributoren zu Paramount, die als erste landesweite Rechte handelt. Durch ihre schiere Größe kann das Unternehmen wesentlich kosteneffizienter arbeiten als die Mitbewerber, ganz abgesehen davon, dass dieses System auch für die Produktionsgesellschaft erhebliche Vorteile mit sich bringt. Das alte System kommt bis 1918 zum Erliegen.\n\n\nVertikale Integration.\nKurz nach ihrer Gründung schließt Paramount Fünfjahresverträge mit Famous Players, Lasky und Bosworth ab, die später auf 25 Jahre verlängert werden. Hier zeichnet sich ein Trend ab, der 1914 zunehmend an Bedeutung gewinnt: Die Verflechtung der bisher getrennten Bereiche Distribution, Produktion und Vorführung, ein Phänomen, das in der Fachliteratur als Vertikale Integration bezeichnet wird. Die Bindung durch die Fünfjahresverträge ist vorteilhaft für alle Beteiligten: Jeder profitiert vom Erfolg des anderen. Wenn das Lasky-Programm sehr gut ist, wird das Paramount-Sortiment von mehr Kinos gekauft, wovon auch Famous Players und Bosworth profitieren, da ihr Programm so auch eine größere Verbreitung findet. Die Kooperation führt dann auch, zwei Jahre später, zur Fusion der genannten und noch einiger weiterer Unternehmen.\n\nDoch es lassen sich durchaus auch frühere Beispiele für vertikale Integration finden. So sind 1912 unter dem Namen Universal erstmals alle drei Bereiche des Filmbusiness vereint. Es fehlte allerdings eine große First-Run-Kinokette. Dennoch schien der Branche die Fusion so bedrohlich, dass die Gründung von Mutual eine direkte Gegenmaßnahme darstellen sollte. Auch hier fanden sich viele Unternehmen unter einem Dach zusammen, denen es explizit nur um Distribution und Produktion ging.\n\nAuch William Fox besitzt 1913 ein Distributions- und ein Produktionsunternehmen, die allerdings erst später zusammengeführt werden. Von Seiten der Kinokettenbesitzer ist zunächst wenig zu hören, erst 1915 schließen sich drei große Ketten, Rowland, Clarke und Mayer, zur Metro Pictures Corporation zusammen, einer Produktionsgesellschaft.\n\n\nKomplette Vertikale Integration.\nDie wirklich große Reaktion der Kinobesitzer kam erst 1917. Zu diesem Zeitpunkt war die fusionierte Paramount zur dominanten Gesellschaft geworden, die ihre Filme mittels Block-Booking vertrieb. Das hieß, um einen Film mit einem Star vom Kaliber einer Mary Pickford zu bekommen, musste man ein komplettes Paket erwerben, dessen große Mehrheit bestenfalls als durchschnittlich zu bezeichnen war. Andererseits konnte man dem Kauf der Pakete schlecht entgehen, wenn man nicht sein Publikum an ein anderes Kino verlieren wollte, das ebendiesen Mary-Pickford-Film zeigte.\n\nUm dieses System zu durchbrechen, schlossen sich 26 der größten nationalen First-Run-Kinokettenbesitzer zum First National Exhibitors Circuit zusammen. Mit ihrer erheblichen Kaufkraft wollten sie gemeinsame Einkäufe tätigen und auch distribuieren. Zuerst war es das Ziel, Stars zu kaufen, ihre Filme zu finanzieren und im Gegenzug das Aufführungsrecht zu erwerben sowie das Recht, die entstandenen Filme regional weiter zu verleihen.\n\nSehr bald kam auch eine eigene Produktion dazu. Zwischen 1917 und 1918 nahm First National Charlie Chaplin und Mary Pickford für jeweils eine Million Dollars unter Vertrag. Beide erhielten vollständige künstlerische Freiheit. First National kontrollierte zu diesem Zeitpunkt bereits ca. 600 Kinos, 200 davon Erstaufführungshäuser.\n\nAus den First-Run-Kinos stammten bis zu 50 Prozent der Einnahmen der Produzenten, außerdem waren Kinos die verlässlichsten Geldverdiener im recht unsteten Filmgeschäft, da das Betreiberrisiko viel geringer war als beispielsweise in der Produktion. Darüber hinaus entschied der Erfolg in den First-Runs über eine lukrative Distribution.\n\nWenn Paramount also seine Abnehmer und sein Publikum nicht verlieren wollte, musste ein Gegenschlag erfolgen. Also stieg die Gesellschaft, mit finanzieller Unterstützung des Bankhauses Kuhn, Loeb & Co., ins Geschäft mit den Kinos ein, anfangs mit einer Summe von 10 Millionen Dollar. Somit wurde Paramount der erste vollintegrierte, oder komplett vertikal integrierte Filmkonzern.\n\n\nDas zweite Oligopol.\nSo wurden aus den alten Independents die Inhaber des zweiten Oligopols. Am Ende der 1910er Jahre war der erste Punkt des freien Wettbewerbs durch das Starsystem und Feature-Filme außer Kraft gesetzt, der zweite Punkt durch die schiere Größe der Unternehmen: Weniger als zehn Unternehmen kontrollierten über 50 Prozent des Marktes. Durch die Vereinigung der Distribution und durch den beginnenden Kampf um die Kinos waren auch die letzten beiden Bedingungen für einen funktionierenden Wettbewerb ausgehebelt.\n\nEin neues Unternehmen konnte weder einen genügenden Zugang zu den Kinos noch Zugriff auf die Stars, also auf die essentiellen Ressourcen der Filmproduktion erhalten. Auch waren die Produktionskosten stark gestiegen. Zwischen 50.000 und 100.000 US-Dollar pro Film waren normal, nach oben gab es keine Beschränkungen. Ein Großteil dieses Geldes floss in die Taschen der Stars, der Rest wurde in bessere Ausstattung investiert, eine weitere Hürde für Neueinsteiger.\n\nUm dem Trend zu höheren Gagen entgegenzuwirken, und um, wie später in einer Anhörung des Obersten Gerichtshofs bekannt wurde, ein Monopol zu errichten, planten First National und Paramount eine Fusion im Wert von 40 Millionen US-Dollar. Es war geplant, mit jedem bedeutenden Kinobesitzer in den Vereinigten Staaten einen Fünf-Jahres-Vertrag abzuschließen. Die Stars hätten dann keine Grundlage mehr für irgendwelche Forderungen gehabt.\n\n\nUnited Artists.\nDie Pläne zu diesem Merger wurden von einem Privatdetektiv aufgedeckt, der im Auftrag von Charlie Chaplin, Mary Pickford, Douglas Fairbanks und D. W. Griffith herausfinden sollte, warum weder First National noch Paramount ihre Verträge verlängerte. Natürlich waren sie entsetzt über solche Aussichten und beschlossen, dem entgegenzuwirken, indem sie ihr eigenes Unternehmen gründeten.\n\n1919 entstand United Artists als Gesellschaft für den Filmvertrieb. Finanziert wurde das Unternehmen durch die Morgan-Gruppe sowie durch eine Einlage von 100.000 US-Dollar für Vorzugs-Anteilscheine durch die Eigentümer. Daneben existierten auch normale Anteilscheine, bei deren Weiterverkauf United Artists ein Vorkaufsrecht hatte.\n\nDie Gesellschaft hatte keine eigenen Studios, sondern nutzte die Studios seiner Mitglieder. Sie war errichtet worden als reine Dienstleistungsgesellschaft, die nicht auf Rendite arbeiten sollte, sondern den Besitzern größtmögliche Autonomie und Profite aus dem Geschäft mit ihren Filmen einräumte. Es gab kein Block-Booking, jeder Film wurde individuell vertrieben und musste allein durch seine künstlerischen Qualitäten überzeugen. Die Verleihgebühren der United Artists lagen deutlich unter denen von First National und Paramount, stellten also eine erhebliche Bedrohung für die marktbeherrschende Stellung der beiden dar.\n\n\nDer Kampf um die Kinos.\nDie Fusion der beiden Giganten war auch gescheitert, weil ihr wichtigstes Kapital, die Stars, sich auf und davon gemacht hatte. First National war also immer noch Konkurrent Paramounts, und die United Artists mit ihren qualitativ sehr hochwertigen Filmen und ihrer enormen Beliebtheit brachten das Unternehmen weiter in Bedrängnis. Also versuchte Paramount das, was man heute eine feindliche Übernahme nennen würde: Stück für Stück wurden die in der First National zusammengeschlossenen Kinoketten aufgekauft.\n\nAuch andere Unternehmen versuchten nun, Kontrolle über die Erstaufführungshäuser zu erlangen, sogar United Artists sah sich später, 1924, mangels Abnehmern gezwungen, eine eigene Kette zu gründen. Wie auch schon in der Vergangenheit, wurden die Kämpfe um die Kinos mit harten Bandagen ausgetragen, vor allem Paramounts „dynamite gang“, auch „wrecking crew“ genannt, wurde ihrem Ruf gerecht. Eine weit verbreitete Methode, Kinos an sich zu binden, war das Blocksystem.\n\n\nZwischen Erstem Weltkrieg und dem Ende der Stummfilmzeit (1918 bis etwa 1930).\n\nDominanz des Weltmarktes.\nSeit 1917 begannen US-amerikanische Unternehmen, ihre Gewinne auf der Basis von in- und ausländischen Verkäufen zu schätzen. Aus dieser Gewinnschätzung ergab sich das Budget der Produktion, das dadurch erhöht wurde, was für die ausländische Konkurrenz doppelt schlecht war. Die Produktionskosten eines Filmes wurden in den Vereinigten Staaten amortisiert, und später wurden die Filme billig im Ausland angeboten, wodurch die internationale Konkurrenz nicht mehr mithalten konnte.\n\nUS-amerikanische Filme galten als qualitativ besser und waren im Erwerb trotzdem günstiger als z.&nbsp;B. deutsche Produktionen. Auch waren die Infrastruktur und die Rationalisierung der Produktionsabläufe nirgends so weit gediehen wie in Hollywood, ein Resultat auch des wachsenden Einflusses der Banken.\n\nAls der Erste Weltkrieg vorbei war, und die Menschen in den bislang abgeschnittenen Ländern wie Deutschland oder Österreich erstmals wieder Hollywood-Produktionen zu sehen bekamen, erlebten sie einen wahren Quantensprung in der Qualität. Die führenden europäischen Filmproduktionsländer, deren isolierte Filmindustrien fünf Jahre lang unter dem Ersten Weltkrieg gelitten hatten, und zudem mit viel geringeren Budgets zu kämpfen hatten, konnten der Konkurrenz aus den Vereinigten Staaten nur noch wenig entgegensetzen. Bis 1927 erhöhte sich der Anteil der amerikanischen Filmproduktion an der Weltfilmproduktion auf nahezu 90 %, was zu Beginn der 1920er Jahre die Filmwirtschaft in England, Frankreich, Italien, Deutschland und Österreich schwer in Bedrängnis brachte und die dortige Filmproduktion stark zurückgehen ließ. Zahlreiche europäische Filmproduktionsgesellschaften mussten schließen. 1925 wurden alleine nach Österreich 1200 US-Produktionen exportiert, obwohl der Bedarf der dortigen Kinos auf lediglich rund 350 geschätzt wurde. In vielen Ländern wurden Filmkontingente eingeführt, die die erlaubte Anzahl an Filmimporten aus den Vereinigten Staaten regelten.\n\nDa rund 45 % der Gewinne zu dieser Zeit aus Europa kamen, wurden die Restriktionen in Europa von den amerikanischen Filmmagnaten mit Argwohn betrachtet. Zumeist erfolglos wurde gegen Einfuhrbeschränkungen Lobbying betrieben. In Ungarn jedoch wurden die geplanten Einfuhrbeschränkungen nicht eingeführt, nachdem die US-amerikanische Filmindustrie den ungarischen Behörden damit gedroht hatte, keine Filme mehr in Ungarn zu zeigen.\n\n\nFilmwirtschaftliche Situation.\n1927 waren nach Zahlen des US-Handelsdepartements beim amerikanischen Film 350.000 Personen beschäftigt. Zur Filmproduktion wurden rund 500.000 Kilometer Filmband verbraucht, wofür mehr Silber benötigt wurde, als der Umlauf an Silbermünzen in den Vereinigten Staaten ausmachte. Es wurden Filme im Ausmaß von 75.000 Kilometer Filmband und einem damaligen Wert von rund 320 Millionen Mark exportiert. Ende des Jahres 1927 zählten die Vereinigten Staaten 21.642 Kinos, die in jenem Jahr insgesamt 3 Milliarden Mal besucht wurden, was wiederum einen Erlös aus dem Eintrittsgeld von rund 2,5 Milliarden Dollar ergab.\n\nWährend Amerika den weltweiten Filmmarkt fast ohne nennenswerte Konkurrenz dominierte, hatten ausländische Produktionen am US-Markt kaum eine Chance. Spielten in manchen Ländern jährlich bis zu 1000 oder mehr US-Filmproduktionen in den Kinos, liefen in den gesamten Vereinigten Staaten im Jahr 1927 nur 65 ausländische Filme, davon 38 aus Deutschland, neun aus England, sechs aus Frankreich, vier aus Russland, je zwei aus Österreich und Italien und je einer aus China und Polen. Selbst diese Filme waren zumeist nur wenig verbreitet und liefen fast ausschließlich auf so genannten Filmkunstbühnen.\n\n\nFrühe Tonfilmära bis Ende des Zweiten Weltkriegs.\nAb 1933, verstärkt jedoch ab Beginn des Zweiten Weltkriegs und der Ausbreitung des Deutschen Reichs auf immer weitere Teile Europas, setzte eine Emigrationswelle von zumeist jüdischen Filmschaffenden aus Europa ein. Waren deren Auswanderungsziele zu Beginn noch häufig europäische Städte mit Filmindustrie wie Wien, Paris oder London, kristallisierte sich bald die aufstrebende Filmindustrie Hollywoods als begehrtestes und vielversprechendstes Ziel der Emigranten heraus&nbsp;– verstärkt durch gezieltes Anwerben europäischer Filmgrößen durch Hollywood-Studiobosse.\n\nVon den etwa 2000 jüdischen Filmschaffenden, die im Deutschen Reich keine Arbeit mehr fanden und auswandern mussten, fanden sich letztendlich rund 800 in Hollywood wieder&nbsp;– darunter fast die gesamte Elite des deutschsprachigen Filmschaffens dieser Zeit. Vielen gelang dort eine ruhmvolle Karriere, viele, vor allem jene, die 1938 und noch später ohne Arbeitsangebot in Hollywood ankamen, konnten nicht mehr an ihre bisherige Karriere anschließen und kamen nur in schlecht bezahlten und unbedeutenden Positionen unter oder mussten nach einer Weile gar das Filmgeschäft aufgeben. Statt der bisher aus Berlin und Wien gewohnten Kaffeehäuser, wo man sich einst regelmäßig traf, wurden nun große Appartements und Villen von in Hollywood erfolgreichen Emigranten neue Treffpunkte. Beliebte Treffpunkte der Film- und Theaterschaffenden waren die Adressen von Henry Koster, Paul Henreid, Ernst Deutsch-Dryden, Paul Kohner und später auch von Sam Spiegel. Die literarische Emigration, inklusive Drehbuchautoren, traf sich häufig bei Salka Viertel und bei Brecht.\n\n\nNach dem Zweiten Weltkrieg.\n\nNew Hollywood.\n\"Hauptartikel: New Hollywood\"\n\n\n\n\nLiteratur.\n\nEnglisch.\nHollywood\n\nExperimentalfilm\n\nDokumentarfilm\n\nIndependent film\n"}
{"id": "51", "url": "https://de.wikipedia.org/wiki?curid=51", "title": "Vorsätze für Maßeinheiten", "text": "Vorsätze für Maßeinheiten\n\nVorsätze für Maßeinheiten, Einheitenvorsätze, \"Einheitenpräfixe\" oder kurz \"Präfixe\" oder \"Vorsätze\" dienen dazu, Vielfache oder Teile von Maßeinheiten zu bilden, um Zahlen mit vielen Stellen zu vermeiden.\n\n\nSI-Präfixe.\nSI-Präfixe sind für die Verwendung im Internationalen Einheitensystem (SI) definierte Dezimalpräfixe. Sie basieren auf Zehnerpotenzen mit ganzzahligen Exponenten. Man unterscheidet zwischen dem Namen des Präfixes und seinem Symbol. Die Symbole sind international einheitlich. Die Namen unterscheiden sich je nach Sprache.\n\nDie Zeichen für Teile einer Einheit werden als Kleinbuchstaben geschrieben, während die meisten Zeichen für Vielfache einer Einheit als Großbuchstaben geschrieben werden. Ausnahmen von dieser Systematik sind aus historischen Gründen die Zeichen für Deka (da), Hekto (h) und Kilo (k).\n\n\nTypographie.\nDie Einheitenvorsatzzeichen werden wie die Einheitenzeichen in aufrechter, nicht kursiver Schrift geschrieben, unabhängig von der Schriftart (Schriftauszeichnung) des umgebenden Textes. Zwischen Einheitenvorsatzzeichen und Einheitenzeichen wird kein Zwischenraum geschrieben.\n\nDas Zeichen „μ“ stammt als einziges Präfix-Symbol aus der griechischen Schrift, was beim Maschinenschreiben und Drucken Schwierigkeiten bereitet hat. In der elektrotechnischen Literatur wurde deshalb ersatzweise häufig ein „u“ verwendet. Das wurde in der Internationalen Norm ISO 2955 von 1983, die 2001 zurückgezogen wurde, auch so empfohlen. Für Deutschland gelten weiterhin die Empfehlungen der DIN-Norm DIN 66030 „Informationstechnik – Darstellung von Einheitennamen in Systemen mit beschränktem Schriftzeichenvorrat“ vom Mai 2002. In Österreich sieht das Maß- und Eichgesetz „μ“ vor. Beim Austausch medizinischer Daten gemäß dem HL7-Standard ist das „u“ anstelle von „μ“ zugelassen.\n\n\n\nSprachliches.\nDer Name eines Einheitenvorsatzes bildet mit dem zugehörigen Einheitennamen ein zusammengesetztes Wort. Beispiele sind \"Nanometer\" oder \"Milligramm.\" Wenn aus dem Zusammenhang klar ist, welche Einheit gemeint ist, wird dieses zusammengesetzte Wort in der Umgangssprache häufig auf den Vorsatz verkürzt. So ist von \"Kilo\" die Rede, wenn \"Kilogramm\" (kg) gemeint ist. Im technischen Bereich wird der Mikrometer (μm) kurz als \"My\" [] bezeichnet; im Englischen ist die Bezeichnung \"micron\" für Mikrometer üblich. Im Österreichischen wird das Kurzwort \"Deka\" für die Masseeinheit \"Dekagramm\" (dag) verwendet, unter Handwerkern auch \"Zenti\" für Zentimeter (cm).\n\nIm Flächenmaß Hektar verschwindet ausnahmsweise an der Wortfügestelle das „o“ von „hekto“, was den Doppelselbstlaut vermeidet.\n\n\nHistorische Präfixe.\nBis 1960 waren in Frankreich die Vorsätze „Myria“ (gr. = zehntausend) mit dem Zeichen ma für das 10‑fache und \"dimi\" mit Zeichen dm für das 10‑fache genormt. Statt \"myria\" wurde Anfang des 19. Jahrhunderts auf einen Vorschlag von Thomas Young hin z.&nbsp;T. auch \"myrio\" geschrieben.\n\nBis um 1900 wurde im Deutschen „Centimeter“ mit C geschrieben.\n\nFrüher waren in Deutschland auch das Symbol D und in Großbritannien dk für \"Deka\" üblich, in Österreich war das Zeichen dk bis Mitte der 1950er Jahre gesetzlich vorgeschrieben.\n\nIn DIN 1301 Teil 1 vom Dezember 1993 wurde der SI-Vorsatz für 10 „Yocto“ geschrieben; diese Schreibweise wurde in der Ausgabe vom Oktober 2002 zu „Yokto“ korrigiert.\n\n\nGeplante Erweiterungen.\nDa sich die Größenordnungen, in denen inzwischen gearbeitet wird deutlich vergrößert haben, werden weitere Vorsätze diskutiert, wie z.&nbsp;B. Ronna (10) und Quecca (10), bzw. Ronto (10) und Quecto (10).\n\n\nPräfixe für Werte von elektrischen Bauteilen.\nBis 1950 wurden die elektrische Kapazität von Kondensatoren, aber auch die Selbstinduktion von Spulen in cm (Centimeter) des CGS-Einheitensystems angegeben. Bisweilen wurde pF&nbsp;(Piko-Farad) auch als μμF geschrieben. Der besseren Lesbarkeit wegen findet sich auf oft kleinen Bauteilen statt μF gelegentlich uF (siehe dazu Abschnitt \"Typographie\"), MF oder (im Englischen) MFD.; geläufig ist auch KV statt kV für Spannung und MEGOHM statt MΩ für Widerstand.\n\n\nUmgangssprachliche Verwendung.\nIm Sprachgebrauch von Internetbenutzern wird mitunter das SI-Präfix k in unüblichen Kontexten verwendet, z.&nbsp;B. bei Zeit- und Stückzahlangaben. Vergleiche auch den besonders speziellen Fall der Bezeichnung Y2K für das Jahr-2000-Problem oder W2K für Microsoft Windows 2000. Im kaufmännisch-technischen Umfeld wird das Präfix k außerdem häufig mit Währungseinheiten verwendet, etwa als k€. Die dort ebenfalls verwendete Kombination T€ stammt nicht aus dem SI, sondern bedeutet „Tausend Euro“.\n\n\nEinheitenvorsätze für binäre Vielfache.\nIn der Datenverarbeitung werden SI-Präfixe auch für Datenmengen (Bits und Bytes) verwendet, allerdings oft in der Bedeutung als Binärpräfix (Vielfache von 1024, z.&nbsp;B. 2, 2, 2 usw.). Bis heute werden bei Datenmengen je nach Kontext, unter Umständen je nach betrachtetem Speichermedium, die SI-Präfixe als Dezimalpräfixe oder Binärpräfixe verwendet, was insbesondere bei höheren Werten zu erheblichen Abweichungen führt.\n\nDie für die Normung in der Elektrotechnik zuständige International Electrotechnical Commission hat daher zuerst in der Norm IEC 60027-2 (ersetzt durch IEC 80000-13:2008) besondere, an die SI-Präfixe angelehnte Binärpräfixe gemäß unten stehender Tabelle definiert und empfiehlt deren Verwendung für Datenmengen. Die dezimalen SI-Präfixe sollen bei Datenmengen das gleiche bedeuten wie bei SI-Einheiten (Dezimalpräfixe). Das für die SI-Präfixe zuständige Internationale Büro für Maß und Gewicht (BIPM) empfiehlt ebenfalls die Anwendung dieser Norm:\n\nDas binäre Präfixsymbol entsteht durch Anhängen von -i an das entsprechende dezimale Präfixsymbol (Ki wird dabei im Gegensatz zu k groß geschrieben). Der Name eines binären Präfixes wird dagegen durch Anhängen von -bi an die ersten beiden Buchstaben des entsprechenden dezimalen Präfixnamens gebildet.\n\nBeispiel für die Verwendung: Ein Kibibyte wird geschrieben als 1&nbsp;KiB = 2 B = 1024&nbsp;B, wobei B für Byte steht.\n\n"}
{"id": "53", "url": "https://de.wikipedia.org/wiki?curid=53", "title": "Abkürzungen/Gesetze und Recht", "text": "Abkürzungen/Gesetze und Recht\n\nViele, aber nicht alle Fachautoren kürzen ohne Punkt und Leerzeichen ab: So wird die Abbreviatur für \"in der Regel\", normalerweise \"i.&nbsp;d.&nbsp;R.\", mit \"idR\" abbreviert.\nUngeachtet dessen werden Akronyme, wie sonst auch, ohne Punkte gebildet.\nFolgende Liste von Abkürzungen aus der Rechtssprache versucht nur sprachlich korrekte Abbreviaturen, also mit Punkt und Leerzeichen, aufzuführen, sofern nicht die geraffte Schreibweise in den Alltag Einzug gefunden hat und dort dominiert (z.&nbsp;B. \"eG\", \"GbR\" oder \"MdB\") oder die Abbreviatur nur in juristischen Schriften und dort auch nur zusammengeschrieben auftaucht.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "54", "url": "https://de.wikipedia.org/wiki?curid=54", "title": "Liste von Abkürzungen (Computer)", "text": "Liste von Abkürzungen (Computer)\n\nDies ist eine Liste technischer Abkürzungen, die im IT-Bereich verwendet werden.\n\n\n"}
{"id": "56", "url": "https://de.wikipedia.org/wiki?curid=56", "title": "Liste von Unternehmen mit Namensherkunftserklärungen", "text": "Liste von Unternehmen mit Namensherkunftserklärungen\n\nDies ist eine Liste von Firmen (Namen der Unternehmen) sowie ihrer Herkunft (Etymologie). Häufig sind die Firmen Abkürzungen oder Akronyme der Namen der Gründer, voriger Gesellschaften oder des Unternehmensziels.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "61", "url": "https://de.wikipedia.org/wiki?curid=61", "title": "ISO 4217", "text": "ISO 4217\n\nISO 4217 ist die von der Internationalen Organisation für Normung publizierte Norm für Währungs-Abkürzungen, die im internationalen Zahlungsverkehr zur eindeutigen Identifizierung benutzt werden sollen. Am 1. August 2015 wurde die neue Version ISO 4217:2015 veröffentlicht. Diese 8.&nbsp;Version ersetzt den Vorgänger aus dem Jahr 2008.\n\n\nSystematik.\n\nAlphabetische Codes.\nDie Abkürzungen umfassen jeweils drei Buchstaben. Die ersten beiden sind üblicherweise die Landeskennung nach ISO&nbsp;3166-1 ALPHA-2 (beispielsweise \"AU\" für Australien), der letzte Buchstabe ist in der Regel der Anfangsbuchstabe des Währungsnamens, so beispielsweise \"D\" für \"Dollar\". Gemeinsam ergibt dies \"AUD\" als genormte Abkürzung für den Australischen Dollar.\n\nWährungen, die nicht von einem Einzelstaat herausgegeben werden, haben als ersten Buchstaben ein \"X\"; die beiden folgenden Buchstaben geben den Namen der Währung an. Dies ist sowohl bei den meisten Währungsunionen der Fall (z.&nbsp;B. der Ostkaribische Dollar (\"XCD\")), als auch bei den IWF-Sonderziehungsrechten (\"XDR\").\n\nVon diesen Regeln wird in den folgenden Fällen abgewichen:\n\nAuch für nicht-geldliche Wertaufbewahrungs- und Transaktionsmittel gibt es Kodierungen. So wird eine Feinunze Gold (=&nbsp;31,1034768 Gramm) beispielsweise mit \"XAU\" abgekürzt (zusammengesetzt aus codice_1 und dem chemischen Symbol für Gold: \"Au\"), Silber entsprechend mit \"XAG\". Transaktionen, in denen keine Währung verwendet wird, werden mit \"XXX\" gekennzeichnet.\n\n\nNumerische Codes.\nNeben der Buchstabenkodierung werden auch dreistellige Zifferncodes verwendet. Dabei bedeuten die Zahlenbereiche\n\nTeilweise wird bei Änderung der Währung und des Buchstaben-Codes die bisherige numerische Kodierung beibehalten, insbesondere wenn sich lediglich der Name des Zahlungsmittels geändert hat.\nAuch wenn eine direkte Währungsumstellung erfolgt, bleibt meist der numerische Code unverändert.\nDiese Codes sind deshalb ohne Kenntnis des Zeitpunkts nicht immer so eindeutig einer bestimmten Währung des betreffenden Landes zuzuordnen, wie das mit den Buchstaben-Codes möglich ist. Allerdings sind sie für konkrete finanzielle Transaktionen vorgesehen, so dass die Angabe bei mehreren Möglichkeiten eigentlich nur bedeutet: „In der am Tag der Wertstellung gültigen Landeswährung“.\n\n\nListen der Währungscodes.\nDer Standard definiert drei Listen, die selbst im Standardtext nicht enthalten sind, sondern auf der ISO-Website in ihrer jeweils aktuellen Fassung zur Verfügung gestellt werden:\n\n"}
{"id": "76", "url": "https://de.wikipedia.org/wiki?curid=76", "title": "Achsensprung (Film)", "text": "Achsensprung (Film)\n\nEin Achsensprung ist ein Filmschnitt, mit dem die Beziehungsachse der Figuren oder Gruppen übersprungen wird. Blickachsen\noder Beziehungsachsen zwischen den Akteuren untereinander oder dem Point of Interest des Protagonisten bilden eine gedachte Linie. Auf die Leinwand projiziert, stellt diese Linie eine „links-rechts-“ und „oben-unten-Beziehung“ zwischen den Akteuren dar. Mit Achsensprung bezeichnet man einen Schnitt, bei dem sich dieses Verhältnis umkehrt. Es wird zwischen Seitenachsensprung und dem Höhenachsensprung unterschieden. Letzterer wird als weniger desorientierend vom Zuschauer empfunden, da die Leinwand weniger hoch als breit ist. \nEin Achsensprung kann beim Zuschauer Desorientierung verursachen, da die Anordnung und Blickrichtung der Akteure im Frame sich relativ zum Zuschauer zu verändern scheint.\n\nAktionsachse (Handlungsachse)\nist die gedachte Linie, in deren Richtung sich die Handlung oder das Inertialsystem der Filmwelt bewegt. Bei einer Autofahrt zum Beispiel ist die Aktionsachse so stark, dass die Beziehungsachsen an Bedeutung verlieren. Die Orientierung bleibt trotz eventuellem Achsensprung bewahrt. Wenn man aus der Fahrerseite filmt, bewegt sich die Landschaft scheinbar von rechts nach links; filmt man aus der Beifahrerseite, bewegt sie sich scheinbar von links nach rechts. Diese Änderung der Bewegungsrichtung ist aber nicht irritierend. Analog werden zwei Autos, die bei einer Parallelmontage in die gleiche Richtung fahren (oft von links nach rechts, weil das unserer Leserichtung entspricht), als einander verfolgend wahrgenommen; wenn eines jedoch von links nach rechts und das andere von rechts nach links fährt, erwartet der Zuschauer einen Zusammenstoß.\n\nIm Continuity Editing des klassischen Hollywoodkinos wird der Achsensprung als Fehler betrachtet und dementsprechend vermieden. \n\nDer Grundsatz, Achsensprünge zu vermeiden, wird \"180-Grad-Regel\" genannt.\n\n\nBewusster Achsensprung.\nIn manchen Fällen kann ein bewusster Achsensprung auch Stilmittel sein, um beispielsweise Verwirrung oder einen Kippmoment zu symbolisieren; Stanley Kubrick wird in diesem Zusammenhang häufig genannt. In Werbespots werden Achsensprünge oft verwendet, um einen rasanten Effekt zu bewirken. Bekannt ist auch eine Szene aus \"Herr der Ringe\", in welcher Sméagol mit sich selbst spricht. Da er mit den Schnitten wechselnd von der einen zur anderen Seite spricht (Achsensprung), entsteht der Eindruck zweier gleich aussehender Personen, womit der gespaltene Charakter der Figur unterstrichen wird.\n\n\nAchsenwechsel.\nIm Gegensatz zum Achsensprung handelt es sich hierbei um eine Bewegung der Kamera (Steadicam oder einer Dollyfahrt) über die Achse oder um eine Änderung der Bewegungsachse bzw. der Blickrichtung der Figuren, wodurch eine neue Achse definiert wird. Der Achsenwechsel wird vom Zuschauer nicht als störend wahrgenommen, weil sich die Bewegung fließend vollzieht. Diese Bewegung wird mitunter auch als Crab bezeichnet. Außerdem kann ein Zwischenschnitt in eine Totale eine Achsenüberschreitung möglich machen, da so die räumliche Anordnung der Akteure für den Zuschauer deutlich wird, oder der Zwischenschnitt auf einen Closeup, da sich der Betrachter danach wieder neu räumlich orientiert.\n\n\n"}
{"id": "77", "url": "https://de.wikipedia.org/wiki?curid=77", "title": "Alfred Hitchcock", "text": "Alfred Hitchcock\n\nSir Alfred Joseph Hitchcock KBE (* 13. August 1899 in Leytonstone, England; † 29. April 1980 in Los Angeles) war ein britischer Filmregisseur, Drehbuchautor, Filmproduzent und Filmeditor. Er siedelte 1939 in die USA über und nahm am 20. April 1955 zusätzlich die US-amerikanische Staatsbürgerschaft an. Er wurde durch Filme wie \"Rebecca\" (1940), \"Berüchtigt\" (1946), \"Cocktail für eine Leiche\" (1948), \"Der Fremde im Zug\" (1951), \"Bei Anruf Mord\" (1954), \"Das Fenster zum Hof\" (1954), \"Vertigo\" (1958), \"Der unsichtbare Dritte\" (1959), \"Psycho\" (1960) und \"Die Vögel\" (1963) bekannt.\n\nHitchcock gilt hinsichtlich seines Stils bis heute als einer der einflussreichsten Filmregisseure. Er etablierte die Begriffe Suspense und MacGuffin. Sein Genre war der Thriller, dessen Spannung er mit Humor verband. Die wiederkehrenden Motive seiner Filme waren Angst, Schuld und Identitätsverlust. Mehrfach variierte er das Thema des unschuldig Verfolgten.\n\nHitchcock legte großen Wert auf die künstlerische Kontrolle über das Werk des Autors. Sein Gesamtwerk umfasst 53 Spielfilme und gehört – gemessen am Publikumserfolg sowie der Rezeption durch Kritik und Wissenschaft – zu den bedeutendsten der Filmgeschichte. Auch dank seiner bewussten Selbstvermarktung zählt Hitchcock heute zu den bekanntesten zeitgeschichtlichen Persönlichkeiten. Er ist dem Autorenfilm zuzurechnen.\n\nAm 3. Januar 1980 wurde er von Königin Elisabeth&nbsp;II. zum Knight Commander des Order of the British Empire ernannt.\n\n\nLeben und Werk.\n\nKindheit, Jugend und Ausbildung.\nAlfred Hitchcock war der jüngste Sohn des Gemüsehändlers William Hitchcock (1862–1914) und dessen Ehefrau Emma Jane Whelan (1863–1942). Durch den Altersunterschied von sieben beziehungsweise neun Jahren zu seinen Geschwistern, durch seine römisch-katholische Erziehung in einem von der anglikanischen Kirche geprägten Land und nicht zuletzt durch sein Äußeres – er war klein und schon als Kind korpulent – hatte er eine einsame Kindheit.\nZwischen 1910 und 1913 war er Schüler des St.-Ignatius-College, einer Londoner Jesuitenschule. Er verließ das College mit knapp 14&nbsp;Jahren und besuchte stattdessen Abendkurse auf der Londoner Universität, diverse Handwerkskurse und später wenige Monate lang die \"School of Engineering and Navigation\". Zudem belegte er Kurse in technischem Zeichnen sowie in Kunstgeschichte an der Londoner Kunstakademie. Seine Freizeit verbrachte er großteils mit dem Lesen von Fahrplänen und dem Studium von Stadtplänen und Landkarten. Mit fortschreitendem Alter flüchtete er sich in Romane, besuchte Theatervorstellungen und ging oft ins Kino. Außerdem verfolgte er Mordprozesse im Gerichtshof Old Bailey und besuchte gerne das Black Museum von Scotland Yard. Der Tod des Vaters Ende 1914, zu dem er kein enges Verhältnis hatte, band Hitchcock noch enger an seine Mutter.\n\n1915 nahm er eine Stelle als technischer Angestellter bei der \"W.T. Henley Telegraph Company\" an, die elektrische Leitungen herstellte. Wegen seines zeichnerischen Talents wurde er bald in die Werbeabteilung versetzt. Unter seinem bis zuletzt gebrauchten Spitznamen „Hitch“ veröffentlichte er in der Betriebszeitschrift seine ersten gruseligen Kurzgeschichten.\n\n\nAnstellung beim Film.\nIm Frühjahr 1920 hörte Hitchcock von der Neugründung eines Studios der amerikanischen Produktionsgesellschaft Paramount Famous Players-Lasky im Londoner Stadtbezirk Islington. Er bewarb sich mit einer Mappe mit Illustrationen und wurde als Zeichner von Zwischentiteln angestellt. In den Jahren 1921 und 1922 zeichnete er die Titel für mindestens zwölf Filme. Nebenbei entwarf er Kostüme, Dekorationen und Szenenbilder. Auch durch Überarbeitungen von Drehbüchern machte er auf sich aufmerksam. Bei zwei Filmen arbeitete er mit George Fitzmaurice zusammen, dessen genaue Produktionsplanung ihn sehr beeinflusste.\n\n1922 bekam Hitchcock Gelegenheit, sich als Regisseur zu versuchen. Mit dem Autor Seymour Hicks stellte er die letzten Filmszenen von \"Always Tell Your Wife\" fertig, nachdem der ursprüngliche Regisseur gefeuert worden war. Bald darauf konnte er einen eigenen Film drehen, \"Number 13\" (in einigen Quellen \"Mrs. Peabody\"), der jedoch unvollendet blieb, da \"Famous Players-Lasky\" im Laufe der Dreharbeiten das Studio wegen finanzieller Schwierigkeiten schließen musste. Das leerstehende Gelände wurde an unabhängige Produzenten vermietet, darunter auch an Michael Balcon, der das Studio 1924 schließlich erwarb. Er stellte Hitchcock als Regieassistent ein, sowie (auf dessen Empfehlung) die Filmeditorin Alma Reville. Die beiden kannten sich seit 1921, seitdem sie gelegentlich an denselben Filmen gearbeitet hatten. Bis 1925 entstanden fünf Filme, bei denen Hitchcock dem Regisseur Graham Cutts assistierte und zu Cutts’ wachsendem Unmut mehr und mehr künstlerischen Einfluss gewann. Neben dem Drehbuch kümmerte er sich auch um die Bauten, das Szenenbild, die Besetzung, die Kostüme sowie die Ausstattung und nahm so mit der Zeit die Aufgaben eines Produktionsleiters wahr.\n\nHitchcocks letzte Zusammenarbeit mit Graham Cutts führte ihn 1924/25 nach Deutschland. Der unter der Beteiligung der deutschen UFA produzierte Film \"Die Prinzessin und der Geiger\" entstand in den Babelsberger Filmstudios – damals die modernsten der Welt. Dabei hatte Hitchcock die Möglichkeit, Friedrich Wilhelm Murnau bei den Arbeiten an \"Der letzte Mann\" zu beobachten; von diesem beeindruckt übernahm er einige Techniken Murnaus für die Szenenbilder seiner aktuellen Produktion. Durch diesen und weitere Besuche konnte Hitchcock fließend Deutsch sprechen; später sprach er zum Beispiel einige Trailer seiner Filme selbst.\n\nZurück in England übertrug ihm Michael Balcon 1925 die Regie für einen eigenen Film. Das Projekt führte den jungen Hitchcock wieder nach Deutschland. Nur die Münchner Lichtspielkunst (Emelka) fand sich bereit, den Film des unbekannten Regie-Debütanten mitzuproduzieren. Für das Melodram \"Irrgarten der Leidenschaft\" (1925) verpflichtete Balcon kostspielige Stars aus Hollywood. Alma Reville, mittlerweile Hitchcocks Verlobte, war als Regieassistentin und Editorin Mitglied des sehr kleinen Filmteams. Balcon war mit Hitchcocks ambitionierter Arbeit zufrieden und vertraute ihm eine weitere deutsch-englische Koproduktion an: \"Der Bergadler\" wurde noch im selben Jahr, diesmal in Tirol, gedreht. Doch beide Filme, die 1925 beziehungsweise 1926 in Deutschland in den Kinos anliefen, wurden in England zunächst nicht veröffentlicht. Der englische Verleiher und Geldgeber C.&nbsp;M. Woolf war, im Gegensatz zu Balcon, nicht von Hitchcocks betont expressionistischem Stil überzeugt. \"Der Bergadler\" ist der einzige von Hitchcocks Filmen, der nicht mehr erhalten ist.\n\n\nKarriere in England.\n\nLeben und Arbeit in England.\nMit dem 1926 gedrehten Film \"Der Mieter\" um einen einzelgängerischen Pensionsgast, der verdächtigt wird, ein Serienmörder zu sein, hatte Hitchcock sein Thema gefunden. Doch nicht zuletzt wegen dessen expressionistischer Bildgestaltung lehnte es Woolf abermals ab, den Film zu veröffentlichen. Balcon zog daraufhin den jungen Ivor Montagu hinzu, der Erfahrung mit Filmüberarbeitungen hatte; mit Hitchcock zusammen wurden einige Änderungen vorgenommen. Der überragende Erfolg bei einer Pressevorführung ebnete dann den Weg zur Veröffentlichung seiner ersten beiden Filme. \"Der Mieter\" kam 1927 in kurzer Abfolge mit \"Irrgarten der Leidenschaft\" und \"Der Bergadler\" in die Kinos und bedeutete für Hitchcock den Durchbruch als Regisseur.\n\nFür Balcons Gainsborough Pictures drehte Hitchcock 1927 noch die zwei Melodramen \"Abwärts\" und \"Easy Virtue\". Beiden Filmen war kein Erfolg beschieden. Bereits zuvor hatte er beschlossen, bei deutlich höherem Gehalt zu der neu gegründeten Firma British International Pictures (BIP) des Produzenten John Maxwell zu wechseln. Dort entstand mit dem Boxerdrama \"Der Weltmeister\" sein erster Film nach einem Originaldrehbuch. Die Presse reagierte äußerst positiv. Obwohl die drei folgenden Stummfilme \"The Farmer’s Wife,\" \"Champagne\" und \"Der Mann von der Insel Man\" abgesehen von einzelnen Szenen als Fingerübungen gelten, hatte sich Hitchcock in Großbritannien innerhalb kurzer Zeit einen Namen gemacht: Die junge britische Filmindustrie, sehr darauf bedacht, sich von der amerikanischen abzuheben, war nur allzu gerne bereit, ihn als kommenden Regiestar zu feiern.\n\nIm Dezember 1926 heirateten Alfred Hitchcock und Alma Reville, die für die Hochzeit zum katholischen Glauben konvertierte. 1928 wurde ihre gemeinsame Tochter Patricia geboren. Beruflich blieb Alma bis zum Schluss seine engste Mitarbeiterin und Beraterin.\n\nDas Aufkommen des Tonfilms hielten viele Regisseure für das Ende ihrer Kunstform. Hitchcock hingegen nutzte das Potential der neuen Technik. \"Erpressung\" (1929) wurde ursprünglich als Stummfilm produziert. Die Produzenten erlaubten Hitchcock jedoch, eine Filmrolle mit Tonmaterial nachzudrehen. Er versah daraufhin einzelne Schlüsselszenen mit wirkungsvollen Toneffekten und gesprochenem Dialog, wobei die tschechische Schauspielerin Anny Ondra, die ihre Rolle stumm spielen musste, von der englischen Schauspielerin Joan Barry simultan synchronisiert wurde. \"Erpressung\" war der erste britische Tonfilm und wurde ein großer Erfolg. Hitchcock nutzte seine gewonnene Popularität und gründete mit der \"Hitchcock Baker Productions Ltd.\" eine Gesellschaft zur Vermarktung seiner Person.\n\nAuf Geheiß seines Studios drehte er \"Juno and the Paycock\" (1930) sowie einige Szenen für die Musikrevue \"Elstree Calling\". Mit \"Mord – Sir John greift ein!\" fand er wieder zu seinem Thema und auch nach Deutschland zurück: In Berlin stellte er die deutsche Sprachversion des Films unter dem Titel \"Mary\" her. Es folgten drei Filme, von denen Hitchcock nur die Komödie \"Endlich sind wir reich\" wirklich interessierte: In dem zusammen mit seiner Frau und Val Valentine verfassten Drehbuch verarbeitete er unter anderem die Erfahrungen seiner noch jungen Ehe. Den ihm aufgezwungenen Thriller \"Nummer siebzehn\" beschloss Hitchcock aus Protest zu sabotieren und zu einer wirren, albernen Parodie zu machen. Die turbulente Verbindung zwischen Humor und Spannung lässt \"Nummer siebzehn\" aus heutiger Sicht als einen Vorläufer späterer Klassiker des Genres erscheinen. Hitchcocks Vertrag mit der \"British International Pictures\" endete nach sechs Jahren mit einem Einsatz als Produzent \"(Lord Camber’s Ladies)\". Die Zusammenarbeit hatte zunehmend unter dem Konflikt zwischen Hitchcocks Streben nach künstlerischer Kontrolle und den Vorschriften des Studios gelitten. Doch auch den folgenden Film \"Waltzes from Vienna\", ein „Musical ohne Musik“ (Hitchcock) für den unabhängigen Produzenten Tom Arnold, drehte er betont lustlos: „Ich hasse dieses Zeug. Melodrama ist das einzige, was ich wirklich kann.“\n\n\nEnglische Meisterwerke.\nUnmittelbar nach \"Waltzes from Vienna\" nahm er die fruchtbare Zusammenarbeit mit dem Produzenten Michael Balcon wieder auf. Als erster Film für die \"Gaumont British\" entstand der Thriller \"Der Mann, der zuviel wußte\" (1934). Das Drehbuch erarbeitete Hitchcock im Wesentlichen mit seiner Frau Alma und dem Drehbuchautor Charles Bennett. Der Film wurde sowohl von der Kritik als auch vom Publikum enthusiastisch aufgenommen. Der humorvolle Spionagethriller \"Die 39&nbsp;Stufen\" (1935, Drehbuch: Charles Bennett) gilt als Blaupause späterer Verfolgungsthriller. Eine turbulente Szene folgt auf die nächste, es gibt keine Übergänge und kaum Zeit für den Zuschauer, über die manches Mal fehlende Logik nachzudenken. Hitchcock ordnete nach eigenem Bekunden alles dem Tempo unter. Der überragende Erfolg des Films sollte ihm recht geben. Es folgten \"Geheimagent\" (1936) und \"Sabotage\" (1936), die insbesondere in Hitchcocks eigener späterer Bewertung gegenüber den beiden Vorgängerfilmen abfielen. Doch die psychologisch vielschichtige Behandlung des Themas „Schuld“ weist bereits auf spätere Werke hin.\n\nNach \"Sabotage\" endete abrupt die zweite erfolgreiche Phase der Zusammenarbeit mit Michael Balcon, als die Produktionsfirma Gaumont British von deren Besitzern geschlossen und Balcon entlassen wurde. Die beiden folgenden Filme drehte Hitchcock daher wieder für die Gainsborough Pictures – diesmal allerdings ohne seinen ehemaligen Förderer. \"Jung und unschuldig\" (1937) war eine weitere, unbeschwerte Variation der Geschichte vom unschuldig Verfolgten. Der gefeierte Thriller \"Eine Dame verschwindet\" (1938) spielt überwiegend in einem fahrenden Zug. Die Dreharbeiten fanden jedoch ausschließlich in einem kleinen Londoner Studio statt, was dank technisch anspruchsvoller Rückprojektionen möglich wurde.\n\nHitchcock festigte mit diesen sechs Filmen seine Ausnahmestellung innerhalb des britischen Kinos. Ende der 1930er Jahre beauftragte er die \"Selznick-Joyce-Agentur\", deren Mitinhaber Myron Selznick, der ältere Bruder des Hollywood-Moguls David O. Selznick, war, seine Interessen wahrzunehmen. Hitchcock, dessen Ruf mittlerweile bis nach Hollywood gelangt war, unterzeichnete schließlich 1938 einen Vertrag für die Produktionsgesellschaft von David O. Selznick, der damals gerade mit der Vorproduktion zu \"Vom Winde verweht\" beschäftigt war. In Gedanken bereits in Hollywood, drehte Hitchcock in England noch einen letzten Film für die Produktionsfirma des nach England emigrierten deutschen Produzenten Erich Pommer. Doch der Kostümfilm \"Riff-Piraten\" wurde von der Presse durchweg verrissen.\n\n\nHollywood und der Zweite Weltkrieg.\nIn seinen ersten Jahren in Hollywood stieß Hitchcock auf unerwartete Schwierigkeiten. David O. Selznick übte starke Kontrolle über die Filme seines Studios aus und achtete darauf, dass sich der freiheitsliebende Hitchcock möglichst eng an die literarische Vorlage seines ersten Hollywoodfilmes hielt. Trotz dieser Spannungen wurde \"Rebecca\" für den britischen Regisseur ein erfolgreicher Einstand in Hollywood: Das psychologisch dichte und düster-romantische Melodram war 1940 elfmal für den Oscar nominiert und gewann schließlich zwei der Trophäen (Kamera und Produktion).\n\nIn den nächsten Jahren machte Selznick sein Geld mit Hitchcock, indem er ihn für beträchtliche Summen an andere Studios auslieh. Der Krieg in Europa weitete sich aus, als der unabhängige Produzent Walter Wanger Hitchcock für ein aktuelles Kriegsdrama engagierte. \"Der Auslandskorrespondent\" blieb Hitchcocks Naturell entsprechend jedoch ein weitgehend unpolitischer Spionagethriller. Nur der nachgedrehte Schlussmonolog, gerichtet an die noch neutralen USA, wirkte aufrüttelnd. Kurz nach Fertigstellung des Films wurde England von Deutschland bombardiert. Der rechtzeitig ausgewanderte Hitchcock musste sich daraufhin scharfe Kritik von ehemaligen britischen Kollegen, allen voran Michael Balcon, gefallen lassen.\n\nMit \"Verdacht\" (1941, RKO), der ersten Zusammenarbeit mit Cary Grant, und \"Saboteure\" (1942, Universal) blieb Hitchcock bei seinen klassischen Themen. Zwischen diesen Produktionen drehte er, für ihn und andere ungewohnt, seine einzige Screwball-Komödie. Obwohl damals durchaus positiv aufgenommen, zeigte er sich mit \"Mr. und Mrs. Smith\" (1941, RKO) nicht zufrieden.\nWeit mehr am Herzen lag ihm die Arbeit an dem Drama \"Im Schatten des Zweifels\" (1943, Universal). Hitchcocks Filme gelten allgemein als stark von seinem Charakter geprägt. Dieses Familienmelodram wird als einer seiner persönlichsten Filme bezeichnet: In allen Hauptfiguren spiegeln sich demnach Eigenschaften und Ängste Hitchcocks. Als während der Dreharbeiten Hitchcocks Mutter in London starb, verstärkte dies die autobiografischen Tendenzen.\n\nWie viele britische Regisseure leistete Hitchcock seine Beiträge für die Kriegspropaganda und drehte unter anderem Kurzfilme zur Unterstützung der französischen Résistance. Auch in seine nächste Hollywood-Produktion arbeitete er stark propagandistische Töne ein, doch sein stets bewusst irritierender Umgang mit Klischees sorgte diesmal für Kontroversen: In einem kleinen Rettungsboot sehen sich englische und amerikanische Schiffbrüchige einem intellektuell überlegenen Nazi gegenüber. Dennoch wurde der formalistisch strenge Psychothriller \"Das Rettungsboot\" (1943, 20th Century Fox) dreimal für den Oscar nominiert (Drehbuch, Kamera und Regie).\n\nPsychologie, wichtige Komponente seines Werks, stand im Mittelpunkt von \"Ich kämpfe um dich\" (1945), der nach langer Zeit wieder für Selznick entstand. Dieser war vom Thema Psychoanalyse schnell begeistert und ließ Hitchcock ungewohnt viel freie Hand, doch kürzte er den Film nach der ersten Probevorführung um rund zwanzig Minuten. Die erfolgreiche Zusammenarbeit mit Ingrid Bergman in der Hauptrolle wurde in der folgenden Produktion \"Berüchtigt\" (1946) fortgesetzt, die Selznick allerdings wieder an RKO verkaufte. Die Geschichte um eine Spionin (Bergman), die aus Pflichtgefühl von ihrem Liebhaber (Cary Grant) gedrängt wird, mit dem Feind zu schlafen, bot für Hitchcocks Obsessionen eine breite Projektionsfläche.\n\nMit dem Gerichtsdrama \"Der Fall Paradin\" (1947) lief der Vertrag Hitchcocks mit Selznick aus. Selznick behielt, bei der Stoffauswahl angefangen, bei dieser relativ chaotisch verlaufenden Produktion die Oberhand. Dass Hitchcock währenddessen Vorbereitungen für seine eigene Produktionsfirma traf, verstärkte die Spannungen zwischen den machtbewussten Männern. Dennoch bot Selznick Hitchcock – erfolglos – eine Vertragsverlängerung an.\n\n\nUnabhängigkeit.\nBereits im April 1946, rund zwei Jahre bevor der Vertrag mit Selznick auslaufen sollte, gründete Hitchcock mit dem befreundeten Kinokettenbesitzer Sidney Bernstein die Produktionsfirma Transatlantic Pictures, für die er seinen ersten Farbfilm inszenierte, \"Cocktail für eine Leiche\" (1948) mit James Stewart in einer der Hauptrollen. Der Film blieb jedoch vor allem wegen eines anderen Hitchcock-Experiments in Erinnerung; jede Einstellung des kammerspielartigen Films dauert so lange, wie es das Filmmaterial in der Kamera erlaubte, also rund zehn Minuten. Durch geschickte Übergänge sollte so der Eindruck entstehen, dass sich die Geschichte in Echtzeit und von nur einer Kamera gefilmt ereignete.\n\n\"Sklavin des Herzens\" (1949), ein für Hitchcock untypischer, melodramatischer Kostümfilm, war vor allem ein Vehikel für Ingrid Bergman. Trotz der Starbesetzung und der technischen Raffinessen wurde er kommerziell ein ähnlicher Misserfolg wie \"Cocktail für eine Leiche\" – Transatlantic ging daraufhin in Konkurs.\n\nNachdem sein Berater und Agent Myron Selznick 1944 gestorben war, wurden Hitchcocks Interessen von mehreren anderen Personen wahrgenommen, bevor er 1948 mit Lew Wasserman zusammentraf. Wasserman war seit 1946 Präsident der weltgrößten Künstleragentur Music Corporation of America (MCA), der sich Hitchcock 1948 anschloss. Es begann eine enge wie äußerst lohnende Zusammenarbeit.\n\n\nWarner Brothers.\nHitchcock schloss mit Warner Bros. einen lukrativen Vertrag über vier Filme ab, bei denen er als Regisseur und Produzent, angefangen bei der Stoffauswahl, völlig freie Hand hatte. Der erste dieser Filme war der Thriller \"Die rote Lola\" (1950) mit Marlene Dietrich, der im Londoner Theatermilieu spielte. Eines seiner Lieblingsmotive stellte er auf den Kopf; am Ende entpuppt sich der „unschuldig Verfolgte“ als der wahre Mörder. Hitchcock drehte in seiner Heimat, spürte allerdings wieder die alten Ressentiments, die nach seiner Auswanderung entstanden waren. Der Film selbst war nicht sonderlich erfolgreich.\n\nIm April 1950 begann Hitchcock, regelmäßige Kolloquien an den Universitäten von Kalifornien und Südkalifornien abzuhalten, in denen unter anderem Previews seiner aktuellen Filme gezeigt wurden. Diese Tradition sollte er die kommenden 20 Jahre beibehalten.\n\n\"Der Fremde im Zug\" (1951, nach einem Roman von Patricia Highsmith) brachte schließlich nach fünf Jahren Flaute wieder einen überragenden Erfolg. Mit diesem Film begann die dreizehnjährige Zusammenarbeit mit dem Kameramann Robert Burks. Wie schon in \"Die rote Lola\" spielte Hitchcocks Tochter Patricia eine Nebenrolle. 1953 folgte mit \"Ich beichte\" der eindeutigste filmische Bezug auf Hitchcocks starke katholische Prägung. Obwohl von der Kritik geschätzt, floppte der Film an den Kinokassen, was Hitchcock vor allem der Humorlosigkeit des Publikums anlastete.\n\nAls Anfang der 1950er Jahre das Fernsehen Einzug in die Wohnzimmer hielt, versuchte die Kinoindustrie, mit neuen technischen Verfahren wie dem Breitbildformat Cinemascope oder dem 3D-Verfahren den Zuschauerschwund aufzuhalten. So drängte Warner Bros. Hitchcock, seinen nächsten Film in 3D zu drehen. Über diese Entscheidung, die zur Einschränkung der Bewegungsfreiheit der Kamera führte, war Hitchcock nicht glücklich; er setzte auch nur wenige explizite 3-D-Effekte ein. \"Bei Anruf Mord\" (1954) ist die Verfilmung eines damals sehr populären Theaterstücks von Frederick Knott, der auch das Drehbuch schrieb. Mit Hauptdarstellerin Grace Kelly drehte Hitchcock im Anschluss noch zwei weitere Filme, ehe sie sich aus dem Filmgeschäft zurückzog.\n\n\nParamount.\nDie Erfahrung mit dem aufgezwungenen 3D-Verfahren zeigte Hitchcock die Grenzen bei Warner Brothers. Er schloss daher 1953 einen Vertrag mit Paramount ab, der ihm völlige künstlerische Freiheit garantierte. 1954 begann die für Hitchcock erfolgreichste Zeit mit \"Das Fenster zum Hof\". Neben Grace Kelly ist ein weiteres Mal James Stewart zu sehen. Die Hauptfigur sitzt während des gesamten Films im Rollstuhl und beobachtet durch ein Teleobjektiv das Geschehen in den gegenüberliegenden Wohnungen – sozusagen stellvertretend für den Zuschauer, aber auch stellvertretend für Hitchcock selbst, der in diesem Film den voyeuristischen Aspekt des Filmemachens aufzeigt.\n\"Über den Dächern von Nizza\" (1955) ist ein leichter, romantischer Thriller, in dem neben Grace Kelly – nach zwei Jahren Filmpause – wieder Cary Grant spielte. Wohl um dem Glamour dieses an der Côte d’Azur angesiedelten Films etwas entgegenzusetzen, drehte Hitchcock noch im selben Jahr die kostengünstig produzierte schwarze Komödie \"Immer Ärger mit Harry,\" in der Shirley MacLaine neben John Forsythe ihren ersten Filmauftritt hatte. Edmund Gwenn, der bereits in früheren Hitchcock-Filmen mitgewirkt hatte, spielte fast achtzigjährig eine seiner wenigen Hauptrollen. Obwohl Hitchcock in vielen seiner Filme schwarzen Humor untergebracht hat, ist es eine der wenigen echten Komödien von ihm.\n\n1955 nahm Hitchcock – rund fünf Jahre nach seiner Frau – die amerikanische Staatsbürgerschaft an. Im selben Jahr begann er mit Doris Day und James Stewart die Dreharbeiten zu \"Der Mann, der zuviel wußte\" (1956), dem einzigen Remake eines seiner Filme in seiner Karriere. Ebenfalls 1955 startete die wöchentliche Fernsehserie \"Alfred Hitchcock Presents\" (ab 1962 \"The Alfred Hitchcock Hour\"). Hitchcock war Produzent, trat in vielen Folgen als Moderator auf und inszenierte insgesamt 18 Folgen. Auch für die Fernsehserien \"Suspicion\" und \"Startime\" nahm er für je eine Folge im Regiestuhl Platz. Nach zehn Jahren beendete er seine Fernseharbeit, an der er zunehmend das Interesse verloren hatte. Hinzu kam, dass die Produktion den Auftraggebern zu teuer wurde und die Zeit von Serien mit jeweils abgeschlossenen Folgen, sogenannten „Anthologies“, zu Ende ging.\n\nMit \"Der falsche Mann\" wurde er 1956 einem seiner Grundprinzipien, der strikten Trennung von Leben und Fiktion, untreu. In dem Schwarzweißfilm mit Henry Fonda und Vera Miles wird an authentischen Schauplätzen die auf Tatsachen beruhende Geschichte eines zu unrecht Verurteilten erzählt. Der Film entstand noch einmal für Warner Bros., da Hitchcock dem Studio bei seinem Ausscheiden noch einen Film ohne Regiegage zugesagt hatte. Allerdings war \"Der falsche Mann\", der viele Stilelemente des Film noir und ein trostloses Ende aufweist, kommerziell ein Flop.\n\n\nHöhepunkt und Wende.\n1957 drehte Hitchcock seinen letzten Film für Paramount: \"Vertigo – Aus dem Reich der Toten\" (1958 veröffentlicht). Das Drehbuch entstand in gemeinsamer intensiver Arbeit von Hitchcock und Samuel A. Taylor. In wenige seiner Filmfiguren projizierte Hitchcock wohl so viel von seiner eigenen Persönlichkeit wie in den von James Stewart verkörperten Scottie Ferguson, der versucht, eine Frau nach seinen Vorstellungen umzuformen. Zu seiner Entstehungszeit nicht besonders erfolgreich, zählt der Film inzwischen&nbsp;– ähnlich wie der folgende \"Der unsichtbare Dritte\"&nbsp;– zu den bedeutendsten Werken Hitchcocks. Hitchcock und sein Drehbuchautor Ernest Lehman konzipierten \"Der unsichtbare Dritte\" (1959, MGM) als eine Abfolge von Abenteuern, in denen ein Unschuldiger (Cary Grant in seinem letzten Hitchcock-Film) um seine Reputation und sein Leben kämpft. Die elegante Leichtigkeit der Erzählung beeinflusste viele nachfolgende Abenteuer- und Agentenfilme, was sich u.&nbsp;a. in den James-Bond-Filmen der darauf folgenden Jahre zeigt. Für Hitchcock selbst blieb es für lange Zeit der letzte vorwiegend heitere Film.\n\nDas im Anschluss vorbereitete Projekt mit Audrey Hepburn in der Hauptrolle wurde durch Hitchcock gestoppt, als Hepburn wegen einer geplanten Vergewaltigungsszene absagte. Mit seiner bewusst kostengünstigen Produktion \"Psycho\" (1960) folgte Hitchcocks wohl bekanntester Film: Die in einer Woche Dreharbeit entstandene „Duschszene“ zählt heute zu seinen meistanalysierten Filmszenen. Ungewöhnlich war auch der Tod einer Hauptfigur nach nur einem Drittel des Films. Die zeitgenössischen Kritiken fielen unerwartet barsch aus, doch das Publikum machte \"Psycho\" zu Hitchcocks größtem kommerziellen Erfolg.\n\nNachdem zwei angedachte Projekte scheiterten&nbsp;– unter anderem, weil Walt Disney dem \"Psycho\"-Regisseur die Dreherlaubnis in Disneyland verweigerte&nbsp;– nahm Hitchcock seinen nächsten Film erst Mitte 1961 in Angriff: \"Die Vögel\" (1963), ein weiterer Horrorfilm, der nicht zuletzt durch seine Dramaturgie und die eingesetzte Tricktechnik&nbsp;– etwa den Sodium Vapor Process&nbsp;– stilbildend wirkte. Der deutsche Komponist Oskar Sala setzte statt Filmmusik elektronisch erzeugte Geräusche ein. Seine Hauptdarstellerin Tippi Hedren hatte Hitchcock im Werbefernsehen entdeckt. Obwohl sie keine Filmerfahrung besaß, nahm er sie für die nächsten sieben Jahre unter Vertrag.\n\n\"Die Vögel\" entstand für Universal, die kurz zuvor teilweise von MCA übernommen worden waren und für die Hitchcock von nun an alle seine Filme drehen sollte. Lew Wasserman, bis zu diesem Zeitpunkt Agent Hitchcocks, wurde Präsident von Universal und gab seine Agententätigkeit auf. Hitchcock selbst trat seine Rechte an \"Psycho\" und seiner Fernsehserie ab und wurde im Gegenzug der drittgrößte Aktionär von MCA.\n\nNach \"Die Vögel\" gibt es in Hitchcocks Werk einen Bruch. Die folgenden drei Filme der 1960er Jahre blieben kommerziell hinter den vorangegangenen Erfolgen zurück. Konflikte mit seiner Hauptdarstellerin Tippi Hedren prägten die Dreharbeiten so weit, dass er das Gelingen seines nächsten Films \"Marnie\" (1964) bewusst zu sabotieren schien. Der Film fiel bei der professionellen Filmkritik durch. Bemängelt wurde, dass das Psychogramm einer verstörten, traumatisierten Frau sich psychologischer Erklärungsmodelle bediene, die überholt und undifferenziert wirkten, und der Film enthalte, untypisch für Hitchcock, viele handwerkliche Fehler. Die Qualität und der Rang des Films in Hitchcocks Werk wurde erst im Nachhinein nach François Truffauts ausführlicher Analyse des Films erkannt.\nDieser erste kommerzielle Misserfolg seit rund fünfzehn Jahren war in mehrfacher Hinsicht ein Wendepunkt in Hitchcocks Karriere. Tippi Hedren war die letzte typische „Hitchcock-Blondine“, \"Marnie\" der letzte Film, den Hitchcocks langjähriger Kameramann Robert Burks dreht. Kurz nach Abschluss der Dreharbeiten verstarb zudem Hitchcocks Filmeditor George Tomasini, mit dem er zehn Jahre lang zusammengearbeitet hatte und für Bernard Herrmann, der seit 1955 Hitchcocks bevorzugter Filmkomponist war, war Marnie die letzte Zusammenarbeit mit Hitchcock.\n\n\nDas Spätwerk.\n\nErfolge und eine Rückkehr in die Heimat.\nFilmproduktionen wurden immer aufwendiger, der Erfolg an der Kinokasse immer wichtiger. Diverse Projekte, die Hitchcock reizten und die er mehr oder weniger intensiv plante, kamen so aus Angst der Produzenten nicht zustande – etwa \"Mary Rose\", die geplante Verfilmung eines skurrilen Theaterstücks. Mit \"R.R.R.R.\", einem Drehbuch mit zahlreichen Verwicklungen über eine italienische Ganoven-Familie in New York, wollte er Jahre nach \"Der unsichtbare Dritte\" wieder einen komischen Thriller drehen und damit alle Nachahmer \"(Charade\", \"Topkapi\" und andere) übertreffen. Das weit fortgeschrittene Projekt scheiterte schließlich an unüberbrückbaren sprachlichen und kulturellen Problemen mit den italienischen Mitarbeitern.\n\nAm 7. März 1965 erhielt Hitchcock für seinen „historischen Beitrag zum amerikanischen Kino“ den \"Milestone Award\" der \"Producers Guild Of America\"&nbsp;– die erste von vielen Ehrungen für sein Lebenswerk.\n\nMit \"Der zerrissene Vorhang\" (1966) kehrte Hitchcock schließlich zum Genre des Spionagefilms zurück, in dem er bereits in den 1930er Jahren in England große Erfolge gefeiert hatte. Die Premiere dieses 50.&nbsp;Hitchcock-Filmes sollte von einer groß angelegten Marketingkampagne begleitet werden. Nicht nur aus diesem Grund setzte Universal die aktuellen Stars Paul Newman und Julie Andrews gegen Hitchcocks Widerstand als Hauptdarsteller durch. Überdies kam es zum Bruch mit dem Komponisten Bernard Herrmann, als dieser nicht die von Universal gewünschte, auch für den Schallplattenverkauf geeignete Unterhaltungsmusik vorlegte. Auch an anderen wichtigen Positionen seines Stabes musste Hitchcock auf vertraute Mitarbeiter verzichten. \"Der zerrissene Vorhang\" fällt handwerklich und dramaturgisch gegenüber Hitchcocks letzten Filmen deutlich ab und wurde von der Kritik durchweg verrissen.\n\nUniversal forderte von ihm zeitgemäßere Themen ein. Als das von ihm und Howard Fast detailliert ausgearbeitete Drehbuch über einen homosexuellen Frauenmörder abgelehnt wurde, zog er sich für ein Jahr ins Privatleben zurück. Anfang 1968 entschloss er sich unter dem Druck der langen Pause seit dem letzten Film und der noch längeren Zeitspanne seit dem letzten Erfolg, den Spionageroman \"Topas\" von Leon Uris zu verfilmen, dessen Rechte Universal kurz zuvor erworben hatte. \"Topas\" wurde dann fast ausschließlich mit europäischen Schauspielern besetzt und völlig ohne Hollywood-Stars. In Europa waren die Französinnen Dany Robin und Claude Jade wie ihre Landsmänner Michel Piccoli und Philippe Noiret und die deutsche Aktrice Karin Dor Stars; die für amerikanische Zuschauer bekanntesten Gesichter waren der Fernsehschauspieler John Forsythe und der aus Kanada stammende John Vernon. Das endgültige Drehbuch wurde erst während der laufenden Dreharbeiten geschrieben, der Schluss nach einer katastrophalen Preview improvisiert. Publikum und Kritik reagierten mit Ablehnung auf Hitchcocks bis dahin teuersten Film, doch er zeigte sich zuversichtlich: „Ich habe meinen letzten Film noch nicht gedreht. \"Topas\" ist mein 51.&nbsp;Film, aber wann ich meinen letzten Film drehen werde, ist von mir, meinen Finanziers und Gott noch nicht entschieden worden.“\n\nIm Spätsommer 1970 nahm Hitchcock sein nächstes Projekt in Angriff und reiste dafür wieder in seine Heimat, wo er diesmal begeistert empfangen wurde. \"Frenzy\" (1972) spielt in London, dem Hitchcock eine liebevolle Hommage erweist, und ist in seinen Worten „die Geschichte eines Mannes, der impotent ist, und sich deshalb durch Mord ausdrückt“. Zunächst verliefen die Drehbucharbeit und auch die Dreharbeiten, die Hitchcock so ernst nahm wie lange nicht mehr, weitgehend reibungsfrei. Doch als seine Frau Alma einen Herzinfarkt erlitten hatte, wurde Hitchcock müde und untätig; die Crew war, ähnlich wie bei den drei vorangegangenen Filmen, wieder weitgehend auf sich alleine gestellt. Dennoch wurde \"Frenzy\", ein brutaler, zum Teil bitterer, von tiefschwarzem britischen Humor durchzogener Film, ein großer Erfolg. Nur in England war man enttäuscht und bemängelte vor allem die anachronistisch wirkende Darstellung Londons und des britischen Lebens.\n\n\nDer letzte Film.\nIm Frühjahr 1973 entschloss sich Hitchcock, den Roman \"The Rainbird Pattern\" von Victor Canning zu verfilmen. Doch die Arbeit am Drehbuch mit Ernest Lehman \"(Der unsichtbare Dritte)\" ging diesmal nicht mehr so reibungslos vonstatten: Hitchcock war merklich müde geworden, seine körperlichen Schmerzen betäubte er zunehmend mit Alkohol. Zwei Jahre benötigte die Fertigstellung des Drehbuchs, so lange wie nie zuvor in seiner Karriere.\n\nMit \"Familiengrab,\" wie der Film schließlich hieß, kehrte Hitchcock zum scheinbar heiteren, diesmal jedoch morbid akzentuierten Unterhaltungsthriller zurück. Wie stets legte er Wert auf eine ausgeklügelte Bildsprache, die erneut mit Hilfe von Storyboards erarbeitet wurde.\nDie Dreharbeiten gestalteten sich reibungslos und in einer entspannten Atmosphäre. Hitchcock, der sich im Rahmen seiner gesundheitlichen Möglichkeiten mit einem lange nicht gezeigten Elan in die Dreharbeiten einbrachte, zeigte sich zu Neuerungen bereit: Er war offen für Improvisationen seiner Schauspieler und nahm noch während der Dreharbeiten Änderungen am Ablauf vor. Die Überwachung der Schnittarbeiten musste er weitgehend seinen Mitarbeiterinnen Peggy Robertson und Suzanne Gauthier überlassen, da sich sein Gesundheitszustand deutlich verschlechterte. Zudem erlitt Alma einen zweiten Schlaganfall.\n\n\"Familiengrab\" wurde nach seiner Premiere im Frühjahr 1976 überwiegend freundlich aufgenommen, und Hitchcock schöpfte aus der Sympathie, die ihm entgegenschlug, für kurze Zeit Kraft, neue Filmideen aufzugreifen. Sein erst Anfang 1978 in Angriff genommenes Projekt, die Verfilmung des Romans \"The Short Night\" von Ronald Kirkbride, wurde aufgrund seines sich weiter verschlechternden Gesundheitszustands jedoch etwa ein Jahr später von Universal gestoppt. Im März 1979 wurde er vom American Film Institute für sein Lebenswerk geehrt. Zwei Monate später schloss er sein Büro auf dem Gelände der \"Universal\"-Studios. Am 3. Januar 1980 wurde Hitchcock in den britischen Adelsstand erhoben.\n\nAm Morgen des 29. April 1980 starb Alfred Hitchcock in seinem Haus in Los Angeles an Nierenversagen. Seine Leiche wurde eingeäschert, die Asche an einem unbekannten Ort verstreut.\n\n\nInhalte und Formen.\nIn rund fünfzig Jahren hat Alfred Hitchcock dreiundfünfzig Spielfilme als Regisseur begonnen und beendet. Die weitaus größte Zahl dieser Filme gehört dem Genre des Thrillers an und weist ähnliche Erzählmuster und Motive auf, wiederkehrende Elemente, visuelle Stilmittel und Effekte, die sich wie ein roter Faden durch sein Gesamtwerk ziehen.\n\n\nInhalt.\n\nMotive.\nDas Grundmotiv in Hitchcocks Filmen bildet meist die Angst der Protagonisten vor der Vernichtung ihrer (bürgerlichen) Existenz. Dabei bezieht sich diese Angst nicht nur auf Mörder, Gangster oder Spione, welche die bürgerliche Ordnung angreifen; die Hauptfiguren finden sich häufig in der Lage wieder, sogar von Vertretern des Gesetzes bedroht zu werden.\n\nZu diesem Motiv der Angst kommt – Hitchcocks katholischer Prägung entsprechend – jenes von Schuld und Sühne hinzu. Der unschuldig Verfolgte in seinen Filmen ist „unschuldig, aber nur in Bezug auf das, was man ihm vorwirft.“ Das heißt, die Figur wird durch das, was ihr im Laufe des Filmes widerfährt, im übertragenen Sinne für andere Defizite oder Vergehen bestraft: In \"Bei Anruf Mord\" etwa wird die Hauptfigur des Mordes verdächtigt; tatsächlich musste sie aus Notwehr töten. Das folgende Unheil kann jedoch als Strafe für den von ihr begangenen Ehebruch angesehen werden.\n\nEine Variation dieses Themas ist die Übertragung der Schuld auf eine andere Person. Unschuldige werden zu Schuldigen (oder Mitschuldigen) an Verbrechen anderer, da sie aus persönlichen Gründen nicht zur Aufklärung beitragen können. Zentral sind hierbei die beiden Filme \"Ich beichte\" und \"Der Fremde im Zug\", in denen die jeweiligen Protagonisten von Morden, die andere begangen haben, profitieren und, nachdem sie selbst unter Verdacht geraten, keine Möglichkeit haben, sich zu entlasten. In \"Vertigo\" macht der wahre Mörder die Hauptfigur durch ein Komplott zunächst scheinbar zum Schuldigen am Tod der ihr anvertrauten Person. Später macht sich das Opfer der Intrige tatsächlich am Tod der Frau schuldig, die er liebt.\n\nFalsche Verdächtigungen, aber auch ausgeprägte Schuldkomplexe, gehen bei Hitchcocks Filmen mit der Bedrohung der Identität einher. Seine traumatisierten oder verfolgten Figuren nehmen selbst falsche Namen an oder werden – aus unbekannten Gründen – für jemand anderen gehalten. Das Motiv des Identitätsverlusts spielt Hitchcock, angefangen von seinem ersten bis zu seinem letzten Film, in unterschiedlichsten Varianten durch, besonders einprägsam in \"Vertigo\": Die weibliche Hauptfigur wird zunächst im Rahmen eines Mordkomplotts in eine andere Person (die anschließend ermordet wird) verwandelt und nimmt daraufhin wieder ihre eigentliche Identität an, nur um anschließend wieder in die andere Person zurückverwandelt zu werden.\n\nOft stehen Schuld und Bedrohung in Zusammenhang mit sexuellen Aspekten. In \"Der Fall Paradin\" genügt bereits der Gedanke an Ehebruch, um das Leben des Protagonisten zu gefährden. In \"Berüchtigt\" ist der Zusammenhang zwischen Sex, Schuld und Bedrohung zentrales Thema. Hitchcocks Verbindung von Sex und Gewalt wird in Mordszenen deutlich, die er oft wie Vergewaltigungen inszeniert, etwa der Schlusskampf zwischen Onkel und Nichte Charlie in \"Im Schatten des Zweifels\", die Scherenszene in \"Bei Anruf Mord\" und die Duschszene in \"Psycho\". Darüber hinaus spielt Sexualität gerade in abnorm empfundenen Erscheinungsformen eine große Rolle in seinem Werk. Aufgrund der Auflagen der Zensur werden jedoch Homosexualität, die in Verbindung mit Schuld und Verderben regelmäßig vorkommt, oder Nekrophilie (in \"Vertigo\") nur in einzelnen Gesten oder Schlüsselszenen angedeutet. Auch Fetischismus (\"Erpressung\", \"Vertigo\", \"Psycho\") und Voyeurismus (\"Das Fenster zum Hof\", \"Psycho\") spielen in seinen Filmen eine gewisse Rolle. In mehreren Filmen wird zudem ein erotischer Bezug der männlichen Hauptfiguren zu ihren Müttern angedeutet, etwa in \"Psycho\" und \"Die Vögel\". Zentral in diesem Zusammenhang ist \"Berüchtigt\". Hier verhalten sich Claude Rains und Leopoldine Konstantin in manchen Schlüsselszenen wie ein Ehepaar. Dieser Eindruck wird durch den geringen Altersunterschied der Schauspieler von nur vier Jahren verstärkt.\n\nUnter den in Hitchcocks Bildsprache verwendeten Symbolen finden sich Vögel als Vorboten des Unglücks (etwa in \"Erpressung\", später als vorherrschendes Thema in \"Die Vögel\"), Treppen, die Verlust oder Freiheit bedeuten können (\"Berüchtigt\", \"Psycho\", \"Vertigo\" und andere), sowie Handschellen und andere Fesseln, um Abhängigkeit und Ausgeliefertsein auszudrücken, meist im sexuellen Kontext (zum Beispiel in \"Der Mieter\"). Auch Spiegel tauchen bei Hitchcock regelmäßig auf – in Zusammenhang mit dem Verlust oder der Erkenntnis der eigenen Persönlichkeit oder als allgemeines Symbol für Täuschungen (einprägende Beispiele: \"Vertigo\" und \"Psycho\").\n\n\nFiguren.\nDie meisten Protagonisten in Hitchcocks Thrillern sind Normalbürger, die zu Beginn der Geschichte in der Regel nichts mit kriminellen Machenschaften zu tun haben. Meist werden sie durch Zufall oder unbekannte Umstände in geheimnisvolle und bedrohliche Vorgänge gezogen. Dem Zuschauer wird so das beunruhigende Gefühl vermittelt, dass auch er jederzeit in derartige Situationen geraten könnte. Professionelle Agenten oder Spione findet man dagegen nur selten unter den Hauptfiguren, obwohl Hitchcock viele Filme drehte, die im Agentenmilieu spielen. Hitchcock drehte bis auf eine Ausnahme (\"Erpressung\", 1929) auch nie einen Film, in dem die Arbeit der Polizei im Mittelpunkt steht; aktive Polizisten tauchen ansonsten nur als Nebenfiguren und üblicherweise als Hindernis auf.\n\n\nMännliche Antihelden.\nDer Prototyp des Antihelden bei Hitchcock sind die von James Stewart gespielten Figuren: In \"Cocktail für eine Leiche\" muss der von Stewart dargestellte Lehrer erkennen, dass zwei seiner Studenten eine seiner Theorien zum Anlass nahmen, einen Mord zu verüben und diesen mit seinen Thesen zu rechtfertigen; am Ende steht er hilflos vor diesem menschlichen Abgrund, in den er nicht nur hineingezogen wurde, sondern den er sogar mit heraufbeschworen hat. In \"Das Fenster zum Hof\" stellt Stewart eine Figur dar, die bindungsscheu sowie körperlich beeinträchtigt und voyeuristisch veranlagt ist und dadurch in Schwierigkeiten kommt.\n\nEs gibt nur wenige positive, ungebrochene Helden bei Hitchcock. Ein Schauspieler, der diesen seltenen Rollentypus verkörperte, war Cary Grant in \"Über den Dächern von Nizza\" und in \"Der unsichtbare Dritte\". Diese Figuren meistern die Herausforderungen zwar mit Charme und Leichtigkeit, doch stehen sie in Verdacht, kriminell zu sein beziehungsweise verlieren sie zeitweise die Kontrolle, womit selbst sie keine gänzlich unantastbaren Helden sein können. Aber sogar Cary Grant spielte in zwei seiner Hitchcock-Filme Figuren, deren Schattenseiten sich zeitweise vor deren positive Merkmale schieben.\n\nIm Laufe der Karriere Hitchcocks gewinnen ambivalente oder gar negativ gezeichnete Hauptfiguren immer stärker an Gewicht. Diese \"Antihelden\" weisen physische oder psychische Probleme auf, sind Verlierertypen oder unsympathisch. Durch ihr obsessives Fehlverhalten wirken sie schwach und können Schaden anrichten. Diese Figuren dienen zwar kaum als Vorbild, doch soll deren ambivalente Persönlichkeit dazu beitragen, dass sich der Zuschauer in ihnen wiederfinden kann.\n\n\nStarke Frauen.\nIn vielen Filmen bedient Hitchcock auf den ersten Blick das klassische Motiv der schwachen, zu beschützenden Frau. Doch während das Klischee verlangt, dass der strahlende Held sie rettet, ist sie bei Hitchcock oft auf sich alleine gestellt. In einigen Fällen ist der vermeintliche Beschützer schwach oder zu sehr mit sich selbst beschäftigt, als dass er der bedrohten Frau helfen könnte, wie zum Beispiel Ingrid Bergman und Cary Grant in \"Berüchtigt\". In anderen Fällen geht von der männlichen Hauptfigur (in der Regel dem Ehemann) sogar ein tatsächliches oder vermeintliches Bedrohungspotential aus. Klassische Beispiele: Joan Fontaine und Cary Grant in \"Verdacht\" sowie Grace Kelly und Ray Milland in \"Bei Anruf Mord\".\n\nDie Rollenverteilung zwischen Mann und Frau kehrt Hitchcock in einigen Filmen gänzlich um: Die Frau ist dem Mann, der zunehmend passiver wird, überlegen und wendet das Geschehen zum Guten. Beispiele sind \"Jung und unschuldig\" (die Tochter des Polizeichefs verhilft einem Verdächtigen zur Flucht und löst letztendlich den Fall), \"Ich kämpfe um dich\" (eine Psychologin dringt in das Unterbewusste des Mordverdächtigen ein und rettet ihn vor der sicheren Verurteilung) sowie \"Der Mann, der zuviel wußte\" (die Ehefrau verhindert zuerst einen geplanten Mord und rettet dann das eigene Kind vor den Verbrechern).\n\nDer Typ, der sich dabei im Laufe der Zeit herauskristallisierte, ist jener der jungen, schönen, kühlen, hintergründigen und undurchsichtigen Blondine. Die oberflächliche Kühle der Hitchcock-Blondine verbirgt jedoch eine stark entwickelte Sexualität. Besonders deutlich wird dies in \"Der unsichtbare Dritte\", wenn Eva Marie Saint zunächst gegenüber Cary Grant zweideutige Bemerkungen macht, dann plötzlich den völlig überraschten Fremden küsst und ihn ohne Zögern in ihrem Schlafwagenabteil unterbringt. Nicht der Mann, sondern die (blonde) Frau spielt hier den aktiven Part und zeigt damit die Fragilität des männlichen, bürgerlichen Weltbildes.\n\n\nSympathische Schurken.\nHitchcock legt durch seine Gestaltung von Figuren und Dramaturgie dem Zuschauer eine Identifikation mit dem Schurken nahe.\nSeine Antagonisten wirken zuweilen auffällig sympathisch und übertreffen mitunter die Ausstrahlung der Hauptfiguren.\nOft konkurrieren Held und Bösewicht um dieselbe Frau; die Liebe des Gegenspielers erscheint dabei tiefer und aufrichtiger als die des Helden. Besonders auffällig ist dies in \"Berüchtigt\" (Claude Rains gegenüber Cary Grant) und in \"Der unsichtbare Dritte\" (James Mason wiederum gegenüber Cary Grant). Selbst ein ausgesprochen heimtückischer Schurke wie Ray Milland in \"Bei Anruf Mord\" wirkt in einzelnen Momenten gegenüber dem unbeholfenen Robert Cummings sympathischer, in jedem Fall jedoch gegenüber der Polizei vertrauenserweckender. Oft sind sie die eigentlichen Hauptfiguren, wie Joseph Cotten als charmanter Witwenmörder in \"Im Schatten des Zweifels\" oder Anthony Perkins als linkischer, von seiner Mutter gepeinigter Mörder in \"Psycho\".\n\n\nDominante Mütter.\nIn vielen seiner Filme – ab Mitte der 1940er Jahre regelmäßig – tauchen dominante Mütter auf, die einen beunruhigenden Einfluss auf ihre meist erwachsenen Kinder ausüben und zum Teil Auslöser oder Ursache dramatischer Ereignisse sind. Erstmals uneingeschränkt bösartig erscheint die Mutter in \"Berüchtigt\" (1946), die ihren Sohn zum Mord an der Frau, die er liebt, antreibt.\nDer extremste Fall tritt in \"Psycho\" (1960) zutage, wo die tote Mutter noch von ihrem Sohn Besitz ergreift und ihn zu ihrem mordenden Werkzeug werden lässt. Daneben gibt es eine Vielzahl weniger dämonischer Variationen, wobei Besitzergreifung allen Mutter-Typen gemein ist:\nIn \"Die Vögel\" (1963) erträgt es die Mutter von Mitch (Jessica Tandy) nicht, dass ihr erwachsener Sohn (Rod Taylor) sich für eine andere Frau interessiert. In \"Marnie\" (1964) wird das Leben der Tochter durch einen von der Mutter übertragenen Schuldkomplex beinahe zerstört.\n\nIn zwei Filmen variiert Hitchcock dieses Rollenmuster: In \"Rebecca\" und \"Sklavin des Herzens\" übernehmen Haushälterinnen die Funktion der dämonischen Mutter.\n\n\nZwielichtige oder leichtgläubige Beamte.\nÜblicherweise positiv besetzte Figuren wie Polizisten, Richter oder andere Vertreter des Staates erscheinen oft zwiespältig: Sie sind nicht in der Lage, die Helden zu beschützen, oder stellen sogar eine Bedrohung für diese dar. Polizisten verdrehen das Recht, sie handeln aus persönlichen Motiven, sie glauben dem ersten Anschein und schützen den tatsächlich Schuldigen aufgrund von dessen vordergründig tadellosem Erscheinen, sie sind tollpatschig oder arbeiten schlampig. Dieses Rollenmuster durchzieht Hitchcocks gesamtes Werk, von \"Der Mieter\" (1927) bis \"Frenzy\" (1972).\n\nDarüber hinaus finden sich vereinzelt Geheimdienstmitarbeiter unter den Nebenfiguren, die sich als Gegner (das Ehepaar Drayton in \"Der Mann, der zuviel wußte\", 1956) oder als Helfer offenbaren, wobei auch letztere Schwierigkeiten bringen können – beispielsweise der „General“ (Peter Lorre) in \"Geheimagent\" oder Leo G. Carroll als CIA-Mitarbeiter in \"Der unsichtbare Dritte\". Indem die feste Trennlinie zwischen Gut und Böse verschwimmt, wird das Gefühl der Verunsicherung beim Zuschauer gesteigert.\n\n\nFormale Elemente.\n\nDramaturgie.\nHitchcocks Credo lautete: „For me, the cinema is not a slice of life, but a piece of cake.“ (etwa: „Für mich ist das Kino nicht ein Stück aus dem Leben, sondern ein Kinderspiel.“) Film war für ihn eine artifizielle Kunstform. Nur einmal&nbsp;– in \"Der falsche Mann\"&nbsp;– wich er von diesem Grundsatz ab. Aber auch hier liegt der Akzent auf jenen Elementen, die nicht dokumentarisch sind&nbsp;– etwa der subjektiven Perspektive des unschuldig Verdächtigten und seiner hilflosen Frau. Einigen seiner weiteren Filme liegen zwar auch reale Ereignisse zugrunde (\"Der zerrissene Vorhang\", \"Das Fenster zum Hof\", \"Der Auslandskorrespondent\" oder \"Cocktail für eine Leiche\"), doch werden diese so weit fiktionalisiert, dass außer dem Grundmotiv kein Bezug zu der ursprünglichen Geschichte übrig bleibt.\n\nEine nicht verwirklichte Idee für \"Der unsichtbare Dritte\", die der Regisseur im Interview mit Truffaut erwähnt, verdeutlicht Hitchcocks Vorstellungen davon, die Realität zu transzendieren: Er wollte zeigen, wie unter den Augen Cary Grants auf einem Fließband ein Auto zusammengebaut wird und anschließend aus dem fertiggestellten Auto eine Leiche fällt&nbsp;– nach realistischen Maßstäben unmöglich. Doch Hitchcocks Begründung für das Verwerfen der Idee zeigt, dass er sich in solchen Fragen nicht an der Wahrscheinlichkeit orientierte: „Wir haben die Idee in der Geschichte nicht richtig unterbringen können, und selbst eine willkürliche Szene kann man nicht ohne Motiv ausführen.“ Den Vorwurf, Gesetze der Plausibilität zu missachten, nahm er bewusst in Kauf: „Wenn man alles analysieren wollte und alles nach Erwägungen der Glaubwürdigkeit und Wahrscheinlichkeit konstruieren, dann würde keine Spielfilmhandlung dieser Analyse standhalten, und es bliebe einem nur noch eines übrig: Dokumentarfilme zu drehen.“ Hitchcock vertraute darauf, dass die Zuschauer unwahrscheinliche Details akzeptieren würden, da er diese nur verwendete, um die Handlung zu dramatisieren, voranzutreiben oder zu straffen.\n\nFür bewusste Irritation sorgte auch Hitchcocks Spiel mit filmtypischen Klischees. So vermied er es insbesondere bei den Nebenrollen, Schauspieler nach festgelegtem Typ zu besetzen. Auch bei der Wahl seiner Spielorte entzog sich Hitchcock den Genre-Gesetzen. So ließ er Verbrechen und bedrohliche Szenen häufig nicht in unheimlichen, dunklen Räumen stattfinden, sondern bei hellem Tageslicht und an scheinbar harmlosen Orten wie einem mit Menschen übersäten Marktplatz (\"Der Mann, der zuviel wußte\" [1956] und \"Der Auslandskorrespondent\"), in einer menschenleeren Landschaft, auf einer öffentlichen Versteigerung und in einer Hotelhalle \"(Der unsichtbare Dritte)\", auf einer idyllischen Bergstraße \"(Über den Dächern von Nizza)\", auf einer Party (\"Berüchtigt\" und \"Jung und unschuldig\") in einer voll besetzten Konzerthalle (beide \"Der Mann, der zuviel wußte\") oder in einem mit lauter freundlichen Menschen besetzten Eisenbahnzug \"(Eine Dame verschwindet)\".\n\n\nSuspense.\nDie klassische, auf das Überraschungsmoment aufbauende Form des Kriminalfilms ist der Whodunit. Bis auf wenige Ausnahmen bediente sich Hitchcock jedoch einer anderen Form des Spannungsaufbaus, des sogenannten Suspense: Dem Zuschauer sind ab einem gewissen Zeitpunkt bestimmte Informationen oder Umstände bekannt, von denen die handelnden Personen nichts wissen. Er fiebert in besonderer Weise mit den Helden, er sieht Ereignisse kommen, möchte den Figuren helfen, kann es aber nicht. In einigen Filmen wird das klassische Suspense dahingehend variiert, dass handelnde Personen die Rolle des Zuschauers übernehmen. Ein Beispiel von vielen: In \"Das Fenster zum Hof\" dringt Lisa in die Wohnung des verdächtigen Nachbarn ein, um nach Beweisen für einen möglichen Mord zu suchen. Ihr Partner Jeff beobachtet das Geschehen von der gegenüber liegenden Wohnung aus und sieht dabei den Nachbarn vorzeitig zurückkommen. Er vermutet sie in Lebensgefahr, kann ihr aber nicht helfen.\n\nFür einige markante Szenen baute Hitchcock zudem bewusst eine Suspense-Situation auf, um den Zuschauer mit einem umso gewaltigeren Überraschungseffekt schockieren zu können. Ein berühmtes Beispiel findet sich in \"Psycho\": Zum einen ist Marion Crane mit verschiedenen Insignien einer typischen Hauptfigur eines Hitchcockfilms ausgestattet, so dass kaum jemand erwartet, dass sie bereits in der ersten Hälfte des Films stirbt. Zum anderen schaltet Hitchcock der Duschszene selbst einen Suspense-Moment vor. Norman Bates beobachtet Marion Crane durch ein Loch in der Wand beim Entkleiden. Sie geht unter die Dusche. Der Zuschauer wird nun eben keinen Mord, sondern schlimmstenfalls eine Vergewaltigung durch Norman befürchten. Der bestialische Mord ist somit völlig überraschend und damit ein Grund für die Berühmtheit der Szene.\n\n\nMacGuffin.\nEin von Hitchcock in seinen Thrillern sehr häufig verwendetes Mittel war der sogenannte MacGuffin: ein Element, das die Handlung vorantreibt oder sogar initiiert, obwohl es für die Entwicklung der Figuren und für den Zuschauer inhaltlich völlig bedeutungslos, geradezu austauschbar ist. Der MacGuffin in \"Der unsichtbare Dritte\" sind schlicht „Regierungsgeheimnisse“, über die der Held oder der Zuschauer während der gesamten Handlung nichts Weiteres erfährt. In \"Psycho\" benutzt Hitchcock unterschlagenes Geld, das die Sekretärin zur Flucht treibt und so in „Bates Motel“ führt, um das Publikum anfangs gezielt in die Irre zu führen und für einen Kriminalfall zu interessieren, der mit der eigentlichen Handlung nur am Rande zu tun hat. Die mysteriösen „39&nbsp;Stufen“ im gleichnamigen Film sind eine Geheimorganisation, über die bis kurz vor Ende des Films überhaupt nichts bekannt ist, außer dass sie gefährlich ist. Ein besonders außergewöhnlicher MacGuffin ist die als Volksliedmelodie getarnte Geheimdienstinformation aus \"Eine Dame verschwindet\".\n\n\nFilmische Mittel.\nBeeinflusst vom Stummfilm beruhte Hitchcocks Filmverständnis auf dem Anspruch, alles Wichtige in seinen Filmen visuell und so wenig wie möglich durch Dialoge auszudrücken. Seine typischen Kameraeinstellungen geben im Bild genau das wieder, was für das Verständnis der Szene wesentlich ist&nbsp;– auch um dem Zuschauer nicht die Möglichkeit zu geben, sich durch unwesentliche Details ablenken zu lassen. So wirken beispielsweise Kuss-Szenen bei Hitchcock immer sehr intim, da er gewöhnlich mit der Kamera sehr nahe an die beiden sich Küssenden heranfuhr und den Zuschauer sozusagen zum dritten Umarmenden machte. Zu den berühmtesten Beispielen dieser visuellen Erzählweise zählen die Duschszene aus \"Psycho\", der Flugzeugangriff auf Cary Grant und die Jagd auf Mount Rushmore in \"Der unsichtbare Dritte\", die Versammlung der Vögel auf dem Klettergerüst in \"Die Vögel\" oder die zehnminütige Konzertszene in der Royal Albert Hall in \"Der Mann, der zuviel wußte\" von 1956.\n\nHitchcocks visueller Arbeitsstil drückt sich unter anderem in den Expositionen vieler seiner Filme aus. Er bringt den Zuschauern die handelnden Figuren und die Umstände der folgenden Handlung ohne die Verwendung von Dialogen nahe. Die Länge dieser Einführungen variiert zwischen wenigen Sekunden und mehreren Minuten. Erstmals verfolgte Hitchcock diese Technik 1929 in seinem ersten Tonfilm \"Erpressung\".\n\nZudem tauchen in Hitchcocks Filmen immer wieder ungewöhnliche filmische Operationen auf, um die Stimmung und Spannung bewusst zu verstärken, beispielsweise eine gegenläufige Zoom-Fahrtbewegung in \"Vertigo\" (später auch als „Vertigo-Effekt“ bezeichnet), lange Kamerafahrten wie die aus einer Totale eines großen Raums bis in die Naheinstellung eines Schlüssels in einer Hand (in \"Berüchtigt\") oder auf ein zuckendes Auge (in \"Jung und unschuldig\") sowie die aus ungefähr siebzig Einstellungen bestehende fünfundvierzig Sekunden lange Mordszene unter der Dusche in \"Psycho\", unmittelbar gefolgt von einer etwa einminütigen Kamerafahrt ohne einen einzigen Schnitt. Der Production Designer Robert Boyle, mit dem Hitchcock bei fünf Filmen zusammenarbeitete, meinte: „Keiner der Regisseure, mit denen ich je zusammengearbeitet habe, wusste so viel über Film wie er. Viele der Regisseure, mit denen ich gearbeitet habe, wussten eine ganze Menge, aber sie besaßen nicht die technischen Fähigkeiten, die er hatte. Er suchte immer nur den visuellen Ausdruck, und so etwas wie eine zufällige Einstellung gab es bei ihm nicht.“\n\nNur einmal griff Hitchcock aus Experimentierfreude auf einen filmtechnischen Kniff zurück, der sich nicht unmittelbar aus der Dramaturgie ergab. In \"Cocktail für eine Leiche\" (1948) drehte er bis zu zehn Minuten lange Einstellungen, die er zum großen Teil sogar über unsichtbare Schnitte ineinander übergehen ließ. Er wollte damit bei dieser Theaterverfilmung die Einheit von Zeit und Raum dokumentieren. Später gab er zu, dass es ein Fehler war, damit gleichzeitig den Schnitt als wesentliches gestaltendes Instrument der Dramaturgie aus der Hand gegeben zu haben.\n\n\nLicht und Farben.\nInspiriert von amerikanischen und deutschen Filmemachern, setzte Hitchcock schon bei seinen ersten Filmen Licht- beziehungsweise Schatteneffekte ein. Typisch für Hitchcock sind Linien und Streifen in Form von Schatten (durch Gitter, Jalousien oder Ähnliches verursacht), die vor allem auf Gesichter fallen und eine unheilvolle Atmosphäre verstärken sollen. Darüber hinaus verwendet er in einzelnen Szenen sehr starke, zum Teil unnatürlich wirkende Kontraste, um einen äußeren oder inneren Gut-Böse-Gegensatz zu visualisieren.\n\nDieses Hell-Dunkel-Spiel unterstützte Hitchcock durch die Kostüme der Figuren. So ließ er Ingrid Bergman am Anfang von \"Berüchtigt\" gestreifte Kleidung tragen, um ihre Zerrissenheit zu unterstreichen. In \"Der Mieter\" trug Ivor Novello zu Beginn Schwarz, später, um seine Unschuld auch nach außen hin deutlich zu machen, Weiß. Die Methode, durch die Farbgebung der Kostüme den emotionalen Zustand der Figuren zu unterstreichen, behielt Hitchcock auch für die Farbfilme bei. In \"Bei Anruf Mord\" wurden die Kostüme von Grace Kelly mit der Dauer des Films immer trister und grauer, entsprechend ihrer inneren Gemütsverfassung.\nZu Hitchcocks Farbwahl von Grace Kellys Kleidern in \"Das Fenster zum Hof\" sagte die Kostümbildnerin Edith Head: „Für jede Farbe und jeden Stil gab es einen Grund; er war sich seiner ganzen Entscheidung absolut sicher. In einer Szene sah er sie in blassem Grün, in einer anderen in weißem Chiffon, in einer weiteren in Gold. Er stellte im Studio tatsächlich einen Traum zusammen.“ In seinen späteren Filmen, allen voran \"Marnie\" und \"Vertigo\", gab es eine ausgefeilte, die Kostüme, die Dekors und die Beleuchtung umfassende Farbdramaturgie.\n\n\nTricktechnik.\nNach Hitchcocks Filmverständnis schafft sich der Film seine eigene Realität und soll oder darf kein Abbild des wahren Lebens sein. Die Nutzung sämtlicher Möglichkeiten, genau das wiederzugeben, was der Regisseur sich vorstellt, ist nach diesem Verständnis nicht nur legitim, sondern erforderlich. Hitchcock hat die Entwicklung der Tricktechnik aufmerksam beobachtet und schon sehr früh&nbsp;– gelegentlich zum Missfallen seiner Produzenten&nbsp;– neue Trickverfahren eingesetzt, zum Beispiel das Schüfftan-Verfahren (in \"Erpressung\") oder das Matte Painting. In seinen englischen Thrillern, vor allem in \"Nummer siebzehn\" und \"Jung und unschuldig\", arbeitete Hitchcock bei Verfolgungsjagden oft und erkennbar mit Modellen. In \"Eine Dame verschwindet\" sind die Rückprojektionen während der Zugfahrt aber bereits so ausgereift, dass sie noch Jahrzehnte später überzeugen. Ähnliches gilt für die Schlussszene von \"Der Fremde im Zug\", in der zwei Männer auf einem sich immer schneller drehenden Karussell kämpfen&nbsp;– in einer virtuosen Kombination von Realeinstellungen, Modellen und Rückprojektionen. \"Die Vögel\" (1963) beinhaltet rund vierhundert Trickeinstellungen, für die Hitchcock auf sämtliche damals verfügbaren Tricktechniken zurückgriff, unter anderem auch auf das ansonsten für Animationsfilme verwendete Rotoskopieverfahren.\n\n\nTon und Musik.\nHitchcock hat seit dem Aufkommen des Tonfilms Musik und Toneffekte eingesetzt, um die Dramaturgie bewusst zu unterstützen. Den Umgang Hitchcocks mit dem Medium Ton beschrieb die Schauspielerin Teresa Wright \"(Im Schatten des Zweifels)\" folgendermaßen: „Wenn ein Schauspieler mit den Fingern trommelte, war das nicht ein zweckloses Trommeln, es hatte einen Rhythmus, ein musikalisches Muster&nbsp;– es war wie ein Geräusch-Refrain. Ob jemand nun ging oder mit Papier raschelte oder einen Umschlag zerriss oder vor sich hin pfiff, ob das Flattern von Vögeln oder ein Geräusch von draußen war, alles wurde sorgfältig von ihm orchestriert. Er komponierte die Toneffekte wie ein Musiker Instrumentenstimmen.“ Gegenüber Truffaut erwähnte Hitchcock, dass er nach dem Endschnitt eines Films seiner Sekretärin ein „Tondrehbuch“ diktiert, das alle von ihm gewünschten Geräusche enthält.\n\nIn \"Mord – Sir John greift ein!\" (1930) versteckte Hitchcock, da ein nachträgliches Bearbeiten der Tonspur zu diesem Zeitpunkt technisch noch nicht möglich war, gar ein komplettes Orchester hinter den Kulissen, um die entsprechenden Stellen musikalisch zu untermalen. Weitere klassische Beispiele für Hitchcocks dramaturgischen Musikeinsatz sind \"Geheimagent\" (1936, der Dauerakkord des toten Organisten in der Kirche), \"Eine Dame verschwindet\" (1938, die Melodie mit dem Geheimcode und der „Volkstanz“), \"Im Schatten des Zweifels\" (1943, der „Merry-Widow“-Walzer), \"Der Fremde im Zug\" (1951, die Szenen auf dem Rummelplatz) und \"Das Fenster zum Hof\" (1954, die im Laufe des Films entstehende Komposition des Klavierspielers). In \"Der Mann, der zuviel wußte\" (1956) schließlich wird Musik, sowohl orchestral wie auch gesungen, aktiv inszeniert und dramaturgisch eingebunden: Sie spielt eine wesentliche Rolle in der Gesamtdramaturgie des Films.\n\nDie Musik der Filme aus den späten 1950er und frühen 1960er Jahren, der Zeit als Hitchcock mit dem Komponisten Bernard Herrmann zusammenarbeitete, ist tragendes Element der jeweiligen Filme. Kritiker bescheinigen der Musik der Filme \"Vertigo\", \"Der unsichtbare Dritte\" und \"Psycho\" sowie den Toneffekten von Oskar Sala zu \"Die Vögel\", wesentlich zum jeweiligen Gesamteindruck des Films beizutragen.\n\n\nPrägende Einflüsse.\n\nVorbilder.\nHitchcock war beeindruckt von den Filmen, die er in seiner Jugend und in seinen frühen Jahren im Filmgeschäft sah, etwa jenen von D. W. Griffith, Charlie Chaplin, Buster Keaton und Douglas Fairbanks senior. Als Stummfilmregisseur in England übernahm er vom US-Film unter anderem die Technik, mit Hilfe von Beleuchtungseffekten Tiefe zu schaffen und den Vorder- vom Hintergrund abzusetzen, was bis in die 1930er Jahre im britischen Film unüblich war. Angetan war er auch von den deutschen Stummfilmregisseuren wie Fritz Lang und Ernst Lubitsch. F. W. Murnaus \"Der letzte Mann\", dessen Dreharbeiten Hitchcock 1922 in München beobachtete, bezeichnete er später als den fast perfekten Film: „Er erzählte seine Geschichte ohne Titel; von Anfang bis Ende vertraute er ganz auf seine Bilder. Das hatte damals einen ungeheueren Einfluss auf meine Arbeit.“ Einfluss auf Hitchcocks Arbeit hatte auch \"Das Cabinet des Dr. Caligari\", den Robert Wiene 1919 drehte. Die Betonung des Visuellen im deutschen Expressionismus prägte seinen eigenen Umgang mit filmischen Mitteln.\n\nAbgesehen von diesen stilistischen Einflüssen vermied es Hitchcock jedoch, Szenen oder Einstellungen bekannter Filme zu zitieren. Als Ausnahme kann \"Panzerkreuzer Potemkin\" (1925) des sowjetischen Regisseurs Eisenstein angesehen werden. In \"Die 39 Stufen\", \"Über den Dächern von Nizza\" und einigen weiteren Filmen erinnern die vor Entsetzen schreienden Frauen an Einstellungen aus der berühmten und oft zitierten Szene an der Hafentreppe in Odessa.\n\nEs gibt außerdem in Hitchcocks Werk aus den 1940er und 1950er Jahren einige motivische und visuelle Überschneidungen mit der Gattung des Film noir, die den amerikanischen Kriminalfilm in jener Zeit bestimmte, etwa in \"Im Schatten des Zweifels\" und \"Berüchtigt\", und besonders in \"Der falsche Mann\", wo das Motiv der allgegenwärtigen Bedrohung der Hauptfiguren eine Rolle spielt. Darüber hinaus bediente er sich gerne einer ähnlich kontrastreichen Bildgestaltung, die er sich in den Grundzügen allerdings bereits in den 1920er Jahren angeeignet hatte. Auch \"Vertigo\" erinnert in der Grundkonstellation und der alptraumhaften Zwanghaftigkeit der Geschehnisse an einige Filme des Genres, wie zum Beispiel \"Frau ohne Gewissen\", hebt sich jedoch formal und stilistisch deutlich vom Film noir ab.\nAls typischer Vertreter des Genres kann Hitchcock jedenfalls nicht angesehen werden.\n\n\nObsessionen und Vorwürfe wegen sexueller Belästigung.\nSeine Vorliebe für Blondinen erklärte Hitchcock gegenüber Truffaut wie folgt: „Ich finde, die englischen Frauen, die Schwedinnen, die Norddeutschen und die Skandinavierinnen sind interessanter als die romanischen, die Italienerinnen und die Französinnen. Der Sex darf nicht gleich ins Auge stechen. Eine junge Engländerin mag daherkommen wie eine Lehrerin, aber wenn Sie mit ihr in ein Taxi steigen, überrascht sie Sie damit, dass sie Ihnen in den Hosenschlitz greift.“ Ähnlich äußerte er sich 1969 gegenüber \"Look\" über die Truffaut-Schauspielerin Claude Jade, die bei ihm in \"Topaz\" gespielt hatte: „Claude Jade ist eine eher ruhige junge Dame, doch für ihr Benehmen auf dem Rücksitz eines Taxis würde ich keine Garantie übernehmen“.\n\nDass Hitchcock zu seinen jungen blonden Schauspielerinnen ein besonderes Verhältnis hatte und ihnen mehr Aufmerksamkeit widmete als allen anderen, war schon früh bekannt. Die Sorgfalt, mit der Hitchcock bereits in den 1930er und 1940er Jahren Madeleine Carroll, Carole Lombard und insbesondere Ingrid Bergman in Szene setzte, entwickelte sich mit der Zeit zu einer sich steigernden Verquickung privater und beruflicher Interessen, die sich zu einer Obsession ausweitete. Mit Vera Miles probte er die Nervenzusammenbrüche, welche sie in \"Der falsche Mann\" darstellen sollte, wochenlang jeweils mehrere Stunden täglich. Für sie wie für Kim Novak ließ er von der Kostümbildnerin eine komplette Garderobe schneidern, die für ihr privates Leben gedacht war. Tippi Hedren \"(Die Vögel)\" ließ er sogar von zwei Crew-Mitgliedern beschatten und begann, ihr Vorschriften für ihr Verhalten im Privatleben zu machen. Hedren warf in ihrer Biografie 2016 Hitchcock vor, sie mehrfach sexuell belästigt zu haben, und bezeichnete ihn als pervers. Er habe sie für sich völlig vereinnahmen wollen, sich auf sie geworfen und begrapscht. Weiter drohte er ihr ihre Karriere zu ruinieren, wenn sie nicht kooperieren würde.\nDiese Vereinnahmung hatte ihren Höhepunkt in sich über Tage hinziehenden Aufnahmen von auf sie einstürzenden, echten Vögeln. Nach einem eindeutigen, erfolglosen Annäherungsversuch während der Arbeiten zu \"Marnie\" kam es schließlich zum Bruch. Die zuvor offen bekundete Zuneigung schlug ins Gegenteil um, und Hitchcock ließ keine Gelegenheit aus, Tippi Hedren bei anderen herabzusetzen. Sie blieb die letzte typische „Hitchcock-Blondine“. Zwar hielt sich Hitchcock darüber stets äußerst bedeckt, doch es gilt als gesichert, dass der Regisseur sich von diesen Schwierigkeiten lange nicht erholen konnte und in seiner kreativen Schaffenskraft beeinträchtigt war.\nEbenso gelten Filme wie \"Vertigo\" und \"Berüchtigt\", aber auch \"Marnie\" oder \"Im Schatten des Zweifels\", die von neurotischen Männern handeln, die Frauen manipulieren, als stark autobiographisch.\n\nAuch die Verbindung zwischen Sex und Gewalt faszinierte Hitchcock, was vor allem in seinen späteren Werken immer deutlicher zutage tritt. Mehrfach inszenierte er vollendete oder versuchte Vergewaltigungen (schon früh in \"Erpressung\", später dann in \"Marnie\" und \"Frenzy\"). In drei nicht realisierten Projekten sollten Vergewaltiger oder Vergewaltigungen eine zentrale Rolle spielen. Morde inszenierte er einige Male als Vergewaltigungen, mit dem Messer als Phallus-Symbol. Doch auch der Tod durch Erwürgen oder Strangulieren übte eine gewisse Faszination auf ihn aus. Einige Würgeszenen gehören zu den bemerkenswertesten Mordszenen seiner Karriere, etwa in \"Cocktail für eine Leiche\", \"Bei Anruf Mord\", \"Der zerrissene Vorhang\" und \"Frenzy\". Sich selbst ließ er oft in „Würgerposen“ ablichten.\n\nÄhnlich offen kokettierte Hitchcock zeit seines Lebens mit seiner panischen Angst vor der Polizei. Hitchcock erzählte gerne, dass er mit fünf Jahren, nachdem er etwas angestellt hatte, von seinem Vater mit einem Zettel auf das nahegelegene Polizeirevier geschickt worden sei. Der Polizist las den Zettel und sperrte Alfred für fünf oder zehn Minuten in eine Zelle mit dem Kommentar, dies würde die Polizei mit ungezogenen Jungen so machen. In seinen Filmen geht von Polizisten stets eine latente Gefahr aus.\n\nZu der Frage, inwieweit das von Hitchcock in seinen Filmen transportierte Bild der besitzergreifenden Mutter von der eigenen Mutter geprägt ist, gab es von ihm selbst keinerlei Aussagen. Das wenige, was man aus seiner Kindheit weiß, legt jedoch autobiographische Ursprünge nahe. Hitchcocks Mutter starb nach langer Krankheit im August 1942 während der Dreharbeiten zu \"Im Schatten des Zweifels\". Dieser bereits von vornherein stark autobiographisch geprägte Film nimmt eindeutig Bezug auf Hitchcocks Verhältnis zu ihr: Der Name Emma scheint nicht die einzige Gemeinsamkeit zwischen ihr und der dominanten Mutterfigur im Film zu sein. Zudem ist im Film noch von einer anderen gebieterischen, jedoch kranken Mutter die Rede&nbsp;– jener des Krimi-Besessenen Herb Hawkins, der wiederum als Selbstprojektion Hitchcocks gilt.\n\nAuffallend oft sind Toiletten in Hitchcocks Filmen zu sehen oder zu hören, in denen konspirative Dinge irgendwelcher Art stattfinden. Laut seinem Biographen Donald Spoto hatte er eine „pubertäre Fixierung“, die in seiner viktorianischen Erziehung begründet lag. Hitchcock äußerte sich zwar oft und gerne über menschliche Körperfunktionen, wollte aber den Eindruck erwecken, er selbst habe mit solchen Dingen nichts zu tun. Bezugnehmend auf seine Körperfülle, deutete Hitchcock hingegen mehrfach an, dass für ihn Essen eine Art Ersatzbefriedigung sei. So gibt es in einigen Hitchcockfilmen eine symbolische Verbindung von Essen, Sex und Tod.\n\n\nZensur.\nIn den USA galt zwischen 1934 und 1967 der \"Hays Code\", auch \"Production Code\" genannt, eine Sammlung von Richtlinien über die Einhaltung der gängigen Moralvorstellungen und über die Zulässigkeit der Darstellung von Kriminalität, Gewalt und Sexualität im Film.\n\nSo musste Hitchcock zum Beispiel das geplante Ende für \"Verdacht\" fallen lassen, weil es Anfang der 1940er Jahre nicht möglich war, den Selbstmord einer schwangeren Frau zu zeigen. Noch bis kurz vor Schluss der Dreharbeiten hatte er kein passendes Ende für den Film gefunden. In \"Berüchtigt\" musste Hitchcock einen Dialog streichen, in dem sich ein Vertreter der US-Regierung positiv über die Möglichkeit einer Ehescheidung äußerte. Bei \"Saboteure\" drehte er politisch heikle Textstellen zur Sicherheit alternativ in entschärften Versionen.\n\nDoch in vielen Fällen gelang es ihm, die Beschränkungen durch die Zensur kreativ zu umgehen. So war es damals unter anderem nicht erlaubt, eine Toilette zu zeigen. Daher verzerrte Hitchcock in \"Mr. und Mrs. Smith\" die eindeutigen Geräusche einer Toilette so, dass man sie für eine Dampfheizung halten konnte. In \"Psycho\" zeigte er eine Toilette, in der ein Papierzettel hinuntergespült wurde. Indem er das Bild der Toilette mit einer dramaturgischen Funktion versah&nbsp;– das Verschwinden eines Beweisstücks musste erklärt werden&nbsp;– verhinderte er, dass die Szene geschnitten wurde. Niemals wurde eine Toilette zu Zeiten des Hays Code expliziter gezeigt.\n\nDa auch die Länge von Küssen im Film damals auf drei Sekunden begrenzt war, inszenierte Hitchcock den Kuss zwischen Ingrid Bergman und Cary Grant in \"Berüchtigt\" als Folge von einzelnen, durch kurze Dialogsätze unterbrochenen Küssen. Hitchcocks größter Sieg gegen die Zensur war die Schlussszene von \"Der unsichtbare Dritte\": Cary Grant und Eva Marie Saint befinden sich in einem Schlafwagen. Er zieht sie zu sich nach oben in das obere Bett, und sie küssen sich. Es erfolgt ein Umschnitt, und man sieht einen Zug in einen Tunnel rasen&nbsp;– eine der explizitesten Andeutungen des Sexualakts in einem US-Film zu Zeiten des \"Production Code\".\n\n\nArbeitsweise.\nEiner der wichtigsten Aspekte der Arbeitsweise Alfred Hitchcocks war, dass er im Idealfall von der Stoffauswahl bis zum Endschnitt nichts dem Zufall überließ, sondern die völlige Kontrolle über die Herstellung des Films beanspruchte.\n\nWenn Hitchcock existierende Vorlagen benutzte, etwa Romane oder Bühnenstücke, übernahm er nur einzelne Grundmotive der Handlung und entwickelte daraus zusammen mit dem jeweiligen Drehbuchautor oft eine völlig neue Geschichte. Hochwertige, komplexe Literatur sperrte sich gegen diesen Umgang und Hitchcock scheute daher deren Verfilmung – auch aus Respekt vor dem Werk.\nHitchcock war meist an der Drehbucherstellung beteiligt, wurde aber nach 1932 bei keinem seiner Filme offiziell als Autor in Vor- oder Abspann erwähnt: „Ich will nie einen Titel als Produzent oder Autor. Ich habe das Design des Films geschrieben. Mit anderen Worten, ich setze mich mit dem Autor zusammen und entwerfe den ganzen Film vom Anfang bis zum Ende.“ Der Autor Samuel A. Taylor: „Mit ihm zu arbeiten, bedeutete auch mit ihm zu schreiben, was auf die wenigsten Regisseure zutrifft. Hitchcock behauptete nie, selbst ein Schriftsteller zu sein, aber in Wahrheit schrieb er doch seine eigenen Drehbücher, denn er sah bereits jede Szene deutlich in seinem Kopf vor sich und hatte eine sehr genaue Vorstellung davon, wie sie ablaufen sollte. Ich merkte, dass ich nur noch die Figuren persönlicher und menschlicher zu gestalten brauchte und sie weiter entwickeln musste.“ Gelegentlich veränderte Hitchcock im Nachhinein noch die Dialoge ganzer Szenen, etwa um die Spannungs-Dramaturgie zu verbessern (Beispiel: \"Das Rettungsboot\") oder um autobiographische Bezüge einzubauen (Beispiel: \"Ich beichte\").\nAuch wenn ihm geschliffene Dialoge wichtig waren, legte Hitchcock sein Hauptaugenmerk stets auf die Ausdruckskraft der Bilder. So wurde im Idealfall jede einzelne Einstellung des Films vor Drehbeginn in Storyboards festgelegt.\n\nSeit Beginn seiner Regisseurtätigkeit verfolgte er das Ziel, jegliche Improvisation so weit es geht zu vermeiden. Gegenüber Truffaut erklärte er: „Ich habe Angst davor gehabt, im Atelier zu improvisieren, weil, selbst wenn man im Augenblick Ideen hat, bestimmt keine Zeit bleibt nachzuprüfen, was sie taugen. [… Andere Regisseure] lassen ein ganzes Team warten und setzen sich hin, um zu überlegen. Nein, das könnte ich nicht.“\n\nNach eigenen Aussagen bereitete Hitchcock die Planung eines Projekts mehr Freude als die eigentlichen Dreharbeiten: Durch zu viele Einflüsse – Produzenten, Technik, Schauspieler, Zeitdruck – sah er die angestrebte Kontrolle über sein Werk bedroht. Außerdem sah er im Idealfall die kreative Arbeit am Film mit Beginn der Dreharbeiten als abgeschlossen an: „Ich drehe einen vorgeschnittenen Film. Mit anderen Worten, jedes Stück Film ist entworfen, um eine Funktion zu erfüllen.“\n\nDiese Grundsätze waren jedoch eher eine Idealvorstellung Hitchcocks. Tatsächlich wurde es ihm spätestens ab 1948 zur Gewohnheit, beim Drehen Alternativen auszuprobieren. Doch auch hier bemühte er sich um möglichst exakte Vorausplanung: Ein Beispiel hierfür ist die Belagerung des Hauses durch die Vögel in \"Die Vögel\". Gegenüber Truffaut beschrieb Hitchcock, wie er die ursprünglich geplante Szene noch unmittelbar am Drehort umschrieb und bis ins kleinste Detail skizzierte, so dass sie kurz darauf entsprechend diesen neuen Entwürfen gedreht werden konnte. Darüber hinaus wurde Hitchcock im Laufe seiner Karriere immer freier, auch kurzfristig vom festgelegten Drehbuch abzuweichen. Entgegen seinen Gewohnheiten ließ er sogar Improvisationen der Schauspieler zu, wenn auch nur bei eher unwichtigen Szenen.\nBill Krohn ging 1999 in \"Hitchcock at Work\" ausführlich auf Hitchcocks Arbeitsweise ein. Er rekonstruierte auf Basis von Originalunterlagen wie Drehbuchversionen, Skripte, Storyboards, Memos, Produktionsnotizen etc. und mit Hilfe von Beteiligten die Produktionsgeschichte diverser Filme (darunter Hitchcocks berühmteste) und widerlegt Hitchcocks Bekenntnis zum „vorgeschnittenen Films“: So kam es bei vielen Filmen vor, dass Hitchcock entscheidende Schlüsselszenen in verschiedenen Varianten drehte und meist erst im Schneideraum über die endgültige Form einzelner Szenen entschied.\n\n\nMitarbeiter.\nIm Laufe der Jahre entwickelte sich mit verschiedenen Autoren eine besonders kreative Zusammenarbeit. Hervorzuheben sind Eliot Stannard, Angus MacPhail, Charles Bennett, Ben Hecht und John Michael Hayes.\nObwohl Samuel A. Taylor \"(Vertigo)\" und auch Ernest Lehman \"(Der unsichtbare Dritte)\" nur je zwei Drehbücher zu tatsächlich realisierten Filmen schrieben, gehörten sie zu den wenigen Mitarbeitern, die mit ihm in den letzten Jahren seiner Karriere regelmäßig zusammenarbeiteten und bis kurz vor seinem Tod Kontakt hatten.\nDoch auch mit namhaften Theater- oder Romanautoren arbeitete Hitchcock mehrfach bei der Drehbucherstellung zusammen, reibungslos mit Thornton Wilder und George Tabori, konfliktbeladen mit John Steinbeck, Raymond Chandler und Leon Uris.\nDer Kult, den Hitchcock gern um seine Person betrieb, und sein manchmal diktatorischer Stil, führte auch zu Konflikten mit befreundeten Autoren. John Michael Hayes, der im Streit von Hitchcock schied: „Ich tat für ihn, was jeder andere Autor für ihn tat – ich schrieb! Wenn man aber Hitchcocks Interviews liest, kann man den Eindruck bekommen, er habe das Drehbuch geschrieben, die Charaktere entwickelt, die Motivation beigesteuert.“\nWenn Hitchcock mit der Arbeit eines Autors nicht zufrieden war, oder wenn er seine Autorität angegriffen fühlte, dann ersetzte er Autoren kurzerhand durch andere.\n\nCary Grant und James Stewart wurden innerhalb der jeweils vier Filme, die sie für Hitchcock drehten, zu Hitchcocks Alter Ego. Grant wurde zu dem, „was Hitchcock gerne gewesen wäre“, wie es Hitchcocks Biograph Donald Spoto formulierte, während Stewart vieles wäre, „von dem Hitchcock dachte, er sei es selbst“. Mit einigen seiner Schauspieler verband Hitchcock zudem eine langjährige persönliche Freundschaft, allen voran mit Grace Kelly. Darüber hinaus sind die als neurotisch zu bezeichnenden Beziehungen zu seinen blonden Hauptdarstellerinnen – insbesondere mit Tippi Hedren – bekannt.\nAm Anfang von Hitchcocks Karriere galt Film in England als Unterhaltung für die Unterschicht. Aus dieser Zeit stammt Hitchcocks oft zitierter Ausspruch „Alle Schauspieler sind Vieh“, der sich auf diejenigen Theaterschauspieler bezog, die nur mit Widerwillen und des Geldes wegen nebenher als Filmschauspieler arbeiteten. Die Aussage verselbständigte sich später und wurde oft als genereller Ausdruck der Geringschätzung Hitchcocks Schauspielern gegenüber angesehen. Tatsächlich hatte er auch später oft Probleme mit Schauspielern, die eigene Vorstellungen durchsetzen wollten, anstatt sich in die vorgefertigte Planung des Regisseurs einzufügen. Anhänger des Method Actings wie Montgomery Clift und Paul Newman waren Hitchcock daher genauso lästig wie Exzentriker oder Egomanen. Große Achtung hatte Hitchcock hingegen vor Schauspielern, die sein Filmverständnis teilten oder sich zumindest seiner Arbeitsweise anpassten, und gewährte etwa Joseph Cotten und Marlene Dietrich große künstlerische Freiheiten. Oft waren es jedoch die Produzenten, die über die Besetzung der Hauptrollen entschieden. Umso mehr nutzte Hitchcock seine größere Freiheit bei den zu besetzenden Nebenrollen, wobei er gerne auf Theaterschauspieler zurückgriff, die er noch aus seiner Zeit in London in bester Erinnerung hatte, zum Beispiel Leo G. Carroll in insgesamt sechs Filmen oder Cedric Hardwicke in \"Verdacht\" und \"Cocktail für eine Leiche\".\n\nDie bekannte Kostümbildnerin Edith Head, mit der er ab \"Das Fenster zum Hof\" bei fast allen Filmen zusammenarbeitete, meinte: „Loyalität war Hitchcock besonders wichtig. Er war Mitarbeitern gegenüber so loyal, wie er es von ihnen erwartete.“\n\nBei fünf Filmen war Robert F. Boyle für das \"Production Design\" verantwortlich; er gehörte bis zu Hitchcocks Tod zu dessen engsten Mitarbeitern. Außerdem griff er im Laufe seiner Karriere gern auf Albert Whitlock als Szenenbildner zurück. Äußerst zufrieden war Hitchcock, dem die Ausdruckskraft der Bilder stets wichtig war, auch mit dem Art Director Henry Bumstead. Der Titeldesigner Saul Bass entwarf nicht nur einige Filmtitel für die Vorspanne sowie Plakate, sondern war bereits bei den Arbeiten an vielen Storyboards maßgeblich beteiligt.\n\nWichtigster Kameramann in seinen frühen Jahren bei den British International Pictures war John J. Cox. Über Hitchcock sagte Kameramann Robert Burks, der mit Ausnahme von \"Psycho\" an allen Filmen zwischen 1951 und 1964 beteiligt war: „Man hatte nie Ärger mit ihm, solange man etwas von seiner Arbeit verstand und sie ausführte. Hitchcock bestand auf Perfektion.“ Mit Leonard J. South, ehemaliger Assistent Burks’, arbeitete Hitchcock über einen Zeitraum von insgesamt 35 Jahren zusammen.\n\nVon den Komponisten der Filmmusiken ist Louis Levy hervorzuheben, der die Soundtracks für die frühen englischen Filme von \"Der Mann, der zuviel wußte\" bis \"Eine Dame verschwindet\" beisteuerte. Als der Hitchcock-Komponist schlechthin gilt Bernard Herrmann, der ab \"Immer Ärger mit Harry\" bis einschließlich \"Marnie\" (1964) alle Filmmusiken für Hitchcock komponierte.\n\nDer Filmeditor George Tomasini war bis zu seinem Tod 1964 ein Jahrzehnt lang enger Mitarbeiter Hitchcocks. Zu Beginn seiner Karriere wirkte seine Frau Alma als Editorin bei seinen Filmen mit; sie blieb bis zuletzt eine der einflussreichsten Mitarbeiterinnen.\n\n\nSelbstvermarktung.\nSchon zu Beginn seiner Karriere war Hitchcock die Bedeutung der Vermarktung der eigenen Person bewusst: Viele seiner späteren Tätigkeiten sind Teil einer Strategie, sich und seinen Namen als Marke zu etablieren. Bereits 1927 führte Hitchcock ein stilisiertes Selbstporträt als Logo, das bis heute bekannt ist. Anfang der 1930er Jahre, als er mit dem Erfolg seiner Filme in England populär wurde, gründete er mit der Hitchcock Baker Productions Ltd. eine Gesellschaft, die bis zu seiner Übersiedlung nach Amerika ausschließlich dafür zuständig war, für ihn und mit seiner Person Öffentlichkeitsarbeit zu betreiben. Anschließend wurden diese Aufgaben von der Künstleragentur Selznick-Joyce, danach von der Music Corporation of America (MCA) wahrgenommen, wobei der Präsident der MCA, Lew Wasserman, zu seinem persönlichen Agenten wurde. 1962 wurde unter Herman Citron eine neue Gesellschaft gegründet, die Hitchcocks Interessen vertrat und seinen Namen vermarktete. Diese Selbstvermarktung diente auch dazu, eine Machtposition im Produktionsprozess seiner Filme zu erlangen, und war somit Teil seines Kampfes um künstlerische Unabhängigkeit.\n\n\nCameos.\nAus Mangel an Statisten in seinen ersten britischen Filmen sah man Hitchcock immer wieder im Hintergrund auftauchen. Daraus entwickelte er eines seiner bekanntesten Markenzeichen: Hitchcocks obligatorischer Cameo. Da das Publikum mit der Zeit immer weniger auf die Handlung achtete, als vielmehr auf Hitchcock lauerte, legte er in späteren Filmen diesen Running Gag möglichst weit an den Filmanfang.\n\nIn drei Filmen hatte Hitchcock keinen eigentlichen Cameo. In zwei von diesen Filmen trat er auf Fotos in Erscheinung: \"Das Rettungsboot\" spielt ausschließlich in einem kleinen Rettungsboot auf dem Meer. Er ist daher in einer zufällig im Boot liegenden Zeitung in einer Werbeanzeige für eine Diät auf einem „Vorher-Nachher-Foto“ zu sehen. Auch in \"Bei Anruf Mord\" war kein Auftritt möglich. Stattdessen taucht Hitchcock auf einem an der Wand hängenden Foto einer Wiedersehensfeier von College-Absolventen auf. In \"Der falsche Mann\" schließlich tritt er am Anfang des Films persönlich auf und spricht den Prolog. Dies ist gleichzeitig seine einzige Sprechrolle in einem seiner Kinofilme.\n\n\nTrailer.\nWährend von den Filmgesellschaften üblicherweise für die Vermarktung eigene Abteilungen oder externe Agenturen beauftragt werden, trugen bei Hitchcocks Filmen die Werbekampagnen deutlich die Handschrift des Regisseurs. Seine Kino-Trailer waren häufig nicht nur Zusammenschnitte des angekündigten Films: Mit steigendem Wiedererkennungswert seiner Person stellte Hitchcock in der Rolle eines „Master of Ceremony“ seine eigenen Filme vor und führte den Zuschauer humorvoll durch die Kulissen. Oftmals sprach er auch die deutschsprachigen Trailer selbst ein.\n\n\nFernsehen.\nAuf den Rat seines Agenten Lew Wasserman hin stieg Hitchcock 1955 in das Fernsehgeschäft ein. Hitchcock gründete die Fernsehproduktionsfirma \"Shamley Productions\" und produzierte bis 1965 seine eigene wöchentliche Fernsehserie. Am Anfang vieler Folgen begrüßte Hitchcock das Publikum, indem er mit ungerührter Miene makabre Ansagetexte sprach. Die Moderationen, die ihn zu einer nationalen Berühmtheit machten, wurden von dem Bühnenautor James D. Allardice verfasst, der fortan bis zu seinem Tod 1966 für Hitchcock auch als Redenschreiber arbeitete. Als Titelmusik für die Serie \"Alfred Hitchcock Presents\" verwendete Hitchcock das Hauptthema von Charles Gounods \"Marche funèbre d’une marionette\" (Trauermarsch einer Marionette), das sich im Weiteren zu einer Erkennungsmelodie für Hitchcocks Öffentlichkeitsarbeit entwickelte.\n\n\nBücher und Zeitschriften.\n1956 schloss Hitchcock einen Lizenzvertrag mit HSD Publications ab, der die Überlassung seines Namens für das Krimi-Magazin \"Alfred Hitchcock’s Mystery Magazine\" zum Inhalt hatte. Die Zeitschrift enthält Mystery- und Kriminalgeschichten, Buchrezensionen und Rätsel und erscheint noch heute. Einführungen und Vorworte, die mit seinem Namen unterschrieben waren, wurden stets von Ghostwritern verfasst.\n\nVon 1964 bis 1987 erschien in den USA die Jugend-Krimi-Reihe „The Three Investigators“, auf Deutsch seit 1968 \"Die drei ???\". Der Journalist und Autor Robert Arthur kannte Alfred Hitchcock persönlich und bat ihn, seinen Namen zur Vermarktung dieser geplanten Buchreihe verwenden zu dürfen. Schließlich baute er die Figur „Alfred Hitchcock“ in die Handlung ein. Anders als in Europa hielt sich der Erfolg der Bücher in den USA in Grenzen. In Deutschland, wo die Bücher besonders populär waren, entstand die gleichnamige Hörspielreihe. Durch diese bis heute erfolgreichste Hörspielproduktion der Welt wurde der Name Hitchcock auch bei vielen bekannt, die mit seinem filmischen Schaffen nicht vertraut waren.\n\n\nWirkung.\nViele Elemente aus seinem Werk sind inzwischen in das Standardrepertoire des Kinos eingegangen, ohne dass sie noch bewusst oder direkt mit Hitchcock in Verbindung gebracht werden, insbesondere der Einsatz von Suspense als spannungserzeugendem Mittel oder die Verwendung von MacGuffins als handlungsvorantreibendes Element. Darüber hinaus gibt es seit den 1940er Jahren unzählige Beispiele für Thriller oder Dramen, teils von sehr namhaften Regisseuren, in denen typische Motive Hitchcocks oder seine Stilelemente bewusst kopiert oder variiert werden. Manche dieser Filme sind als Hommage des jeweiligen Regisseurs an Hitchcock zu verstehen, in anderen Fällen wurde Hitchcocks Stil übernommen, da er sich als erfolgreich und wirksam erwiesen hat.\n\n\nUSA.\nInsbesondere Hitchcocks Erfolgsfilme aus den 1950er bis Anfang der 1960er Jahre inspirierten in den Folgejahren Hollywood-Produktionen, die inhaltlich oder stilistisch oft mit Hitchcock in Verbindung gebracht werden.\n\nZu den vielen Hollywood-Regisseuren, die Alfred Hitchcock mehr oder weniger direkt beeinflusste, zählt Brian De Palma, der mit vielen Verweisen und Zitaten auf Hitchcocks Werk arbeitet. Überdies übernahm er in einigen Filmen Grundstrukturen aus dessen Filmen. So entwickelt er in \"Dressed to Kill\" (1980) das Grundmotiv aus \"Psycho\" weiter und zitiert aus weiteren Hitchcock-Filmen. 1976 lehnte sich \"Schwarzer Engel\" stark an \"Vertigo\" an. 1984 spielt de Palma in \"Der Tod kommt zweimal\" mit eindeutigen Bezügen auf \"Das Fenster zum Hof\" und \"Vertigo\".\n\nAuch wenn Steven Spielberg selten direkt stilistische Motive kopiert oder adaptiert und nur wenige seiner Filme thematische Parallelen aufzeigen, erinnert \"Der weiße Hai\" (1975) in Spannungsaufbau und Dramaturgie an \"Die Vögel\" und ist die \"Indiana-Jones-\"Filmreihe (1981–1989) stark von \"Der unsichtbare Dritte\" (1959) beeinflusst. Auch ein Film wie \"Schindlers Liste\" (1993) wäre in dieser Form ohne den Einfluss Hitchcocks nicht möglich gewesen. Der von Hitchcocks Kameramann Irmin Roberts entwickelte Vertigo-Effekt wird bisweilen auch als „Jaws Effect“ bezeichnet, da Spielberg diese relativ schwierig umzusetzende Kameraeinstellung im \"Weißen Hai\" (Originaltitel: \"Jaws\") als einer der ersten prominenten Regisseure 16&nbsp;Jahre nach \"Vertigo\" einsetzte. Inzwischen gehört dieser emotional sehr wirkungsvolle Kameratrick zum Standardrepertoire des Hollywood-Kinos.\n\nWeitere amerikanische Regisseure, die erkennbar von Hitchcock beeinflusst wurden oder sich auf dessen Werk berufen, sind John Carpenter, David Fincher, David Mamet, Quentin Tarantino, Martin Scorsese, David Lynch und M. Night Shyamalan.\n\n\nFrankreich.\nBereits seit Mitte der 1950er Jahre war Hitchcock insbesondere in Frankreich bei den Vertretern der Nouvelle Vague hoch angesehen. 1957 veröffentlichten die damaligen Filmkritiker und späteren Regisseure Éric Rohmer und Claude Chabrol das erste Buch über ihn. 1956 erschien ein Sonderheft der \"Cahiers du cinéma,\" das maßgeblich zu Hitchcocks Popularität in Frankreich beitrug. Als er im Mai 1960 zu einem Filmfestival reiste, das die Cinémathèque française ihm zu Ehren in Paris abhielt, wurde er von Dutzenden jungen Filmemachern frenetisch gefeiert. Die internationale Ausgabe der \"Herald Tribune\" schrieb, dass Hitchcock in dieser Woche „das Idol der französischen Avantgarde geworden“ sei.\n\nIm August 1962 gab Hitchcock dem damals dreißigjährigen französischen Filmkritiker und Regisseur François Truffaut ein fünfzigstündiges Interview. Truffaut befragte Hitchcock chronologisch zu dessen bisherigen achtundvierzig Filmen. Das Interview erschien 1966 als \"Mr. Hitchcock, wie haben Sie das gemacht?\" in Buchform und gilt als Standardwerk der Filmliteratur. Einzelne Filme Truffauts zeigen den Einfluss Hitchcocks deutlich, etwa \"Die Braut trug schwarz\" (1968) oder \"Das Geheimnis der falschen Braut\" (1969), die Geschichte eines Mannes, der einer Betrügerin und Mörderin verfällt und auch nicht von ihr lassen kann, als sie ihn zu töten versucht. Der Film ist stark von verschiedenen inhaltlichen und stilistischen Motiven aus \"Vertigo\", \"Marnie\" und \"Verdacht\" beeinflusst. Sein letzter Film \"Auf Liebe und Tod\" (1983), in dem ein unschuldiger Mann eines Mordes beschuldigt wird, ist voll von hitchcockschen Motiven und Anspielungen auf dessen Werk. Weitere Filme, die Truffaut selbst in der Tradition Hitchcocks sah, waren \"Die süße Haut\" und \"Fahrenheit 451\". 1968/69 besetzte Hitchcock die bevorzugte Schauspielerin Truffauts, Claude Jade, für seinen Film \"Topas\".\n\nIn vielen Filmen von Claude Chabrol wird eine scheinbar heile bürgerliche Welt angegriffen und durcheinandergebracht. Die hitchcockschen Hauptmotive der Schuldübertragung sowie der doppelten oder der gespaltenen Persönlichkeit tauchen bei Chabrol immer wieder auf. Einige Beispiele sind \"Schrei, wenn du kannst\" (1959), \"Das Auge des Bösen\" (1962), \"Der Schlachter\" (1970) und \"Masken\" (1987). Neben Chabrol und Truffaut haben sich in Frankreich unter anderen auch Henri-Georges Clouzot und René Clément des hitchcockschen Repertoires bedient.\n\n\nÜbriges Europa.\nAußerhalb Frankreichs war in Europa der unmittelbare Einfluss Hitchcocks auf andere Filmemacher deutlich geringer. Einige europäische oder europäischstämmige Regisseure haben jedoch einzelne Filme gedreht, denen eine Stilverwandtschaft anzuerkennen ist oder die unmittelbar als Hommage an Hitchcock gedacht sind, zum Beispiel \"Ministerium der Angst\" von Fritz Lang (1943), \"Der dritte Mann\" von Carol Reed (1949), \"Zeugin der Anklage\" von Billy Wilder (1957), \"Frantic\" von Roman Polański (1988) und \"Schatten der Vergangenheit\" von Kenneth Branagh (1991).\n\n\nFilmografie.\nAlle Filme, an denen Hitchcock beteiligt war, in der Reihenfolge ihrer Produktion:\n\n\n1940–1947.\nIn diese Phase fällt auch Hitchcocks einzige Mitarbeit an einem längeren Dokumentarfilm \"(German Concentration Camps Factual Survey)\" von Mai bis Juli 1945 in London. Er hat dies später im Interview als seinen Beitrag zum Krieg bezeichnet. Der Film wurde nicht fertiggestellt.\n\n\n1948–1957.\nZwischen 1955 und 1965 trat der Regisseur in insgesamt 360&nbsp;Folgen der Fernsehserien \"Alfred Hitchcock Presents\" (267&nbsp;Folgen) und \"The Alfred Hitchcock Hour\" (93&nbsp;Folgen) in der Rolle des Gastgebers auf.\n\n\nAuszeichnungen.\nHitchcock wurde sechsmal für den Oscar nominiert: fünfmal für die Beste Regie, einmal für den Besten Film (als Produzent). Alle sechs Mal ging er leer aus, was ihn zu dem Kommentar veranlasste: „Immer nur Brautjungfer, nie die Braut“. Dennoch blieb er nicht oscarlos, denn 1968 gewann er den Irving G. Thalberg Memorial Award als Spezialoscar für besonders kreative Filmproduzenten. Zudem wurde Rebecca 1941 mit dem Oscar für den besten Film ausgezeichnet, den aber nicht Hitchcock, sondern der Produzent David O. Selznick entgegennehmen durfte.\n\n\nEr wurde mit zwei Sternen auf dem Hollywood Walk of Fame geehrt. Den einen in der Kategorie Film findet man bei der Adresse 6506 Hollywood Blvd, den anderen in der Kategorie Fernsehen am 7013 Hollywood Blvd.\n\nRegie: Julian Jarrold; Besetzung: Toby Jones (Alfred Hitchcock), Sienna Miller (Tippi Hedren), Imelda Staunton (Alma Reville Hitchcock), Conrad Kemp (Evan Hunter), Penelope Wilton (Peggy Robertson)\nRegie: Sacha Gervasi; Besetzung: Anthony Hopkins (Alfred Hitchcock), Helen Mirren (Alma Reville Hitchcock), Scarlett Johansson (Janet Leigh), Danny Huston (Whitfield Cook), Toni Collette (Peggy Robertson), Michael Stuhlbarg (Lew Wasserman), Michael Wincott (Ed Gein), Jessica Biel (Vera Miles), James D’Arcy (Anthony Perkins)\n\n\n\n\nLiteratur.\n\nWerkschauen.\nSortiert in der chronologischen Reihenfolge der jeweiligen Originalausgabe.\n\n\n\n\nAnmerkungen.\nHauptquellen sind die beiden Biografien von Taylor und Spoto sowie die Bücher von Truffaut und Krohn.\n"}
{"id": "78", "url": "https://de.wikipedia.org/wiki?curid=78", "title": "Auteur-Theorie", "text": "Auteur-Theorie\n\nDie Auteur-Theorie (von frz. „Auteur“ = Autor) ist eine Filmtheorie und die theoretische Grundlage für den Autorenfilm&nbsp;– insbesondere den französischen&nbsp;– in den 1950er Jahren, der sich vom „Produzenten-Kino“ abgrenzte. Auch heute noch wird die Definition des \"Auteur\"-Begriffs ständig weiterentwickelt. Im Zentrum des Films steht für die Auteur-Theorie der Regisseur oder Filmemacher als geistiger Urheber und zentraler Gestalter des Kunstwerks.\n\n\nGeschichte der Auteur-Theorie.\nEnde der 1940er Jahre wurde eine erste Auteur-Theorie von dem französischen Filmkritiker Alexandre Astruc formuliert, indem er die Frage nach dem geistigen Besitz eines Films aufwarf.\nIm traditionellen Schaffensprozess lassen sich die Anteile von Drehbuchautor, Kameramann und Filmregisseur am Gesamtwerk nur schwer zuordnen. Durch die Zuteilung der Teilaufgaben als Honorartätigkeit durch die Filmgesellschaften leide die Kreativität, so die These. Im Umkehrschluss fordert diese Theorie die Zusammenführung der Tätigkeiten zu einer kreativen Einheit. Er formulierte seinen Entwurf in dem Aufsatz „\"La caméra-stylo\"“. Die Kamera sollte wie ein Stift verwendet werden. Er war sich sicher, dass bedeutende Schriften in Zukunft nicht mehr als Text, sondern mit der „Kamera geschrieben“ würden.\n\nDoch durchgesetzt haben sich solche und ähnliche Ideen der Auteur-Theorie erst in den 1950er Jahren. Deren gängiger Begriff als Wegbereiter für die heutige Auteur-Theorie lautete zunächst \"politique des auteurs\" (Autoren-Politik), was erst im Laufe der Zeit zur \"Theorie\" umgeformt wurde. Das Wort \"politique\" bzw. Politik stand hier also eher für Parteilichkeit, welche für filmwissenschaftliche Diskussionen eher hinderlich ist (siehe unten).\n\nDie \"politique des auteurs\" wurde zu dieser Zeit von einer Gruppe von jungen Filmkritikern um André Bazin entwickelt, die für die Filmzeitschrift \"Cahiers du cinéma\" schrieben. Eine wesentliche Rolle spielte dabei François Truffaut: Im Januar 1954 veröffentlichte er seinen Aufsehen erregenden Aufsatz \"Eine gewisse Tendenz im französischen Film\" (\"Une certaine tendance du cinéma français\"), in dem er sich mit scharfer Polemik gegen den etablierten französischen „Qualitätsfilm“ wandte. Bei diesem trat der Regisseur gegenüber dem Drehbuchautor und dem Autor der literarischen Vorlage oft in den Hintergrund. Truffaut plädierte dagegen für einen Film, bei dem Form und Inhalt vollständig vom Regisseur selbst als dem eigentlichen „auteur“ des Films bestimmt werden. Er fand das bei traditionell als Autoren ihrer Filme betrachteten europäischen Regisseuren wie Luis Buñuel, Jean Renoir und Roberto Rossellini, außerdem aber auch und vor allem bei Regisseuren wie Alfred Hitchcock, Howard Hawks, Fritz Lang und Vincente Minnelli, die (zum großen Teil als Vertragsregisseure) im Studiosystem Hollywoods arbeiteten, deren Filme aber trotzdem einen persönlichen Stil aufweisen.\n\nDas Konzept des Regisseurs als \"auteur\" seiner Filme wurde für die Filmkritik der \"Cahiers du cinéma\" bestimmend, und damit für die Regisseure der Nouvelle Vague, die daraus hervorgingen, neben Truffaut etwa Jean-Luc Godard, Jacques Rivette oder Claude Chabrol – Filmemacher, die sich zur Umsetzung ihrer künstlerischen Ziele einer jeweils ganz eigenen filmischen Form bedienten.\n\nRoland Barthes hingegen misst in seinem Essay \"La mort de l'auteur\" (1968, \"Der Tod des Autors\") dem Autor für die Literatur eine weitaus geringere Bedeutung bei, als es bisher der Fall war. Der „Auteur-Dieu“ („Autoren-Gott“) wird von Barthes durch den „écrivain“ (den Schriftsteller) ersetzt und folgt damit einer Kritik, die Julia Kristeva bereits 1967 in ihrem Aufsatz \"Bakhtine, le mot, le dialogue et le roman\" (\"Bachtin, das Wort, der Dialog und der Roman\", 1972) aufbrachte.\n\nFür den europäischen Film blieb die Auteur-Theorie aber noch bis in die 1970er prägend. Danach setzte auch hier eine Abkehr von der „verhängnisvollen Macht der Regisseure“ (Günter Rohrbach) ein. Wirtschaftlicher Druck zwang zur Rückkehr zu einer arbeitsteiligen Produktionsweise, wie sie für den Produzenten-Film charakteristisch ist. Damit einher ging notwendigerweise auch wieder die Einigung aller Beteiligten auf einen kleinsten gemeinsamen Nenner und somit auch häufig eine gewisse Banalisierung der Filminhalte, die umso stärker zu Tage tritt, je weniger der Produzent als Projektverantwortlicher in den eigentlichen schöpferischen Prozess eingebunden ist.\n\nIn der Filmwissenschaft wurden auch immer neue Autorschaften von Teammitgliedern entdeckt. In der Realität ist Film Teamarbeit und es ist dem Film nicht anzusehen, ob zum Beispiel die Idee für eine Einstellung nun vom Regisseur oder vom Kameramann stammt. Im Dogma-Film ist der Kameramann nicht weisungsgebunden. Die „Polnische Schule“ bindet den Kameramann bereits in den Prozess des Drehbuchschreibens ein. Unerfahrene Regisseure sind meist sehr auf die Kreativität des Kameramanns oder der Kamerafrau und anderer Teammitglieder angewiesen.\n\nDurch das Aufkommen digitaler Aufnahmetechniken wie Digital Video seit Ende der 1990er Jahre sehen viele Filmemacher, wie etwa Wim Wenders, wieder günstigere Bedingungen für individuelle, subjektive Produktionen gegeben.\n\n\nKritik und Diskussion.\nDie von François Truffaut und Jean-Luc Godard proklamierte „politique des auteurs“ (Autorenpolitik) der fünfziger Jahre war ursprünglich ein Versuch, bestimmte Regisseure wie Alfred Hitchcock als Künstler anzuerkennen, die ihre völlig eigene Bildsprache entwickelten oder, wie Truffaut selber, sämtliche Aspekte ihrer Filme selbst bestimmten. Ein Autorenfilmer ist demnach ein Regisseur, der einen Film – möglichst ohne Kompromisse – so gestaltet, wie er ihn selbst haben möchte.\n\nDie „politique des auteurs“ geriet schnell in die Kritik. Kritiker wie Andrew Sarris und Peter Wollen wiesen auf ein empirisches Problem hin: Niemand kann beweisen, wie viel Einfluss der Regisseur wirklich auf seine Filme hatte bzw. welchen Einfluss Form und Inhalt wirklich auf das haben, was wir als Autorschaft wahrnehmen.\n\nAls Beispiel hierfür gilt der Vorspann von \"Vertigo – Aus dem Reich der Toten\" (1958), den Alfred Hitchcock nicht selbst angefertigt hat, oder die Tatsache, dass viele seiner Filme auf einer Buchvorlage fremder Autoren basieren und selbst die Drehbücher selten von ihm selbst stammten. Gerade Hitchcock aber ist eine zentrale Figur in der „politique des auteurs“.\n\nWie der Name „politique des auteurs“ sagt, handelte es sich um eine Politik, einen gezielten polemischen Eingriff. Der Village-Voice-Kritiker Andrew Sarris übersetzte „politique des auteurs“ jedoch 1962 mit „auteur theory“, wobei unklar blieb, in welchem Sinne es sich hier tatsächlich um eine \"Theorie\" handelt. Sarris popularisierte diese „Theorie“ im englischen Sprachraum und benutzte sie vor allem, um die absolute Überlegenheit des Hollywood-Kinos darzulegen, war er doch davon überzeugt, es sei „the only cinema in the world worth exploring in depth beneath the frosting of a few great directors at the top“. Nun war die Frage: Wo ist die Grenze? \"Wen\" oder vielmehr \"was\" nehmen wir als Autor wahr?\n\nSoziologisch gesehen war die Autorentheorie eine Distinktionsstrategie junger Kritiker, die auf sich aufmerksam machen wollten. Godard hat dies später offen zugegeben: „Wir sagten von Preminger und den anderen Regisseuren, die für Studios arbeiteten, wie man heute fürs Fernsehen arbeitet: ‚Sie sind Lohnempfänger, aber gleichzeitig mehr als das, denn sie haben Talent, einige sogar Genie …‘, aber das war total falsch. Wir haben das gesagt, weil wir es glaubten, aber in Wirklichkeit steckt dahinter, dass wir auf uns aufmerksam machen wollten, weil niemand auf uns hörte. Die Türen waren zu. Deshalb mussten wir sagen: Hitchcock ist ein größeres Genie als Chateaubriand.“\n\nIn den siebziger Jahren folgte dann die stärkste Kritik an der „politique des auteurs“. Roland Barthes proklamierte bereits 1968 vor einem poststrukturalistischen Hintergrund den „Tod des Autors“. Der Autor wurde nun aufgrund des empirischen Dilemmas der Beweisbarkeit von Autorschaften als Image-Figur erkannt, die sich aus ihrer Umwelt formt und in die Werke einschreibt. Auch von feministischer Seite wurde die „politique des auteurs“ scharf angegriffen, diene sie doch dazu, den kollektiven Charakter des Filmemachens zu verdecken und in der Tradition patriarchaler Heldenverehrung Männer zu Superstars zu stilisieren. Claire Johnston verteidigte den Ansatz insofern, als dieser einer zu monolithischen Sicht des Hollywood-Kinos entgegenwirke.\n\nIn den neunziger Jahren schließlich ging die Tendenz zu der Annahme, dass Autorschaften zum Großteil (z. T. kommerziell) konstruiert sind. Timothy Corrigan nennt dies den „commercial auteur“. Es wird damit gerechnet, dass das Publikum den Film eines als Autor bekannten Regisseurs als z. B. „Der neue Woody Allen!“ wahrnimmt, ohne wirklich zu wissen, wie viel Einfluss Woody Allen tatsächlich auf den Film hatte.\nDana Polan verfolgte einen weiteren interessanten Ansatz: Er sieht den „auteurist“ als Hauptverantwortlichen für konstruierte Autorenbilder. Das sind Kritiker, die den Autor als höchste Instanz suchen und damit – wie François Truffaut – auf einen Filmemacher als Künstler hinweisen wollen und nebenbei ihre eigene Erkenntniskraft zelebrieren. Der Begriff dafür lautet „Auteur Desire“. Dieser Ansatz zeigt noch einmal den größten Vorwurf gegenüber der „politique des auteurs“ auf. Doch trotzdem ist die Nennung eines Regisseurs parallel zu&nbsp;– beispielsweise&nbsp;– einem Buchautor als Schöpfergeist auch unter reflektierenden Filmkritikern und -wissenschaftlern weiterhin außerordentlich beliebt. Steckt also doch mehr dahinter?\n\nEin neuerer Ansatz, die kontextorientierte Werkanalyse von Jan Distelmeyer, versucht diese Frage zu klären. Als Grundlage dienen Publikums- und Kritikerrezeption auf der einen Seite und die Konstruktion des Autors aus Biografie, Filmindustrie und kulturellem Umfeld auf der anderen Seite.\nDiese zweiseitige Annäherung erkennt das empirische Dilemma der Definition von „auteur“ an und maßt sich auch keine Bestimmung dessen an, was jetzt eigentlich das Werk von Autor XYZ ist. Viele andere Filmtheoretiker verfolgen heutzutage ähnliche Konzepte. Doch auch eine solch freie Handhabung kann das Problem nicht vollständig lösen, da die wichtigsten Elemente variabel sind und sich so einer eindeutigen Aussage verschließen.\n\nDer Schwerpunkt kritischer Tendenzen liegt also zum Großteil in der Empirie. Einen Filmemacher als „auteur“ anzuerkennen fordert uneingeschränktes Vertrauen in seine Aussagen, wie viel Einfluss er auf seine eigenen Filme hatte. Da dies in Zeiten einer sehr starken Vermarktung aller möglichen mehr oder weniger (un)abhängigen Regisseure seitens von Filmindustrie und Verleih ein fast aussichtsloses Unterfangen ist, ist ein Restzweifel und das stete Hinterfragen der „auteur“-Definition angebracht.\n\n"}
{"id": "79", "url": "https://de.wikipedia.org/wiki?curid=79", "title": "Aki Kaurismäki", "text": "Aki Kaurismäki\n\nAki Olavi Kaurismäki (* 4. April 1957 in Orimattila) ist ein vielfach preisgekrönter finnischer Filmregisseur.\n\n\nLeben und Werk.\nAki Kaurismäki studierte an der Universität Tampere Literatur- und Kommunikationswissenschaften. Neben diversen Aushilfsjobs, etwa als Briefträger oder in der Gastronomie, war er Herausgeber eines universitären Filmmagazins. Darüber hinaus schrieb er von 1979 bis 1984 Filmkritiken für das Magazin \"Filmihullu\". Das erste Drehbuch folgte 1980 für den mittellangen Film \"Der Lügner (Valehtelija)\", bei dem sein Bruder Mika Regie führte.\n\nKaurismäkis Filme thematisieren häufig Schicksale von gesellschaftlichen Außenseitern in städtischen Zentren wie Helsinki. Sie sind nicht nur für ihre sparsamen Dialoge, sondern auch für einen skurril-lakonischen Humor bekannt. Kaurismäki arbeitet regelmäßig mit einem festen Stamm befreundeter Schauspieler und Musiker, die seine Filme auch stilistisch geprägt haben: Matti Pellonpää, Kati Outinen, Kari Väänänen und Sakke Järvenpää. Als Reminiszenz an Alfred Hitchcock hat er in seinen Filmen gelegentlich Cameo-Auftritte, was auch Hitchcock zu tun pflegte.\n\nIn Deutschland wurden seine Filme zum ersten Mal 1986 auf dem Filmfestival \"Grenzland-Filmtage\" in Selb gezeigt. Aki Kaurismäki führte dabei die Filme \"Der Lügner, Calamari Union\" und \"Crime and Punishment\" persönlich vor. Während des Festivals schrieb er das Drehbuch für seinen Film \"Schatten im Paradies,\" den er 1988 erneut persönlich bei den Grenzland-Filmtagen in Selb präsentierte. Dieser Film brachte ihm den internationalen Durchbruch. Ein Großteil der Filmmusik kam von der Band \"Nardis\" aus Erlangen, die Kaurismäki 1986 auf den Grenzland-Filmtagen kennengelernt hatte.\nDem breiten deutschen Publikum bekannt wurde der finnische Regisseur durch seine Teilnahme an der Berlinale 1988. Für großes Aufsehen sorgte Kaurismäki im Herbst 2006, als er sich weigerte, seinen Film \"Lichter der Vorstadt\" als offiziellen finnischen Beitrag für eine Oscar-Nominierung in der Kategorie Bester fremdsprachiger Film zuzulassen, obwohl das Drama von der finnischen Filmkammer einstimmig ausgewählt worden war. Kaurismäki begründete seine Ablehnung mit seiner seit Jahren vertretenen kritischen Haltung gegen den Irak-Krieg der USA.\n\nZusammen mit seinem Bruder Mika Kaurismäki gründete er das Midnight Sun Film Festival im lappischen Sodankylä sowie die Verleihfirma Villealfa. Der Name geht zurück auf die Figur Ville Alfa, den Protagonisten im Film \"Der Lügner\". Gleichzeitig handelt es sich um ein Anagramm von \"Alphaville,\" einem Film von Jean-Luc Godard.\n\n1989 emigrierte Kaurismäki mit seiner Frau nach Portugal, weil „es in ganz Helsinki keinen Platz mehr gebe, wo er seine Kamera noch postieren könne“.\n\nRainer Gansera, der für die Zeitschrift epd Film mit dem „Chef-Melancholiker des europäischen Autorenkinos“ 2006 in Hof gesprochen hat, zeigte sich auch von seinem Auftreten persönlich beeindruckt und beschrieb atmosphärisch:\n\nAls persönliche Leitbilder will Kaurismäki Bresson, Ozu und Godard gesehen haben, der Ausbildung an den Filmhochschulen seines Landes dagegen habe er nicht viel Positives abgewinnen können.\nBei \"Pandora\" sind Ende 2006 als „Aki Kaurismäki DVD-Collection“ 14 Spielfilme und fünf Kurzfilme (mit digital restaurierten Bildern) in vier Boxen erschienen.\n\n2011 stellte Kaurismäki nach fünf Jahren mit \"Le Havre\" einen Spielfilm fertig, der ihm wieder einmal eine Einladung in den Wettbewerb der Filmfestspiele von Cannes einbrachte. Der in Frankreich gedrehte Film handelt von einem Schuhputzer aus der gleichnamigen Hafenstadt, der sich eines illegalen Flüchtlingskindes aus Afrika annimmt. \"Le Havre\" gewann in Cannes den FIPRESCI-Preis.\n\nFür den Spielfilm \"Die andere Seite der Hoffnung\" erhielt Kaurismäki 2017 eine Einladung in den Wettbewerb der 67. Internationalen Filmfestspiele Berlin. Der Film spielt in Helsinki und erzählt von der Begegnung eines älteren finnischen Handelsvertreters (dargestellt von Sakari Kuosmanen) mit einem jungen syrischen Flüchtling (Sherwan Haji). Der Film kam am 3. Februar 2017 in die finnischen Kinos.\n\n\n\n\n\n"}
{"id": "81", "url": "https://de.wikipedia.org/wiki?curid=81", "title": "Anime", "text": "Anime\n\nAnime (jap. , [], deutsch häufig [], Plural: \"Animes\") bezeichnet in Japan produzierte Zeichentrickfilme. In Japan selbst steht \"Anime\" für alle Arten von Animationsfilmen und -serien, für die im eigenen Land produzierten ebenso wie für importierte. Er bildet das Pendant zum Manga, dem japanischen Comic. Japan besitzt die umfangreichste Trickfilmkultur weltweit.\n\n\nDefinition und Begriffsgeschichte.\nIm Japanischen kann „Anime“ jegliche Animationsfilme bezeichnen, sowohl die aus dem eigenen Land als auch aus dem Ausland. Außerhalb Japans wird der Begriff ausschließlich für Animationsfilm japanischer Herkunft verwendet. Historisch wurden Animationsfilme in Japan lange Zeit nicht \"Anime\" genannt. Zu Beginn des 20. Jahrhunderts gab es zunächst die Worte \"senga\" („Linienkunst“) und \"kuga\" („Klapp-Bilder“, vgl. Daumenkino), \"dekobō shin gachō\" („schelmische neue Bilder“) oder \"chamebō-zu\" („verspielte Bilder“). Später kamen \"manga eiga\" (, „Manga-Filme“) und \"dōga\" (, „bewegte Bilder“) auf. Erst in den 1970er Jahren tritt das Wort \"Anime\" in Japan zusammen mit der Wortschöpfung \"Japanimation\" auf. Letztere wird ausschließlich für die damals erstmals stark wachsende eigene Animationsbranche verwendet. \"Anime\" entstand als eine Verkürzung des japanischen Lehnwortes \"animēshon\" (, von ). Während \"animēshon\" eher als Bezeichnung für höher geachtete Animationskunst und Kinofilme verwendet wurde, wurde dessen Kurzform vor allem für die ab den 1960er entstehenden, günstiger produzierten Fernsehserien verwendet. Letztlich setzte sich \"Anime\" als Bezeichnung aller Animationsproduktionen durch. Die anderen Begriffe blieben nur in Nischen erhalten. Als Bezeichnung nur für japanische Animationsfilme und -serien hat sich der Begriff seit den 1980er Jahren in den USA und Europa durchgesetzt. Im englischen Sprachraum wurde seit Ende der 1970er Jahre in der kleinen Fanszene und von ersten kommerziellen Vertrieben zunächst häufiger \"Japanimation\" verwendet, was jedoch zu Verballhornungen und Missverständnissen führte. So wurde dieses Wort um 1990 von dem aus Japan übernommenen, kürzeren \"Anime\" abgelöst.\n\nIm wissenschaftlichen oder journalistischen Diskurs ist die genaue Abgrenzung zwischen Anime und Animation allgemein jedoch umstritten, auch wenn Anime und japanischer Animationsfilm meist synonym verwendet werden. So nennt Thomas Lamarre eine Abgrenzung von Full Animation und Limited Animation im japanischen Diskurs. Anime, besonders Fernseh-Anime in der einfachen, kostengünstig produzierten Form der 1960er Jahre wird dann als eine Form der Limited Animation aufgefasst. Produzenten von Full Animation wollen sich davon abgrenzen. Beispielsweise lehnt das Studio Ghibli den Begriff \"Anime\" für ihre Filme ab und verwendet stattdessen \"manga eiga\". Aufwändige Filme und Fernsehproduktionen haben jedoch gemeinsam, dass die Ästhetik traditioneller Animationstechniken fast immer erhalten wird, auch wenn vielfach Computeranimationen zum Einsatz kommen. Reine Computeranimationsfilme, denen diese Technik auch anzusehen ist und die nach Realismus streben, kommen zwar vor, sind aber eine Ausnahme. Stevie Suan und José Andrés Santiago Iglesias nennen eine Reihe von Stil-Merkmalen, die japanischen Fernsehproduktionen und solchen mit japanischem Einfluss zu eigen sind: Eine durchgehende Erzählung über alle Folgen, der Gebrauch von Cliffhangern und deren Auflösung durch Eukatastrophen, reduziertes und ikonisches Charakterdesign, größere Vielfalt in der Farbpalette, Gebrauch von Limited Animation und die Erzeugung eines Raumeindrucks durch Bewegung sowie der Einsatz vieler Schnitte, Kameraeinstellungen und Montagen zur Darstellung vieler Perspektiven und Details einer Szene. Die Gesamtheit dieser Merkmale fassen Suan und Iglesias unter dem Begriff \"animesque\" zusammen. Ein \"animesque\"s Werk wird von Fans als Anime erkannt, da es die erzählerischen und formalen Erwartungen erfüllt, die mit dem japanischen Animationsfilm verbunden werden. Brian Ruh verweist neben den von Iglesias genannten Stilmerkmalen auf die Transnationalität des Mediums Anime, die sich in dessen Geschichte als auch in international verständlichen Designs äußert. Ähnlich beschreiben auch andere Autoren ein Verständnis von Anime als in einen Medienmix eingebundene (Fernseh-)Serienproduktionen mit auch in ihren Filmadaptionen wiedererkennbaren, im Fernsehen entstandenen Stil- und Erzählmerkmalen. Daneben gibt es Produkte des japanischen Animationsfilms, die von einigen Rezipienten nicht als Anime angesehen werden, sondern einem unbestimmten Alternativen japanischen Animationsfilm oder nur allgemein Animationsfilm zugerechnet werden.\n\nJonathan Clements weist ebenfalls auf mehrere Abgrenzungsunschärfen des Begriffs hin: Neben der technischen zwischen Full und Limited Animation gibt es Autoren, die Anime historisch abgrenzen als japanischen Animationsfilm ab den Fernsehserien der 1960er Jahre. Japanischer Animationsfilm aus der Zeit davor wird dann nicht als Anime bezeichnet, obwohl die frühen Filme von großer Bedeutung für die späteren Produktionen waren. Weitere Unschärfen ergeben sich aus dem umfangreichen Prozess von Produktion, Vertrieb und Konsum. So sind einige ausländische – insbesondere US-amerikanische – Trickfilmproduktionen unter japanischer Mitwirkung oder sogar großteils in Japan entstanden, andererseits waren japanische Unternehmen Auftraggeber für Animationsarbeit in anderen Staaten, und schließlich sind bei der Lokalisierung, Vertrieb und Vermarktung ausländischer Produktionen japanische Firmen beteiligt. All die damit in Verbindung stehenden Werke können daher auch als Teil der japanischen Animationsfilmindustrie beziehungsweise von dessen Geschichte begriffen werden. Und schließlich gehören zu Anime nicht nur die Objekte, die hergestellten und verkauften Werke, sondern auch die Ereignisse der Vorführung und des Konsums. Steven T. Brown geht so weit, die Bedeutung des Begriffs Anime in der Präsentation und Anordnung der Informationen bei der Vorführung und in der Interaktion des Rezipienten mit dem Werk zu suchen.\n\nIn der Beschreibung von Anime als Medium wird auch oft darauf hingewiesen, wie stark sich der japanische Animationsfilm von dem unterscheidet, was westliche Zuschauer von Animationsfilmen – insbesondere amerikanischen Cartoons oder Filmen von Walt Disney – erwarten. Viele Animes sind nicht für Kinder gemacht, manche sogar pornografischer Natur, erzählen dramatische oder actionreiche Geschichten und bedienen eine große Vielfalt an Genres. Im Vergleich zu vielen im Westen erfolgreichen Trickfilmen kommen Musicaleinlagen, Tierfiguren und Slapstick-Humor deutlich seltener vor. Trotz dieser häufig genannten Merkmale beziehungsweise auffälligen Unterschiede werden diese nicht zur Definition herangezogen. Als Anime um 2000 herum in den USA immer größeren Zuspruch fanden, zugleich aber Vorurteile gegenüber einem als für Kinder gefährlichen Kulturimports herrschten, wurde \"Anime\" auch pejorativ verwendet – vor allem von solchen Vertrieben, die selbst japanische Animationsfilme importierten und ihre Produkte vor den Vorurteilen schützen wollten.\n\n\nHistorische Entwicklung.\nDie Einteilung sowie überhaupt der Beginn der Geschichte des Animes ist – wie auch beim Manga – in Forschung und Fachjournalismus umstritten. Je nach Definition von Anime als Medium oder nur eine bestimmte Erscheinungsform von japanischem Animationsfilm oder als Genre, wird der Beginn von dessen Geschichte am Anfang des 20. Jahrhunderts, erst in dessen Mitte oder erst in den 1970er Jahren gesehen. So ist die genaue Rolle und Wirkung Osamu Tezukas als Pionier des Fernsehanimes strittig, da einige Autoren viele wichtigen Entwicklungen bereits zuvor bei den Kinofilmen Tōei Animation angelegt sehen.\n\nAnimationstechniken und optische Spielzeuge waren in Japan bereits lange vor 1900 bekannt und wurde wie in westlichen Ländern vor allem von Schaustellern vorgeführt oder waren für wohlhabende Bürger als Spielzeug zu kaufen. Der manchmal als erster Anime bezeichnete Filmstreifen \"Katsudō Shashin\", der auf 1907 bis 1912 datiert wird, gehörte zu einem solchen Spielzeug und wurde nie öffentlich vorgeführt. Ab etwa 1910 kamen westliche Trickfilme nach Japan, wurden zunächst bei Bühnenshows und dann zunehmend in Kinos gezeigt. Erste Japanische Nachahmer sind aus dem Jahr 1917 bekannt, in dem die Pioniere Ōten Shimokawa, Jun'ichi Kōuchi und Seitarō Kitayama ihre ersten Filme aufführten. Kitayama gründete 1921 mit Kitayama Eiga Seisaku-sho das erste nur dem Animationsfilm gewidmete Studio. Die verwendeten, noch experimentellen Animationstechniken umfassten unter anderem Kreidezeichnungen, Tuschezeichnungen und Scherenschnitt. Die Aufführung von Filmen wurde damals noch begleitet von einem \"Benshi\", der die Kurzfilme erzählerisch verband und erläuterte. Der Beruf kam aus der Tradition der Schausteller und lebte beim Straßen-Papiertheater Kamishibai noch bis in die 1950er Jahre weiter. Wie auch der Manga wurde der japanische Animationsfilm durch die Benshi und das Papiertheater erzählerisch und ästhetisch beeinflusst. Die in den 1910er Jahren aufgekommene Filmzensur traf auch den Animationsfilm. In Folge dessen waren die entstandenen Kurzfilme meist Komödien oder zeigten japanische und chinesische Mythen, Märchen und Fabeln, die von der Zensur weniger scharf beurteilt wurden. Außerdem entstanden Lehrfilme, die durch ihre Entstehung im öffentlichen Auftrag vor Zensur sicher waren.\n\nNachdem zu Beginn die Produktion an Trickfilmen zunahmen, wurden beim Großen Kantō-Erdbeben 1923 die Studios und die meisten bis dahin produzierten Filme zerstört. Die Branche verlagerte sich ind ie Kansai-Region, wo ihr Schwerpunkt bis nach dem Zweiten Weltkrieg blieb. Sie konnte sich dort von den Einflüssen der traditionellen Theater- und Schaustellerszene in Tokio lösen. Dazu kam der Einfluss amerikanischer Trickfilme, die nun zunehmen in Japan aufgeführt wurden. Die Produktion wurde rationalisiert und entfernte sich von der noch kunsthandwerklichen Herstellung der ersten Jahre. Dabei wurden einfache Kamera- und Bild-Gestelle für die Belichtung genutzt. Das verwendete Material blieb zunächst meist Papier mit Schwarz-Weiß-Zeichnungen, die Inszenierung einfach gehalten und die Studios hatten nicht mehr als eine Handvoll Mitarbeiter. Von diesen blieb Kitayamas das über lange Zeit erfolgreichste, wobei mehrere Mitarbeiter sich abwandten und eigene Studios gründeten. Der Großteil der Produktion waren nun Lehr- und Werbefilme und fiktionale Inhalte wurden die Ausnahme. Um 1930 kam der Tonfilm auch nach Japan und bereitete damit dem Beruf der Benshi ein Ende. Einige von ihnen wurden die ersten Synchronsprecher. Der aufwändige Tonfilm erhöhte auch wieder den Bedarf nach kurzen Filmen und damit nach Trickfilmen. Der von diesen wohl am häufigsten gesehene war das 1931 entstandene \"Kokka Kimiyao\" von Ōfuji Noburō – die japanische Nationalhymne zum Mitsingen, die zu Beginn der meisten Filmvorführungen gezeigt wurde. Während ab 1934 mit der Einrichtung einer Celluloid-Produktion von Fujifilm die Produktion von Papier auf modernere Cels umgestellt werden konnte, blieben Farbfilme bis nach dem Krieg die Ausnahme. In den 1930er Jahren wurden zunehmend Propagandafilme produziert, die vor amerikanischem Einfluss warnten und Japans Kriege in China und die Expansion im Pazifik vorbereiteten und begleiteten. Die Mehrheit der produzierten Filme waren weiterhin keine Unterhaltungsfilme, wobei der Schwerpunkt sich zu animierten Anleitungen für militärisches Personal sowie (Propaganda-)Nachrichtenfilmen verlagerte. Das Verbot ausländischer Filme, die Regierungsaufträge und der staatliche Druck insbesondere auf Schulkinder, die Filme anzuschauen, brachten den Animatoren bis dahin ungekannte Nachfrage und Ressourcen und Studios konnten über die Zahl weniger Mitarbeiter hinaus wachsen. Die japanische Regierung war außerdem gewillt, ihre Überlegenheit auch in der Filmproduktion unter Beweis zu stellen und mit den Filmen Disneys und dem chinesischen Film \"Tiě shàn gōngzhǔ\" von 1941 gleichzuziehen. So konnte auch der erste abendfüllende Animefilm entstehen: \"\". Die in schwarz-weiß erzählte Geschichte von Tieren, die Pazifikinseln von der britischen Kolonialmacht befreien und ihnen die japanische Kultur bringen, kam 1945 nicht lange vor der Kapitulation in die Kinos.\n\nNach Ende des Krieges scheiterten Versuche, die japanischen Animatoren bei wenigen Studios zu bündeln an der schlechten Wirtschaftslage, Arbeitskämpfen und politischer Konflikte um Mitwirkung an Kriegspropaganda. Dazu kam die Konkurrenz durch die jetzt ins Land kommenden ausländischen Filme. So kamen nur noch selten japanische Animationsfilme heraus. Die nun vor allem kleinen Studios der Branche verlegten sich großteils auf Werbefilme, denen im Laufe der 1950er Jahre mit dem Privatfernsehen ein wachsender Markt entstand. Künstlerische Qualität und die Produktionsprozesse blieben hinter den Entwicklungen der vorherigen beiden Jahrzehnte zurück. Von den Studiogründungen nach Kriegsende blieb nur Nichidō, das schließlich von Tōei aufgekauft wurde. Aus diesem Zusammengehen unter dem Namen Toei Animation ging 1958 mit \"Hakujaden\" wieder ein abendfüllender, nun farbiger Anime-Kinofilm hervor. Er war der Beginn einer Reihe von Filmen des Studios, die als Klassiker vor der Zeit des Fernseh-Animes gelten und erheblichen Einfluss auf die späteren Produktionen hatten. Ihre Ästhetik war bereits vom zeitgenössischen Manga beeinflusst und an den Produktionen waren viele beteiligt, die später eigene Studios gründeten und dabei ihre ersten Erfahrungen von Toei mitnahmen. Eine wichtige Figur der Anime-Geschichte ist, ähnlich wie beim Manga, Osamu Tezuka. Nachdem er an einem Spielfilm mit Toei Animation gewirkt hatte, wollte der bereits erfolgreiche Mangaka die eigenen Serien verfilmen und schuf schließlich 1963 mit \"Astro Boy\" die erste Anime-Fernsehserie mit halbstündigen Folgen. Sein Studio produzierte weitere Serien, die auf Tezukas Geschichten basierten, darunter den mit amerikanischer Finanzierung entstandenen, ersten farbigen Fernsehanime \"Kimba, der weiße Löwe\". Ab Ende der 1960er Jahre schuf Tezuka dann anspruchsvollere und experimentellere Filme sowie einige der ersten erotischen Animefilme. Tezuka war – wie auch bei seinen Mangas – noch stark von den Filmen Walt Disneys und deren Ästhetik beeinflusst. Seine eigenen Werke hatten wiederum großen Einfluss auf die ihm nachfolgenden Filmschaffenden, entweder in Anlehnung oder Abgrenzung seiner Stile und Arbeitsweisen. Mit seinem und den in Folge entstandenen Studios expandierte die Branche von nach Schätzungen etwa 500 Mitarbeitern um 1960 auf die doppelte Zahl 1967 und schließlich etwa 2.000 um 1970. Neben und durch Tezuka hatten vor allem die Hollywood-Filme, die nach dem Krieg in großer Zahl in Japan in die Kinos kamen, starken Einfluss auf die aufwachsende Generation späterer Animatoren und Regisseure. In ihren eigenen Werken orientierten sie ihre Inszenierungen an den westlichen Vorbildern und beherrschten und verwendeten cineastische Techniken bald umfangreicher als ihre Kollegen in amerikanischen Fernsehproduktionen. In effizienter Arbeits- und Produktionsweise für Fernsehserien wiederum waren amerikanische Studios wie Hanna-Barbera Vorbilder.\n\nIn den späten 1960er und den 1970er Jahren entstanden vor allem Science-Fiction-Serien und mit ihnen wuchs die erste Generation an Fans auf, die ab den 1980er Jahren selbst als Produzenten in der Anime-Industrie aktiv wurde. Mit dieser Generation nahm auch die Quervermarktung von Spielzeugen zusammen mit Animeserien zu. Neben den von Mecha dominierten Science Fiction entstanden weiterhin Serien für das Kinderprogramm, vor allem Märchenadaptionen bei großen Studios wie Toei Animation und Nippon Animation, die auch international ausgestrahlt wurden. Seit Ende der 1960er wurden Comedy- und Drama wichtige Genres, dabei vor allem Sportdramen. Dabei entstanden im bisher männlich dominierten Medium die ersten Serien, die sich an Mädchen richteten. \"Ribon no Kishi\" (1967) von Tezuka und \"Mila Superstar\" (1969) zählten zu den ersten, mit denen der Entstehung des Shōjo-Mangas auch dessen Adaptionen als Anime folgten. Diese Serien brachten neuen Themen in das Medium, insbesondere Emanzipation, Selbstfindung und Liebesgeschichten. In diesem Umfeld entstanden neue Genre: In den 1980ern die von sich magisch verwandelnden Mädchen erzählenden Magical-Girl-Serien, in den 1990ern kamen Geschichten über homoerotische Beziehungen zunehmend aus dem Manga auch in den Anime und generell fanden ästhetische Prinzipien aus Serien für Mädchen stärkere Verbreitung, darunter die Darstellung schöner Jungen als Bishōnen. Auch das Aufkommen von Videokassetten und Videotheken in den 1980er Jahren veränderte das Medium: Studios konnten nun direkt über Kaufmedien veröffentlichen und mussten nicht mehr an Fernsehsender verkaufen. Die erste Original Video Animation wr 1983 \"Dallos\", weitere vor allem aus dem Bereich Science-Fiction folgten. Dieser neue Vertriebsweg öffnete den Markt für kleine Studios, die sich direkt über kleine Aufträge von Videovertrieben finanzieren konnten. Zur gleichen Zeit wurden neue Studios mit realistischen und fantastischen Stoffen auch im Kino erfolgreich, insbesondere Studio Ghibli. In den 1990er Jahren wurden dann im Fernsehanime realistischere Stoffe, vor allem Komödien und Dramen an Oberschulen, beziehungsweise Coming of Age-Geschichten beliebter, teilweise auch in Verbindung mit Magical Girls und Fantasy.\n\nSeit den 1970er Jahren kamen Animes ins europäische und amerikanische Kino und Fernsehen, zunächst vor allem Kinderserien, die auch in Koproduktion mit westlichen Sendern und Studios entstanden. Gegenüber erwachseneren, actionhaltigen Stoffen bestanden große Vorbehalte. In den Videotheken wurden zunächst vor allem pornografische und erotische Titel vertrieben, was Anime generell einen entsprechenden Ruf gab. Vereinzelt kamen Science-Fiction-Serien in die Kabelprogramme in den USA und Süd- und Westeuropa. Das änderte sich ab Ende der 1980er mit dem Erfolg des Science-Fiction-Films \"Akira\" im Kino, in den 1990ern dann mit den Filmen des Studio Ghibli und mit international erfolgreichen Fernsehserien wie \"Sailor Moon\", \"Pokémon\" und \"Dragon Ball\", sodass das Medium um das Jahr 2000 herum international seinen Durchbruch erlebte und eine große Fangemeinde gewinnen konnte.\n\n\nInhalte, Genres und Zielgruppen.\nAnimes decken ein breitgefächertes Themenspektrum für alle Altersstufen ab. Geschichten sind oft komplexer und vielschichtiger als bei vielen westlichen Trickfilmproduktionen üblich. Es findet mehr Charakterentwicklung statt und auch der Tod wichtiger, vom Zuschauer lieb gewonnener Charaktere kann vorkommen. Bei Heldenfiguren ist ihre Motivation, Loyalität und Durchsetzungswille wichtiger als dass sie gewinnen und Antihelden treten häufiger auf. Auch historische Heldenideale wie der Ninja oder Samurai (Bushidō) spiegeln sich in vielen Animes wieder. Ein klassischer Konflikt aus dieser Tradition ist \"giri-ninjō\", der Konflikt zwischen der gesellschaftlichen Verpflichtung und den eigenen Bedürfnissen. Antagonisten sind häufig differenziert und verfügen über eine Hintergrundgeschichte, die ihre Handlungen erklärt. Auch der häufiger vorkommende Seitenwechsel einer Figur trägt dazu bei. Klischees über typische Bösewichte werden gebrochen und beispielsweise gerade die Gegenspieler als besonders schön dargestellt. Frauen haben öfter wichtigere Rollen, sind starke Heldinnen gleichberechtigt mit Männern und werden dennoch meist attraktiv dargestellt. Obwohl oder gerade weil dies oft noch immer nicht der Stellung von Frauen in der japanischen Gesellschaft entspricht, trägt es zu einer breiteren Zuschauerschaft bei. Dennoch lassen sich oft klassische Frauenrollen feststellen. Dazu gehört die Rolle der Mutter und (im Hintergrund) sorgenden Hausfrau oder der ruhigen, niedlichen, empathischen Schülerin. Diese Rollen lassen sich mit dem japanischen Begriff „yasashii“ (rein, warm, empathisch) beschreiben. Dazu gehören auch Kriegerinnen, die \"yasashii\" mit den Idealen des Bushidō verbinden, und Mädchen und Frauen mit magischer Begabung. Fragen von Sexualität und Geschlecht werden in Animes oft ungezwungen und mit für westliche Zuschauer ungewöhnlichen Perspektiven behandelt, sind Gegenstand von Comedy, Klischees und erotischen Geschichten. Homosexualität kommt explizit und noch häufiger in Form von Andeutungen vor, ebenso unscharfe Geschlechterrollen und androgyne Figuren, die sich auch in Idealbildern wie Männern als Bishōnen oder immer wieder vorkommenden Frauen in Männerrollen zeigen. Dabei ist homosexuelles Verhalten oft nicht Identifikationsmerkmal der Charaktere, diese begreifen sich gar nicht als homosexuell oder werden durch viele weitere Merkmale charakterisiert und können in den unterschiedlichsten Rollen auftreten. Geschichten über gleichgeschlechtliche Liebe sind ganze Genres gewidmet. Nichtsdestotrotz werden auch in Geschichten über gleichgeschlechtliche Liebe bisweilen konservative Werte vermittelt, insbesondere in älteren Werken finden die Beziehungen ein tragisches Ende und die Protagonisten werden für Verstöße gegen die gesellschaftlichen Normen bestraft.\n\nDa das Medium lange Zeit nur in Japan konsumiert wurde, sind die meisten Produktionen nur mit Blick auf den heimischen Markt entstanden, haben starke, von Außen schwer verständliche Bezüge zur japanischen Kultur und berücksichtigen Gepflogenheiten oder Tabus anderer Kulturen nicht, wie es bei auslandsorientierten Produktionen wie aus den USA üblich ist.\n\nWährend sich Fernsehproduktionen häufiger an Kinder richten, ist die Zielgruppe des Videomarktes ältere Jugendliche und Erwachsene. In seiner Gesamtheit kann Anime alle Altersgruppen und Gesellschaftsschichten ansprechen. In Anlehnung an die übliche Einteilung von Manga in Zielgruppen nach Alter und Geschlecht, erfolgt dies oft auch bei Anime. Bei vielen Animes, vor allem solchen ohne Manga-Vorlage, ist die Einteilung in diese Gattungen jedoch schwer oder gar nicht möglich:\n\n\nAuch wenn grundsätzlich alle möglichen Themen auftreten können, so sind doch besonders solche mit Bezug zum japanischen Alltag oder der japanischen Kultur verbreitet. Die kann zum einen Sport und Kunst, Probleme des Alltags und dessen Regeln oder des Lebens im modernen, technisierten und in den Metropolen dicht besiedelten Landes sein, zum anderen traditionelle Künste, Themen des Buddhismus und Shinto und der japanischen Geschichte und Mythologie. Traditionelle Geschichten, wozu neben Mythologie auch japanische Klassiker wie \"Hakkenden\" und \"Genji Monogatari\" zählen, werden dabei oft aufgegriffen, modernisiert und in einer zeitgenössischen Deutung oder Moral präsentiert. Auch nicht-japanische, insbesondere jüdisch-christliche Symbolik und chinesische Mythologie wird immer wieder eingewoben. Dazu kommen typisch japanische Sujets wie \"mono no aware\", die sich nicht nur in Ästhetik, sondern auch in den Geschichten selbst ausdrücken können. Diese Themen werden nicht selten gemischt mit Science-Fiction- und Fantasy-Elementen und die meisten Werke lassen sich nicht klar einem einzigen Genre zuordnen. Daneben gibt es typisch japanische und in Animes weit verbreitete Themen wie Vorteile und Konflikte in einer dicht gewobenen Gesellschaft sowie als Gegenstück dazu das verlockende, freie aber auch gefährliche Leben von Außenseitern und Einzelgängern. Dazu kommen politische Themen wie Wertschätzung der Natur, Umweltschutz, Krieg und Frieden. Neben den international bekannteren, pazifistischen Werken in fantastischen, realistischen oder dystopischen Setting gibt es auch Geschichten, die an Kriegspropaganda des 2. Weltkriegs anknüpfen oder den japanischen Imperialismus herunterspielen und eine japanische Opferrolle im Krieg betonen. Aus anderen Bereichen der japanischen Populärkultur finden sich Japanische Idols, berühmte Persönlichkeiten, die singen, schauspielern und werben, auch in Animes immer wieder: als zentrale Figur einer Geschichte über das Showgeschäft oder in abgewandelter Form in einer fantastischen Erzählung. Auch gibt es seit den 1990er Jahren immer wieder Werke, die die Fanszene selbst in den Fokus nehmen und sich wiederum vor allem an diese als Zielgruppe wenden.\n\nVon Literaturverfilmungen (z.&nbsp;B. \"Das Tagebuch der Anne Frank\" oder \"Heidi\") über Horror bis zu Comedy und Romanzen werden nahezu alle Bereiche und Altersklassen abgedeckt. Die Werke können Wissen über Geschichte, einen Sport oder einen anderen thematischen Fokus eines Animes vermitteln, sowie moralische Werte lehren wie Teamwork oder Respekt. Auch gibt es Genres, die ausschließlich in Anime und Mangas vorkommen oder in diesen Medien entstanden sind:\n\n\nPornographische Animes, sogenannte Hentai, machen nur einen kleinen Teil des japanischen Kaufvideo-Marktes aus; im Fernsehen und im Kino werden diese in Japan überhaupt nicht gezeigt. Viele Animes beinhalten jedoch erotische Elemente, ohne dem Hentai-Genre zugeordnet werden zu können, insbesondere die des Genres Etchi. Solche sowie Serien mit hohem Anteil von Gewalt oder anspruchsvollen Inhalten laufen im japanischen Fernsehen im Nachtprogramm und finanzieren sich in der Regel nicht durch die Ausstrahlung, sondern durch die mit Fernsehausstrahlung beworbenen DVD-Verkäufe. Erotische Geschichten und der relativ freizügige Umgang mit Sexualität in der Populärkultur haben in Japan eine lange Tradition, so gab es in der Edo-Zeit viele solche Ukiyo-e, Shunga genannt. In Hentai als auch in Etchi-Manga sind, wie in der japanischen Erotik allgemein üblich, Sexszenen oft in eine humoristische oder parodistische Erzählung eingebettet. Sexuelle Gewalt und Fetische werden vergleichsweise häufig thematisiert. Neben Komödien wird entsprechend auch Horror oft für Erotik und Pornografie verwendet. Kritiker stellen dazu fest, dass solche Geschichten zwar eine Vielfalt sexueller Praktiken zeigen, aber zugleich konservative Gesellschaftsbilder vermitteln, indem zu überschwängliches oder den Normen widersprechendes Verhalten bestraft wird. Neben Humor und Horror gibt es außerdem eine Reihe romantischer, sentimentaler Pornografie, die frei von Gewalt und Übertreibung über Sex als Ausdruck menschlicher Gefühle und Charakterentwicklung erzählt. Benannt wird dieses Untergenre oft nach \"Cream Lemon\", einem der ersten Vertreter.\n\nErotik ist in Japan stark geprägt von der unter der amerikanischen Besatzung entstandenen Gesetzgebung, die die Darstellung des erwachsenen Genitalbereichs und andere „anstößige“ Inhalte unter Strafe stellte (§ 175 des jap. Strafgesetzbuchs). Dies wurde von vielen Künstlern umgangen, indem die Figuren und ihre Genitalien kindlich gezeigt wurden. Zusammen mit der Kawaii-Ästhetik beförderte das die Entstehung vieler erotischer und pornografischer Geschichten mit kindlichen Figuren und die Etablierung der Genres Lolicon und Shotacon. Auch wenn die Auslegung der Gesetze gelockert wurde, blieb diese Strömung erhalten. Andere Wege, die Zensurgesetzgebung zu umgehen, sind die Verwendung von Balken oder Verpixelung, Auslassungen oder stark reduzierte, symbolhafte Darstellung von Geschlechtsorganen. International waren erotische Anime zeitweise kommerziell deutlich erfolgreicher und verbreiteter als andere Genres, was zur Legende führte, alle Anime seien pornografisch. Dieser Eindruck ist jedoch auch Ergebnis der Stereotype des westlichen Publikums und wirkte vermutlich auch auf die japanischen Produktionen zurück, die wiederum an amerikanischen Filmen orientierend den Frauen größere Brüste und den Männern mehr Muskeln gaben. Nacktheit kann darüber hinaus auch jenseits sexueller Szenen vorkommen, in Alltags- oder Kinderserien, da es in solchen Situationen in Japan nicht als anstößig gilt. Dazu zählen Baden, auch öffentlich und gemeinsam im Badehaus, und Stillen. Außerdem werden Nacktheit, sexuelle Anspielungen wie auch Fäkalhumor in einigen Comedy-Animes gern eingesetzt.\n\n\nProduktion und Arbeitsbedingungen.\nAm Entstehungprozess von Anime sind neben den Animatoren, denen die eigentliche Arbeit des Erstellens der Animation zufällt, viele weitere Berufsgruppen beteiligt. Diese sind die in der Filmbranche üblichen Beteiligten wie Regisseur, Drehbuchautoren, Filmproduzenten, Filmeditor und Filmkomponist sowie speziell für den Animationsfilm benötigte wie Synchronsprecher. Darüber sind in der Branche beispielsweise Mitarbeiter für Übersetzung importierter Filme tätig. In die Produktionen oder Vermarktung von Anime und des umgebenden Medienmixes involviert sind auch Verleger, Mangaka, Beschäftigte bei Aufsichtsbehörden, bei Fernsehsendern, Spielzeugherstellern, Videospielherstellern und -verlegern, sowie aller Unternehmen, die sich an der Produktion eines Animes beteiligen beziehungsweise diesen sponsern. Auch einfachen Mitarbeitern in Organisation und Vertrieb können entscheidende Rollen zufallen. So ist in der oft von Termindruck geprägten Branche die Arbeit und Zuverlässigkeit von Boten, die Rohmaterial oder eine fertige Produktion ausliefern, von großer Bedeutung und bisweilen Gegenstand von Auseinandersetzungen und Anekdoten. Neben all diesen offiziellen Beschäftigten kann man, vor allem international, auch Fansubber und illegale Verbreiter dazuzählen, da auch diese einen Einfluss auf das von einem Zuschauer konsumierte Werk haben. Branchenverband ist die Association of Japanese Animations (AJA).\n\nTraditionell entstanden Animes, wie auch Trickfilme aus anderen Ländern, als Cel-Animation. Dabei werden bemalte Folien („Cels“) vor einen ebenfalls gemalten Hintergrund gelegt und abgelichtet. Die Zahl der produzierten Folien und Einzelbilder pro Sekunde hängt dabei von Budget und der beabsichtigten Qualität beziehungsweise Flüssigkeit der Bewegung ab. Tendenziell wurden Kinofilme aufwändiger und höherwertiger, mit mehr Cels für die gleiche Zeit, produziert als Fernsehserien. Auch Produktionen direkt für den Videomarkt haben in der Regel eine höhere Qualität als für das Fernsehen. Die Cel-Animation wird seit den 1990er Jahren zunehmend von der Computeranimationen verdrängt. Bis in die 1930er Jahre wurde noch vorrangig Papier verwendet, da Kunststofffolien kaum zu bekommen waren, ehe Fujifilm 1934 eine eigene Celluloid-Produktion einrichtete. Von da an blieb die materielle Grundlage der Animation in Japan bis in die 1990er Jahre Cels und die damit verbundenen Produktionsverfahren. Seit den Fernsehserien ab Beginn der 1960er Jahre – für manche aus diesem Grund die eigentliche Geburt von Anime – herrscht Limited Animation als Animationsprinzip vor. Bei den Produktionen von Kinofilmen in den Jahren zuvor wurde noch nach dem Vorbild Disneys Full Animation angestrebt, das heißt möglichst realistische Darstellung und bestmögliche Illusion der Bewegung, wobei eine größere Zahl von Bildern pro Sekunde und entsprechender Aufwand eingesetzt wird. 18 unterschiedliche Bilder pro Sekunde sind dafür üblich, 24 in der Spitze. Dieser Ansatz ist unüblich geworden, nur Kinoproduktionen insbesondere von Studio Ghibli verfolgen diesen noch, und wurde weitgehend durch Limited Animation abgelöst. Diese kommt mit durchschnittlich gerade mal acht Bildern pro Sekunde aus. Der Eindruck von Bewegung wird nicht nur durch unterschiedliche Bilder, sondern auch Arrangement der Bilder und Schnitte erzeugt. So kann der Eindruck von Bewegung, anstatt durch verschiedene Bilder, durch das Verschieben von Vorder- und Hintergrund gegeneinander erzeugt werden. In Actionszenen wechseln eher Standbilder in ausdrucksstarken Posen in schnellen Schnitten als dass Bewegungen tatsächlich gezeigt werden. Raumtiefe kann durch das Übereinanderlegen der Ebenen und deren Überschneidung zumindest angedeutet werden. Bildelemente werden häufig wiederverwertet. Zu diesem Zweck verwenden Studios Bibliotheken der einzelnen Figuren in unterschiedlichen Positionen und Bewegungen, die erneut eingesetzt werden können. Die deutliche Kostenersparnis dabei war zunächst einer der wichtigsten Gründe dafür. Osamu Tezuka hat seine ersten Folgen für nur 500.000 Yen jeweils verkauft. Doch auch als ab den 1970er Jahren die Studios langsam mehr Geld zur Verfügung hatten, wurde von diesem Prinzip nicht abgewichen. Stattdessen wurden, wie schon von Beginn an von Tezuka, dessen künstlerische Möglichkeiten erkundet oder zusätzlicher Aufwand in Hintergründe und Designs investiert. Die kostensparenden Methoden der Limited Animation wurden bereits ab den 1920er Jahren eingesetzt, jedoch nicht systematisch.\n\nDie Vertonung einschließlich Aufnahmen der Sprecher finden nach Herstellung der Animation statt, weswegen Lippenbewegungen gerade in günstigeren Produktionen nicht zum Ton passen. Außerdem werden meist nur drei Bilder von Mundstellungen verwendet, an Stelle von acht bei aufwändigen amerikanischen Produktionen. Die umgekehrte Verfahrensweise mit Tonaufnahmen vor Herstellung der Animation war in den 1930er Jahren mit Aufkommen des Tonfilms entwickelt worden und seitdem mehrere Jahrzehnte üblich.\n\nAls in den 1980er Jahren erstmals Computeranimationen eingesetzt wurden, gehörte die japanische Filmwirtschaft zu den ersten Anwendern. Seitdem wird immer wieder mit 3D-Animation experimentiert. Anders als in den USA konnte die Computeranimation aber nicht die traditionelle 2D-Ästhetik ablösen, reine 3D-Animationsfilme bleiben eine Seltenheit. Stattdessen werden 3D-Animationen als Effekte in Szenen klassischer Animation eingesetzt, beispielsweise Lichteffekte und am Computer animierte Bildelemente werden in einer Weise gerendert, die sie wie handgezeichnet erscheinen lässt. Besonders letzteres ist seit Ende der 1990er Jahre durch neue Software einfacher umsetzbar geworden und wurde daher zunehmend in Bildelementen eingesetzt, die mit der Hand gezeichnet zu aufwändig oder nur schwer zufriedenstellend umzusetzen sind. Insbesondere die Charaktere aber bleiben handgezeichnet und die Ästhetik der traditionellen Cel-Animation wird beibehalten; wobei Zeichnungen häufig digitalisiert und anschließend als Computergrafik koloriert und animiert werden. Zugleich bringt der Einsatz von Computern neue Möglichkeiten für die Einbindung von Fotografie und Rotoskopie.\n\nDie Produktionen sind in der Regel auf viele Unternehmen verteilt, wobei eines als Hauptproduzent fungiert und weitere Nachunternehmer sind. Manches Studio hat sich auf Zuarbeiten spezialisiert, einige heute bedeutende Studios wie Madhouse haben lange fast nur Zuarbeiten für andere geliefert, ehe sie in Eigenregie produzierten. Die Herstellung findet nicht nur in Japan statt, sondern aus Kostengründen auch in anderen asiatischen Ländern, Amerika und Europa. Die wichtigsten Nachunternehmer sitzen in Korea, China und Thailand. Ausgelagert werden vor allem die Herstellung von Zwischenphasenbildern und die Koloration. Die Entwicklung von Drehbüchern und Storyboards, Designs und Schlüsselbildern bleibt in der Regel in Japan. Auch gleichberechtigte Koproduktionen gab es in der Geschichte des Mediums immer wieder. So beispielsweise mit europäischen Sendern in den 1970er Jahren. Seit in den 1990er Jahren das Interesse an Anime in westlichen Ländern zugenommen hat, kommen auch solche Koproduktionen wieder häufiger vor, vor allem mit amerikanischen Firmen. Auf der anderen Seite arbeiten japanische Studios, insbesondere solche auf Zuarbeiten spezialisierte, auch Produktionen in anderen Ländern zu, vor allem amerikanischen.\n\nWichtigste Einnahmequelle der Studios waren in den 2000er Jahren an erster Stelle das Merchandising, danach Senderechte und in deutlich geringerem Umfang Kinoeinnahmen und Werbefilme. Hauptabnehmer ist das Fernsehen, gefolgt vom Kino und dem Videomarkt an letzter Stelle. Für eine einzelne Folge werden etwa 5 Millionen Yen, in früheren Zeiten eher 7 Millionen, und anderthalb Monaten Produktionszeit aufgewendet. In den 2000er Jahren wurden jedes Jahr 30 bis 50 Fernsehserien und 10 bis 20 Kinofilme produziert.\n\nAutorschaft und kreativer Einfluss auf das Endprodukt sind bei Animes meist nicht klar zu erkennen und wie bei vielen anderen Film- und Fernsehproduktionen auf eine große Zahl an Beteiligten verteilt. Am deutlichsten erkennbar sind sie noch bei Kinofilmen, insbesondere solchen bekannter Regisseure und Autoren, denen der Großteil der kreativen Entscheidungen und Ideen zugeschrieben werden. Autorenfilme und entsprechend bekannte Autoren sind im Anime selten. Auf der anderen Seite ist die Autorschaft bei lang laufenden Serien, die eine Vorlage adaptieren, besonders unklar. Trotz der Vorlage werden dabei in der Regel Teile der Geschichte weggelassen oder neue ergänzt, insbesondere zusätzliche kurze Geschichten, falls ein „Strecken“ der Serie notwendig ist. Auch kann bei der Adaption eine Anpassung an ein anderes, beispielsweise jüngeres Publikum stattfinden. An diesen Entscheidungen sind eine Reihe von Personen beteiligt: Eine Serienregie entscheidet über Handlungsbögen. Regisseure einzelner Folgen entscheiden über die Nähe der Inszenierung zur Vorlage, über deren konkrete Umsetzung die leitenden Animatoren. Beim Charakterdesign werden die Figuren der Vorlage an die Bedürfnisse der Adaption in Bezug auf technische und cineastische Umsetzung, inhaltliche Veränderungen und Zielgruppe angepasst.\n\nLaut einer im Jahr 2013 durchgeführten Studie arbeiten japanische Anime-Zeichner im Durchschnitt 10 bis 11 Stunden pro Arbeitstag bzw. 263 Stunden pro Monat bzw. 4,6 freie Tage/Monat. Animatoren verdienen pro Jahr durchschnittlich (Mittelwert) 3,3 Millionen Yen (ca. 23.000 €) bzw. am häufigsten (Modalwert) 4,0 Mio. Yen (28.000 €), angefangen bei Einstiegspositionen wie Zwischenzeichnern mit 1,1 Mio. Yen (8000&nbsp;€) über Schlüsselzeichner mit 2,8 Mio. Yen (20.000 €) und Storyboarder/3D-Animatoren mit 3,8 Mio. Yen (26.000 €) bis zu Regisseuren mit 6,5 Mio. Yen (45.000 €). Zeichner werden häufig nach einem Schema bezahlt, bei dem sie zusätzlich zu einem festen Lohn noch nach fertiggestellten Einzelbildern bzw. -szenen bezahlt werden.\n\n\nBildsprache und Erzählmittel.\nDie häufig anzutreffenden Stile, Bildsprache und Symbolik von Animes sind zum einen geprägt durch die lange Zeit übliche Produktion als Cel-Animation, zum anderen durch Einflüsse von Mangas und insbesondere in der frühen Zeit aus dem amerikanischen und französischen Film. Ähnlich wie im Manga sind die handelnden Figuren einfach und stilisiert gehalten, während die Hintergründe detaillierter und realistischer gezeichnet sind. Auf diese Weise wird sowohl die Identifikation mit den Charakteren erleichtert als auch das Eintauchen in die Welt der Geschichte. Der Detailreichtum der Hintergrundbilder kann insbesondere in Kinoproduktionen ein großes Ausmaß erreichen und bildet einen entsprechenden Kontrast zu den Charakteren im Vordergrund. In Aktionszenen dagegen verschwindet der Hintergrund meist ganz, die gezeigte Bewegung tritt gänzlich in den Fokus. Charakteristisch ist, vor allem im Vergleich zu früheren westlichen Produktionen, eine große Vielfalt in der Farbpalette und deren individuell unterschiedlicher Einsatz in jedem Werk. Das Charakterdesign ist stark vom Manga beeinflusst und entspricht beispielsweise oft dem Niedlichkeitskonzept Kawaii. Bei älteren Charakteren oder Werken für erwachseneres Publikum kommt auch das reifere, nicht niedliche Designprinzip „kirei“ vor. Darüber hinaus finden japanische Idealvorstellung von schönen Frauen und Männern, Bishōjo und Bishōnen, Anwendung und in der Charakterentwicklung werden Archetypen, Stereotype und Klischees verwendet, von denen sich manche von denen anderer Kulturkreise unterscheiden. Das oft eher „europäisch“ wirkende Äußere und Frisuren in allen Farben auch bei japanischen Charakteren sind für westliche Zuschauer besonders auffällig und irritierend. Tatsächlich sind die europäisch wirkenden großen Augen und andere Merkmale des Charakterdesigns im Shōjo-Manga entstanden und dienen vor allem der Vermittlung von Emotionen, haben symbolische Bedeutung oder dienen der Unterscheidung von Figuren. Daher haben auch weibliche Figuren generell sowie die Figuren in romantischen Geschichten – vor allem für Frauen – tendenziell größere Augen. Waren die so dargestellten Figuren früher meist tatsächlich europäisch, werden diese Merkmale in Japan heute nicht mehr als Kennzeichen einer bestimmten Herkunft wahrgenommen. Wenn tatsächlich die Herkunft verdeutlicht werden soll, geschieht das durch andere Merkmale, wie für Europäer mit einer markanteren Nase. Bisweilen sind die Darstellungen dann deutlich stereotyp. Eine andere Art häufig anzutreffenden Charakterdesigns ist Chibi oder Super Deformed. Diese in der Regel für Comedyserien oder eingestreute komische Szenen verwendete Variante zeigt Charakteren in extrem verkleinerter, verniedlichter Form.\n\nDaneben ist – im Unterschied zum westlichen Animationsfilm – nicht die realistische Bewegung Ziel der Animation, sondern der Übergang zwischen ausdrucksstarken Posen. Dies lässt sich sowohl auf Kostenzwänge in der Produktion, in der stehende Bilder günstiger sind, als auch auf eine japanische ästhetische Tradition zurückführen, die sich beispielsweise auch im Kabuki-Theater zeigt, in dem Posen ebenfalls eine wichtige Rolle einnehmen. Auch über den Einsatz von Posen hinaus verweisen viele Stilmittel und Symbole im Anime auf ältere japanische Kunstformen wie Drucke, Kamishibai und Theater sowie generell japanische Ästhetik. Details werden nur spärlich, aber sehr gezielt eingesetzt, um den Eindruck größerer Detailfülle zu verleihen als tatsächlich vorhanden. Wichtige Momente können ungezeigt bleiben oder nur angedeutet und der Zuschauer muss die bleibenden Lücken selbst füllen. Auch Erzähl- und Sprechformen des Animes stammen vom Theater und Kamishibai ab. Im Gesamteindruck bleibt dabei oft eine ästhetische Distanz zwischen Werk und Rezipient (vgl. Verfremdungseffekt) und ein größerer Fokus auf Künstlichkeit und Symbolismus, wie er in vielen japanischen Künsten üblich und dem japanischen Publikum vertraut ist. Völliger Realismus und die Illusion einer echten Begebenheit sind im Gegensatz zu manchen westlichen Medien dagegen nicht das Ziel. Trotz dessen werden auch bei Animes emotionale Reaktion und Empathie des Zuschauers ausgelöst und gefordert. Nicht nur die Bildsprache, auch Motive und andere Gestaltungsmittel sind von der japanischen Kultur geprägt. So die Jahreszeit markierende Geräusche wie die Zikaden im Sommer oder die oft ähnliche Inszenierung von Kämpfen. Darüber hinaus gibt es ein weites Feld an Körpersprache und Symbolen, die in Anime eingesetzt werden und außerhalb Japans nicht immer verständlich sind und auch japanische Höflichkeitsformen bilden sich, schwer zu übersetzen, in den Geschichten und deren Inszenierung ab.\n\nDie einfach gehaltene, günstigere Animationstechniken der sogenannten Limited Animation prägten den Anime. Dazu zählt auch die häufiger nicht zum Ton passenden Lippenbewegungen, da nur wenige Mundstellungen verwendet werden und die Sprecher nach der Animation aufnehmen. Erzählerisch können durch die zurückhaltende Animation der Geschichte und den Charakteren größeres Gewicht gegeben werden als Bewegung, Action und visuellem Humor. Limited Animation geht mit einer sich von Full Animation unterscheidenden Ästhetik einher: Neben der Betonung von Posen der Figuren auch eine große Bedeutung von Rhythmus bei Einsatz und Wiederverwendung der Einzelbilder und Schnitten. Durch die Posen fällt dem vor der Animation stattfindende Charakterdesign ein größeres Gewicht zu als bei der auf Bewegung fokussierten Full Animation. Diese Hinwendung zu den beziehungsweise Betonung von einzelnen Charakteren dient zugleich dem multimedialen Einsatz der Figuren, die sich leichter aus ihrer Serie herauslösen und in anderen Kontexten verwenden und vermarkten lassen. Während die Techniken der Limited Animation für manche nicht als richtige Animation gelten, sind sie für andere eine modernere, weiterentwickelte Animation, die sich von der Nachempfindung des Realfilms gelöst hat und in der sich sowohl kommerzielle Massenwerke als auch experimentelle Kunst realisieren und manchmal kaum noch unterscheiden lassen. Thomas Lamarre nennt einige Einsätze dieser Techniken, wie sie beispielsweise bei Gainax geschehen, wegen ihrer besonderen Ästhetik und dem ausgefeilten Einsatz von Schnitt und Rhythmus sogar „Full Limited Animation“. Hiroki Azuma macht die Folienbibliotheken der Studios als eine frühe Form von Datenbanken zu einem wichtigen Element seiner Analyse der Anime-Fankultur und deren Verhältnis zu Technik und Information. Die im Anime übliche, flächenhafte Ästhetik, die durch stilisierte Figuren und die Techniken der Cel-Animation mit ihrer Arbeit in Schichten entsteht, wird mit dem Begriff Superflat in Verbindung mit einem größeren Trend in der japanischen Kunst und Populärkultur gebracht, der von Holzschnitt bis zu Computerspielen und Manga reicht.\n\nÜber die international bekannte Ästhetik von Anime hinaus gibt es auch viele Werke, die diesen Vorstellungen nicht entsprechen. Dazu können schon ältere Anime-Klassiker zählen, die nicht mehr heutigen Stilen ähneln, aber auch Puppentrickfilme von Kihashiro Kawamoto oder Animation mit Silhouetten und Schattenrissen. Besonders im Animationsfilm bis zum Zweiten Weltkrieg sind die Einflüsse des japanischen Theaters und Kunst in der Vorliebe zu flächiger Bildgestaltung und für Choreografie deutlich zu erkennen.\n\n\nAnime in Japan.\nAnime sind ein fester Bestandteil des japanischen Kulturgutes. Zu den erfolgreichsten Kinofilmen in Japan zählen viele Animes, so \"Prinzessin Mononoke\", \"Pokémon: Der Film\" und \"Chihiros Reise ins Zauberland\". Nach einer Umfrage sind die 100 beliebtesten Zeichentrickserien in Japan alle Anime, mit Ausnahme von \"Tom und Jerry\".\n\n\nKinofilme.\nNeben Filmen mit originären Stoffen, für die insbesondere das Studio Ghibli weltweit bekannt ist, kommen viele Animes ins japanische Kinos, die vor allem romantische und dramatische Stoffe aus bekannten Romanen und Mangas umsetzen. Außerdem ist es für mehrere, über lange Zeit erfolgreich laufende Manga- und Animeserien meist für Kinder üblich, jedes Jahr einen Kinofilm zum Franchise in die Kinos zu bringen. Auf diese Weise kann das Publikum die ihm bereits vertrauten Figuren, manchmal in nur wenig abgewandelten oder die Serie zusammenfassenden Geschichten, in höherer Animationsqualität im Kino erleben.\n\n\nDirektvermarktung.\nNeben Fernsehserien und Kinofilmen werden Animes seit den frühen 1980er Jahren als Original Video Animation, kurz \"OVA\", für den Kaufvideo- und DVD-Markt produziert. OVAs oft sehr kurze Serien oder Kurzfilme, mit Geschichten die für eine Fernsehserie nicht ausreichen oder für die kein großes Publikum zu erwarten ist. Die Qualität ist sehr unterschiedlich, bei manchen Produktionen nur sehr gering, aber oft deutlich über Fernsehniveau. Die Zielgruppe sind zum einen Jugendliche und junge Erwachsene, darunter besonders Anime-Fans. Angebote für diese enthalten in der Regel mit viel Fanservice und Action sowie teils pornografische Inhalte, außerdem Zusatzinhalte zu bekannten Serien. Daneben sind die Käufer von Heimvideos zu großem Teil Familien und Kinder. Letztere Zielgruppen machten in den 1990er Jahren noch den überwiegenden Anteil am Direktverkaufsmarkt aus, wobei dieses Angebot auch viele Animationsfilme aus den USA, vor allem von Disney und Warner Brothers enthielt. In Verbindung mit diesem Videomarkt gab es in Japan in der Vergangenheit auch einen umsatzstarken Leihvideomarkt. Beide profitieren davon, dass in Japan bereits seit langem weniger Vorbehalte gegenüber Animationsfilm bestehen und daher Animes aller Genres und für alle Altersgruppen produziert werden, sodass eine breite Käuferschaft besteht. Die Etablierung dieses Vertriebsweges erlaubte es seit den 1980er Jahren mehr und kleineren Studios den Eintritt in den Animemarkt, da OVAs die Gelegenheit kleinerer, direkt finanzierter Aufträge für die Videovertriebe oder sogar direkt für Fans bieten. Im Gegensatz dazu sind für Fernseh- und Kinoproduktionen größere finanzielle Anstrengungen oder entsprechend größere Produktions-Beteiligungen nötig.\n\nSeit 2000 gibt es auch Serien direkt für das Internet, Original Net Animation (ONA) genannt.\n\n\nFernsehen.\nAnime-Fernsehserien haben für gewöhnlich 12–13, 24–26, sowie seltener 52 oder mehr Folgen, so dass bei wöchentlicher Ausstrahlung eine Laufzeit von einem viertel, halben oder ganzen Jahr erreicht wird. Ein solches Vierteljahresintervall wird als \"cours\" (, \"kūru\") bezeichnet. Die \"cours\" sind dabei saisonal, d.&nbsp;h., es gibt Winter-, Frühlings-, Sommer- und Herbst-Cours, die im Januar, April, Juli bzw. Oktober beginnen. Die meisten Anime-Serien sind nicht als Endlosserien ausgelegt, obwohl insbesondere Verfilmungen langer Manga-Serien auf weit mehr als 100 Folgen kommen können.\n\nIm Jahr 1963 wurden sieben Serien gesendet, dies wird generell als der Beginn von Anime-TV-Serien angesehen. 1978 wurde die 50er-Grenze mit 52 Serien gebrochen. 1998 wurde die 100er-Grenze mit 132 Serien erreicht. Mit 233 Serien wurde die 200er-Grenze im Jahr 2004 erreicht. Seitdem hat sich die Anzahl der Serien mehr oder weniger etabliert, jedoch gab es Jahre wie 2006 und 2014, in denen die 300er-Grenze erreicht wurde.\n\nDer Anstieg der Anime-Anzahl in den 90ern ist darauf zurückzuführen, dass seit 1996 die Mitternachtsprogrammplätze für Anime verwendet werden, aber auch darauf, dass durch den großen Erfolg (und die Kontroverse) von \"Neon Genesis Evangelion\" immer mehr Studios, Videounternehmen und Verlage Werke produzieren ließen. Diese schließen sich dann oft mit Merchandising-Partnern zu Produktionskomitees (, \"seisaku iinkai\") zusammen und kaufen einen Mitternachtsprogrammplatz – daher auch als Mitternachtsanimes (, \"shin’ya anime\") bezeichnet – bei mehreren Sendern, üblicherweise für ein bis zwei \"cours\". Der größte Teil dieser Programmierungen geschieht auf Regionalsendern, die keinem der großen Networks angeschlossen sind. Da diese auf UHF-Band ausstrahlen, werden derartige Anime auch UHF-Anime (UHF) genannt. Mitternachtsanimes erreichen durchschnittliche Einschaltquoten von etwa 2 %, während 4 bis 5 % schon außergewöhnlich hoch sind. Einschaltquoten spielen bei Mitternachtsanimes und damit den meisten Animes seit den späten 90ern kaum eine Rolle, sondern die Ausstrahlung dient der Werbung für die DVD- oder Blu-ray-Veröffentlichungen, mit denen und den Merchandise-Artikeln der Gewinn gemacht wird. Abhängig von deren Verkaufszahlen entscheidet sich dann, ob weitere Staffeln produziert werden. Viele der Anime, die ein bestehendes Werk adaptieren, dienen letztendlich aber auch der Bewerbung der Vorlage, so dass für das auftraggebende Produktionsunternehmen auch die Animeverkäufe zweitrangig sein können, sofern die Vorlagenverkäufe anziehen, was sich daran äußert, dass teilweise auch nur wenig erfolgreiche Anime Fortsetzungen bekommen können.\n\nAnders sieht dies bei am Tage ausgestrahlten Animes aus, die meist langläufig sind (über zwei \"cours\") und sich zudem auch entweder an ein junges oder ein Familienpublikum richten. Seit der Einführung der Mitternachtsanime hat sich die Anzahl der Serien mit hohen Einschaltquoten verringert, und auch die Art der Serien im Tagesprogramm hat sich verändert.\n\nAnime mit den höchsten Einschaltquoten:\n\nAnime mit den höchsten Heimvideoverkäufen:\n\nDurch die sich erholende Wirtschaft während der 1990er Jahre, die starke Berichterstattung über die steigende Beliebtheit von Animes im Ausland und den „Moe-Boom“ investierten aber auch branchenfremde Unternehmen wie Finanz- und neue IT-Unternehmen in diesen früheren Nischenmarkt. Der Rückgang seit 2006 wird auf die sinkenden Geburtenraten und die wirtschaftliche Rezession zurückgeführt.\n\nJapanische Fernsehsender gehen aber auch dazu über, den ausländischen Markt direkt zu beliefern. In den USA wird der Rückgang der Marktgröße für Animes von 4,8 Mrd. Dollar im Jahr 2003 auf 2,8 Mrd. Dollar für 2007 hauptsächlich mit der Fansubbing-Szene in Zusammenhang gesetzt, die Serien bereits kurz nach deren Erstausstrahlung im japanischen Fernsehen untertitelt über Filesharing verbreitet. Im Januar 2009 begann TV Tokyo als erster größerer Fernsehsender, seine Animes nur Stunden nach deren Ausstrahlung im japanischen Fernsehen englisch untertitelt auf einer abopflichtigen Website zu veröffentlichen. Heute wird ein großer Teil der Neuerscheinungen gleichzeitig zur japanischen Ausstrahlung (Simulcast) auf Websites mit englischen (Funimation und Crunchyroll), aber auch deutschen Untertiteln gestreamt.\n\n\nZusammenspiel mit anderen Medien.\nDie Produktion und Veröffentlichung von Animes ist oft eng mit anderen Medien verknüpft. In der Vergangenheit basierten fast alles Animes auf erfolgreichen Mangas. Ab den 2000er Jahren hat die Zahl der Adaptionen von Computerspielen und Light Novels deutlich zugenommen. Es wird aber auch umgekehrt nach dem Erfolg eines Animes ein entsprechender Manga gezeichnet. Vergleichsweise selten sind „Anime-Comics“ bei denen der Manga nicht neu gezeichnet, sondern aus Einzelbildern des Animes und eingefügten Sprechblasen zusammengesetzt wird. Zu den diversen üblichen Merchandising-Artikel von Franchises, unter anderem Artbooks, Soundtrack-CDs und Klingeltöne, nehmen Model-Kits und fertige Figuren eine wichtige Rolle ein. Diese werden in Japan in großer Zahl für viele Serien verkauft, sind außerhalb des Landes jedoch nur schwer erhältlich.\n\nDie Anime-Produktionen werden in der Regel von Produktionskomitees geleitet, denen oft Unternehmen unterschiedlicher Branchen angehören, so neben Buch-, Spieleverlagen, Studios, auch Lebensmittelfirmen, die Kapital einbringen und sich die Rechte am Werk aufteilen. Durch diese frühe Verknüpfung erscheinen parallel zum Anime auch Manga, Romane und weitere Artikel. Teilweise werden diese Franchises dann gezielt zur Werbung für ein Produkt oder eine Produktgruppe eingesetzt. Die Zusammenarbeit in Produktionskomitees soll auch gewährleisten, dass inhaltliche und Designelemente über die einzelnen Werke hinaus im Franchise genutzt werden, beispielsweise der Einsatz von Charakteren in einem Videospiel bei der Entwicklung für eine Fernsehserie schon mitgedacht wird. Dabei können jedoch Entwicklung der Geschichte, der Figuren und die Darstellung von Konflikten und menschlichen Beziehungen in den Hintergrund treten. So werden Charaktere für die Verwendung in Computerspielen stark vereinfacht, Spielmechaniken angepasst und dabei unter Umständen austauschbar gemacht und die Handlung und Charakterentwicklung der Vorlage aufgebrochen oder außer Acht gelassen. Solche Charakterzeichnung kann sich auch im Anime zeigen, wird es bei dessen Produktion bereits mitgedacht. Die Adaption anderer Werke bietet außerdem die Möglichkeit, Details von Geschichten oder Nebenerzählungen wegzulassen, da die mit der Vorlage bereits vertrauten Zuschauer in der Lage sind,d die Lücken zu füllen. Enge Zusammenarbeit über Komitees hat ab den 1990er Jahren deutlich zugenommen, in denen auch der Medienmix an Bedeutung gewonnen hat. Praktiziert wurde diese Zusammenarbeit aber bereits seit langem, so war schon \"Astro Boy\" 1963 von Süßwaren- und Spielzeugherstellern mitfinanziert, die passendes Merchandising auf den Markt brachten.\n\nOft ist die Computerspiel-Industrie an der Anime-Produktion beteiligt, die auf Grundlage der Animes Computer- und Konsolenspiele produziert. Ebenfalls gab es Animes, die wie \"Pokémon\" und \"Digimon\" auf erfolgreichen Spielen basieren. Auch eine parallele Entwicklung oder der Beginn der Animeproduktion noch vor Start eines Spiels kommen vor. Dabei ist jedoch oft eines der Werke das Zugpferd, an dessen Popularität sich die Adaption hängt. Die Zweitverwertungen werden dann vor allem von Fans und Sammlern gekauft wird, die weitere Einsätze ihrer Lieblingscharaktere oder -welt erleben wollen. Unabhängig solcher Franchise- und Lizenzprodukte beeinflussten Anime und Computerspiele sich in ihren Darstellungsformen und Erzähltechniken gegenseitig. Insbesondere die Ästhetik von Manga und Anime hat den Stil von japanischen Computerspielen geprägt, vor allem was das Charakterdesign betrifft. Bereits frühe japanische Computerspiele orientierten sich in der Darstellung ihrer Figuren an Manga und Anime. und Techniken wie Cel-Shading, das Computeranimationen den Anschein klassischer Animationen gibt, oder Prinzipien der Limited Animation werden auch in Spielen eingesetzt.\n\nWie in Kinofilmen wird im Anime die Musik als wichtiges künstlerisches Mittel benutzt. Am häufigsten wird die Musik als Thema für einen Charakter genutzt, oder um als Hintergrundmusik die Stimmung einer Szene wiederzugeben. Serien haben ein Vorspannlied als Einleitung und einen mit Musik unterlegten Abspann.Lieder werden häufig von bekannten Musikern oder japanischen Idolen gesungen, aber auch von den Synchronsprechern (Seiyū). Bis in die 1970er Jahre waren nur einfache musikalische Untermalungen und die Veröffentlichung von einzelnen Liedern als Single üblich, mit Kindern als Zielpublikum. 1977 wurde dann nach dem Vorbild des Soundtracks zu \"Star Wars\" zum in Japan ähnlich erfolgreichen \"Uchū Senkan Yamato\" ein Album herausgebracht. Dessen Erfolg führte rasch zu vielen Nachahmern und die Veröffentlichung von Soundtrack-Alben wurde zu einem üblichen Teil der Verwertungskette. Die Musik wurde aufwändiger komponiert und zu den bis dahin nur mit Orchester oder in Form klassischer Lieder aufgenommenen Stücken traten in den 1980er Jahren Pop- und Elektroklänger, etwas später auch Rockmusik. Die Verwertungskette wurde bald noch um Tonträger ergänzt, auf denen die Synchronsprecher Lieder und Texte in ihrer Rolle im Anime aufgenommen haben. Das förderte die Bekanntheit der bis dahin wenig berühmten Sprecher, die seitdem selbst zu Idolen werden können. Eine weitere Variante von Anime-CD-Veröffentlichungen sind \"Drama-CDs\": Hörspiele, in denen die Sprecher eine Geschichte erzählen, die häufig im Anime nicht vorkommt.\n\nDurch die enge Verzahnung verschiedener Medien und der Übertragung von Inhalten von Manga zu Anime und darüber hinaus haben sich seit den 1990er Jahren zunehmend Charaktere von ihren Erzählungen gelöst, wie es Kulturwissenschaftler Gō Itō 2005 erstmals beschrieb. Der von ihm so genannte \"kyara\" (\"Chara\", kurz für engl. \"Character\") ist eine vereinfachte, auf Äußerlichkeiten und wenige prägnante Merkmale reduzierte Form eines Charakters, der als vollwertige literarische Figur nur innerhalb einer Erzählung existiert. Es sind dann nicht die Figuren, sondern oft nur deren Kyara, die in vereinfachten Formen in Adaptionen wie Computerspielen und Musik auftreten, die in Fanwerken verarbeitet werden oder die die Werke eines Franchise verbinden, nicht Geschichten oder detailreiche Charaktere. Kyara sind demnach seit der Jahrtausendwende zunehmend Objekt der Zuneigung der Fans (\"moe\") und zum Angelpunkt von Franchises geworden, während die Bedeutung von und das Interesse an übergreifenden, großen Erzählungen abgenommen habe.\n\n\nGesellschaft und Politik.\nIm Vergleich zum Animationsfilm in anderen Ländern war Anime in Japan bereits vergleichsweise früh gesellschaftlich anerkannter. So wurden einige Animatoren und Regisseure seit den 1980er Jahren prominent und ähnlich geachtet wie ihre Kollegen aus dem Realfilm. Während Anime, genauso wie andere japanische Populärkultur, noch bis in die 1990er Jahre von der Politik wenig beachtet und wenn überhaupt nur als Problem wahrgenommen wurde, änderte sich dies nach 2000. Insbesondere durch Außenminister und später Premierminister Tarō Asō, selbst Fan von Anime und Manga, wurde ab japanische Populärkultur als wichtiger Teil der japanischen Wirtschaft und Kultur begriffen und dessen Export beworben und gefördert. Mehrere Gesetzesvorhaben stärkten den Urheberschutz und Vertreter der Anime-Industrie wurden in politische Beratungsgremien aufgenommen. Anime und Manga sollten als Soft Power beziehungsweise im Kontext der Idee von Cool Japan ein positives Bild Japans in der Welt vermitteln und wichtiger Teil der Exportwirtschaft sein.\n\nDiesen Initiativen entgegen steht, dass Anime wie auch Manga nicht so neu und modern, nicht so stilistisch und inhaltlich einheitlich sind wie die politischen Strategien sie bewerben. Stilmerkmale und Marketingstrategien haben eine weit vor 2000 zurückreichende Geschichte und sind oft im Zusammenspiel mit westlichen Einflüssen entstanden, sind also weniger originär japanisch als politisch suggeriert wird. Manche Animeserien werden dafür krititiert, nationalistische oder revanchistische Ideen zu transportieren. Japanischen Animefans wird ein stärkeres Nationalbewusstsein zugeschrieben als anderen Teilen der Gesellschaft. Zugleich findet im Medium auch Kritik an Politik, Gesellschaft und Nationalismus statt, viele Werke zielen auf einheimisches Publikum und sind international nicht verständlich und die Szene und deren Vertreter versuchen sich politischer Einvernahme zu entziehen. Des Weiteren ist fraglich, ob die auch in Japan recht kleine Branche die von der Politik in sie gesetzten wirtschaftlichen Hoffnungen erfüllen kann.\n\n\nBekannte Anime-Studios.\nEines der international bekanntesten Anime-Studios ist Studio Ghibli, das seit 1985 unter der Leitung von Hayao Miyazaki Filme produziert, z.&nbsp;B. \"Prinzessin Mononoke\" 1997, \"Chihiros Reise ins Zauberland\" 2001 oder \"Das wandelnde Schloss\" 2004. Seinen bisher größten weltweiten Erfolg hatte Studio Ghibli mit \"Chihiros Reise ins Zauberland\". Der Film erhielt neben zahlreichen internationalen Zuschauer- und Kritikerpreisen im Jahr 2002 den Goldenen Bären auf der Berlinale und im Jahr 2003 den Oscar als bester Animationsfilm, was ihn zum meistausgezeichneten Zeichentrickfilm aller Zeiten macht.\n\nBei Animeserien ist Tōei Animation bedeutend, das bei frühen Science-Fiction-Klassikern wie \"Uchū Senkan Yamato\" (auch \"Space Battleship Yamato\") und \"Captain Future\" und später bei \"Sailor Moon\", \"Dragon Ball\" und weiteren Serien beteiligt war, die große internationale Verbreitung gefunden haben. Darüber hinaus produzierte Tōei die bedeutendsten Anime-Filme der 1960er Jahre.\n\nWeitere bekannte Anime-Studios:\n\n\nAnime international.\nAußerhalb Asiens sind hauptsächlich die USA, Frankreich und Italien für die Verbreitung von Anime in Nachbarländern wie Spanien, Portugal, Arabien, Lateinamerika und auch Deutschland verantwortlich. Der Erfolg von Anime in westlichen Ländern öffnete diese Märkte in den 1990er Jahren auch dem verwandten Medium Manga, dem japanischen Comic, der hier zuvor nur wenig Zuspruch finden konnte. In einigen Nachbarländern Japans – Südkorea und Taiwan – war der Manga dagegen schon früh präsent, ehe auch Animes gezeigt wurden. In der Volksrepublik China dagegen erfuhren beide Medien wie außerhalb Japans erst in den 1990er Jahren größere Verbreitung, auch hier gingen Fernsehserien den Comics voraus. Die, gerade in westlichen Ländern unerwartete, Popularität von Anime – und in Verbindung damit auch Manga – in der Jugendkultur wird von Antonia Levi schon Mitte der 1990er Jahre als „Sieg des Multikulturalismus“ bezeichnet. Einer, der zugleich die Comic- und Animationsszene in Amerika und Europa mit neuen Ideen befruchtete.\n\nDie Veröffentlichung findet sowohl synchronisiert als auch untertitelt statt, wobei insbesondere Fans untertitelte Fassungen oft bevorzugen, um nicht originalgetreue Übersetzungen zu meiden. Da viele Animes Bezüge zur japanischen Kultur haben und daher für Zuschauer in anderen Ländern nicht immer leicht zugänglich sind, waren außerhalb Asiens zunächst vor allem neutrale Stoffe oder solche basierend auf europäischen oder internationalen Märchen erfolgreich, ehe auch stärker japanische Serien ein Publikum finden konnten. Diese hatten und haben es teilweise noch immer schwer, ihr Publikum zu finden, da speziell japanische Stilelemente, Bildsprache und Symbole außerhalb Japans für viele nicht verständlich sind. Außerdem wurden bei vielen Lokalisationen die Dialoge und teilweise auch die Handlung angepasst, um sie kindgerechter zu gestalten, da Zeichentrick im Westen ausschließlich als Medium für Kinderunterhaltung verstanden wurde. So wurden sexuelle Anspielungen oder Gewaltdarstellungen durch Dialogänderungen oder Schnitte entfernt oder „entschärft“, bis hin zu sinnentstellenden Änderungen. Auch die Komplexität einiger Animes wurde durch Kürzungen reduziert oder die Musik durch neue ersetzt, um die Werke dem Publikum gefälliger zu machen. Bei anderen Serien, die sich eher an Anime-Fans richten, wurden stattdessen zusätzliche Erläuertungen beigelegt, um die japanischen Eigenheiten zu erklären. Der Erfolg des Mediums seit den 1990er Jahren wird teils mit dessen Andersartigkeit beziehungsweise des „Japanischseins“ erklärt, das gerade für Jugendliche einen besonderen Reiz ausmache. Andere Autoren verweisen darauf, dass Serien mit weniger explizitem Japanbezug wie \"Pokémon\" und \"Sailor Moon\" die erfolgreichsten sind und daher einer im Medium verbreiteten Mischung aus Fantasy und dem Publikum bekannten Alltagsthemen, die zur Identifikation einladen, eine wichtigere Rolle zukommt. Die früh im Westen erfolgreichen Serien wie \"Astro Boy\" ließen keinen japanischen Bezug erkennen, beziehungsweise diesen leicht herausbearbeiten, und wurden daher gern als kulturell neutral wirkende Produkte eingekauft. Durch diese frühen Importe wurde das Publikum bereits an japanische Ästhetik und Erzählmuster gewöhnt.\n\nBesondern in den 1970er und 1980er Jahren, aber auch noch danach, findet ein erheblicher Teil von Animes sein Publikum über illegale Kopien. Vor allem dort, wo noch keine Unternehmen Animes lizenziert hatten oder keine Verkaufsinfrastruktur vorhanden war, wurden das Medium nur durch illegale Verbreitung überhaupt erst bekannt und gewann eine Anhängerschaft. Waren das früher Kopien von Videokassetten, die privat und auf Conventions weitergereicht und von Fans untertiteln wurden, wurden daraus später über Websites zum Herunterladen angebotene Fansubs. Als sich international ein kommerzieller Markt für Animes entwickelte, begannen deren Unternehmen den damit entstehenden Einnahmeeinbußen entgegenzutreten. In den USA taten sich 1995 zusammen, um gegen illegale Kopien und Verleihe der lizenzierten Werke vorzugehen. Dies sollte sich gegen kommerziell orientierte Rechtsverletzungen richten, geriet jedoch schnell in die Kritik der Fanszene, dass es auch gegen Fangruppen vorgehe. Die Arbeit wurde ohne Öffentlichkeitsarbeit fortgesetzt, um keine Reaktionen der Fans mehr zu provozieren. Nichtkommerzielle Kopien werden von den Unternehmen oft toleriert, nicht wenige von ihnen begannen selbst als Fangruppen. Viele Fansubgruppen wiederum stellen die Verbreitung ein, wenn ein Anime in der jeweiligen Sprache lizenziert wurde und offiziell veröffentlicht wird.\n\n\nUSA.\nAnime-Serien sind im Westen erstmals in den Vereinigten Staaten im Fernsehen aufgetaucht. Dort sind in den 1960er Jahren unter anderem \"Astro Boy\", \"Kimba, der weiße Löwe\", \"Gigantor\" und \"Speed Racer\" gelaufen. Letztere wurde als einzige schon als japanisch erkannt und prägte lange das Bild von Anime in den USA. Danach waren Anime-Serien weniger präsent, als es in Europa der Fall war. Die populärsten Serien kamen aus dem Science-Fiction-Bereich, wie \"Star Blazer\", \"Voltron\" und \"Robotech\". Auch gab es Koproduktionen zwischen den USA und Japan, dazu zählen \"Das Letzte Einhorn\" und \"Transformers\". In den 1990ern begannen sich nach ersten erfolgreichen Animes im Kinos auch amerikanische Filmproduzenten für die japanische Filmindustrie zu interessieren und finanzierten unter anderem \"Ghost in the Shell\" mit und eine Vereinbarung zwischen Disney und Tokuma Shoten brachte die Filme des Studio Ghibli nach Nordamerika. Kurz darauf war wie in Deutschland die internationale Vermarktung der Serien \"Sailor Moon\", \"Pokémon\" und \"Dragon Ball Z\" für die Wahrnehmung von Anime im Speziellen verantwortlich gewesen. Der Film \"Pokémon the Movie: Mewtwo Strikes Back\" von 1999 wurde zum erfolgreichsten Anime-Film in den japanischen Kinos. Erfolgreiche Ausstrahlungen von Anime-Serien hatten Einfluss auf die Cartoon-Industrie in den USA selbst. Zunächst betraf das vor allem die Comicszene, die sich in Ästhetik und Themen von Anime und Manga inspirieren ließ. Es folgten Animationsproduktionen, die zunächst noch zurückhaltend auf düsterere Atmosphäre und erwachsenere Themen setzten. Serien wie \"Galaxy Rangers\" in den 1980ern sowie \"Avatar – Der Herr der Elemente\", \"Monsuno\" und \"Teen Titans\" in den 2000ern waren von der Anime-Ästhetik beeinflusst. Seit Ende der 2000er wurden auch mehrfach Manga- beziehungsweise Anime-Titel von amerikanischen Studios als Realfilm adaptiert. Nach dem Boom Anfang der 2000er Jahre gingen zum Ende des Jahrzehnts die Verkäufe von Anime-DVDs zurück, was vor allem auf die Konkurrenz durch legale und illegale Online-Angebote zurückzuführen ist. Eine ähnliche, noch deutlichere Entwicklung nahm der nordamerikanische Manga-Markt.\n\nIm US-Fernsehen werden für Anime, die im Kinderprogramm laufen, die umfangreichsten Bearbeitungsmaßnahmen unternommen. Diese Fassungen werden dann oft international vermarktet. Der amerikanische Jugendschutz ist im Vergleich zum europäischen Standard in Deutschland, Frankreich etc. weitaus strenger. In den USA stehen den Unternehmen umfangreiche Mittel zur Verfügung, um Bilder zu retuschieren, Namen zu ändern, Folgen auszulassen, zusammenzuschneiden und somit die Handlung zu verändern. Auch die Musik wurde teilweise verändert. Freizügige, gewalttätige, religiöse oder japanisch-kulturelle Inhalte und auch Bezüge zu Alkohol, Waffen und Drogen werden entfernt. Ernsthafte Themen wie der Tod werden umschrieben oder ausgelassen. Diese Maßnahmen unterscheiden sich von Serie zu Serie und auch von Firma zu Firma. Die konsequentesten und umfangreichsten Bearbeitungen finden bei 4Kids (\"One Piece\", \"Yu-Gi-Oh\"), Harmony Gold (\"Robotech\"), Saban Brands (\"Digimon\", \"Glitter Force\") und DiC (Sailor Moon) statt.\n\nWeitgehend unbearbeitete Serien haben Popularität durch Videokassetten oder durch Nachtprogramme von Sendern wie Cartoon Network oder SyFy gewonnen. Speziell im Nachtprogrammblock von Cartoon Network sind \"Cowboy Bebop\" und \"Big O\" sehr populär geworden. \"Space Dandy\", \"\" und eine zweite Staffel von \"Big O\" wurde von amerikanischen Geldern mitfinanziert. Netflix plant, mehrere Serien mitzufinanzieren, die dann als Netflix Original beworben werden. Seit den 2000ern sind Anime, zuvor nur wenigen bekannt, stärker Teil des amerikanischen Mainstreams geworden, was auch zu zunehmender gesellschaftlicher und politischer Kritik am Medium führte. Während zunächst vor allem die Fans kritisiert wurden – als seltsam und unreif – geraten mittlerweile auch die Werke selbst in die Kritik, meist weil sie nicht kinderfreundlich genug seien.\n\n\nAnime in Deutschland.\nAls erster Anime in Deutschland wurde ab dem 16. März 1961 der Film \"Der Zauberer und die Banditen\" von Toei Animation aus dem Jahr 1959 in den Kinos gezeigt. Die erste Anime-Serie im deutschen Fernsehen war \"Speed Racer\", von der 1971 aber nur einige Folgen gezeigt wurden ehe sie wegen Protesten abgesetzt wurde. Ähnliches geschah zu Erstausstrahlung von \"Captain Future\" in den 1980er Jahren. Entsprechend wurden in den 1970er und 80er Jahren nur kinderfreundliche Serien, zum Beispiel des \"Masterpiece Theater\" gezeigt, bei denen keine Proteste zu befürchten waren. Einige davon waren deutsche Koproduktionen wie \"Wickie und die starken Männer,\" \"Die Biene Maja\" und \"Nils Holgersson\". Die ersten deutschen Kauf-Animes gab es im Jahr 1975 auf sogenannten TED-Bildplatten. In den 1980er Jahren erschienen zahlreiche Animes auf VHS-Kassetten. Dieser Markt war neben der Zweitverwertung von Kinderserien aus dem Fernsehen geprägt von erotischen Werken, die von Trimax herausgebracht wurden. Diese Importe führten dazu, dass „Anime“ noch bis nach 2000 von vielen eng mit pornografischen oder stark gewalthaltigen Werken verknüpft wurden.\n\nMit Beginn der 1990er Jahre sind im deutschen Kino häufiger Anime-Filme gezeigt worden, darunter \"Akira\" (1991), \"Ghost in the Shell\" (1997) sowie einige Produktionen von Studio Ghibli wie \"Prinzessin Mononoke\" (2001) und \"Chihiros Reise ins Zauberland\" (2003). Mit dem Aufkommen des Privatfernsehens kam auch eine Vielzahl von Anime-Serien ins Fernsehen, zunächst über Einkäufe von europäischen Programmpaketen, in denen neben westlichen Zeichentrickserien auch vereinzelt Anime enthalten waren. Mit der Zeit wurden auch Serien für Jugendliche ins Programm genommen, und im August 1999 erhielten Animes den Programmblock \"Moon Toon Zone\" bei RTL 2. Dieser Block bestand aus \"Sailor Moon, Dragon Ball\" und \"Pokémon\" und wurde mit \"Anime@RTL2\" ab 2001 und \"PokitoTV\" im Jahr 2004 ausgebaut. Durch den Erfolg der RTL-2-Ausstrahlungen begann das bewusste Lizenzieren von Anime durch RTL2 und andere Fernsehsender. K-Toon, MTV, VIVA und VOX sendeten Animes für ein älteres Publikum. Ab 2007 ging dieses Angebot von Animes im Fernsehen wieder deutlich zurück. 2013 wurde das Programm bei RTL II vollständig abgesetzt. Von 2007 bis Juni 2016 gab es mit Animax Deutschland ein eigener Pay-TV-Sender für den deutschsprachigen Raum. Heute senden nur noch ProSieben MAXX (seit 2013) und Nickelodeon regelmäßig Animes. Die Nachbearbeitung von Animes geschah im deutschen Fernsehen und Kino lange Zeit in großem Maße und war oft Gegenstand großer Kritik von Fans. Dabei wurden zahlreiche Schnitte und inhaltliche Änderungen meist mit dem Jugendschutz begründet, da Trickserien als Kinderprogramm gelten und für dieses Publikum eingekauft und gezeigt werden.\n\nDas erste deutsche Anime-Label war OVA Films, gegründet 1995. Um 2000 kamen immer mehr Label auf den Markt, von denen sich jedoch viele nicht halten konnten. Erst ab 2010 traten neue Unternehmen dazu, von denen einige seit 2015 auch regelmäßige Anime-Festivals veranstalten. AV Visionen startete im September 2007 das erste deutsche Anime-Video-on-Demand-Portal \"Anime on Demand\". Mit der Zeit folgten weitere deutsche wie internationale Angebote, die jedoch nicht alle von Dauer waren. So stellte das zu ProSiebenSat.1 Media gehörende MyVideo sein 2011 gestartete Angebot 2016 wieder ein. Seit 2013 bedient das amerikanische Portal Crunchyroll auch den deutschen Markt.\n\nEine Fanszene entwickelte sich ab den 1980er Jahren in kleinerem Maße. Mit zunehmender Verbreitung und Popularität von Animes wie auch Mangas nach der Veröffentlichung von \"Akira\" im Westen und umso mehr nach dem Erfolg von Fernsehserien, darunter \"Sailor Moon\" und \"Dragon Ball\", entwickelte sich eine größere Fangemeinde. Diese stützte sich stark auf kommunikation über Chats und Foren, es entstanden Fanzines und Veranstaltungen der Szene sowie Treffen auf Buchmessen. Darüber hinaus gehört Cosplay, das Verkleiden als Figur aus einem Manga oder Anime, und Fan-Art zu wichtigen Hobbys in der Szene. Außerdem findet nicht selten eine Auseinandersetzung mit japanischer Kultur und Gesellschaft jenseits von Populärkultur statt. Bedeutende Veranstaltungen, auf denen sich Fans vorrangig treffen, sind Anime- und Manga-Conventions sowie der Japantag, die Buchmessen oder Veranstaltungen zum japanischen Film. Das einzige derzeitige professionelle deutschsprachige Anime-Fachmagazin ist die \"AnimaniA\", die seit September 1994 erscheint. Dazu kommen Jugendmagazine mit eigenen Anime-Bereichen, wie \"Mega Hiro\", \"Koneko\" und \"Kids Zone\". Mittlerweile eingestellt sind die vom Verein Anime no Tomodachi herausgegebene \"Funime\" sowie die \"MangasZene\".\n\n\nFrankreich.\nIn Frankreich sind Anime zum ersten Mal mit den Serien \"Kimba, der weiße Löwe\" und \"Choppy und die Prinzessin\" (Erstausstrahlung folgte in Deutschland in 1996) im Jahr 1972 und 1974 aufgetaucht. Ähnlich wie \"bei Wickie und die starken Männer\" und \"Die Biene Maja\" gab es französisch-japanische Koproduktionen (\"Barbapapa\", \"Odysseus 31\" und \"Die geheimnisvollen Städte des Goldes\") und viele Serien des \"World Masterpiece Theater\" wurden gezeigt. Mit der Toei-Produktion \"Grendizer\", in Frankreich \"Goldorak\" genannt, wurde 1978 eine Serie ausgestrahlt, die maßgeblich dafür verantwortlich war, dass im Kinderprogramm vermehrt auf Anime gesetzt wurde. Die Serie erreichte hohe Einschaltquoten, löste aber auch große Anfeindungen und Proteste gegenüber gewalthaltigen japanischen Produktionen aus. TF1 ist der größte Privatsender Frankreichs und setzte im Kinderprogramm stark auf Anime, viele Serien waren verantwortlich für die große Fanszene in Frankreich. Es folgten mehrere Science-Fiction-Serien von Leiji Matsumoto. Das als \"Albator\" gezeigte \"Captain Harlock\" wurde von einem französischen Studio auch als Comic umgesetzt. Für weibliches Publikum wurde 1978 die Serie \"Candy Candy\" aus Japan importiert. Während RTL2 insgesamt etwa 60 Serien zeigte, waren es auf TF1 weit über 100. AB Productions hat die Serien jedoch als billiges Kinderprogramm angesehen und diese Massen an Serien dann so im Schnitt und Dialog zusammengestutzt. 1997 wurde das Programm auf TF1 nach Protesten und einen Konflikt über Anime, der über 15 Jahre anhielt, vollständig abgesetzt. Danach haben sich verschiedene Spartensender gefunden, die ein Animeprogramm sendeten, während im Kinderprogramm der großen Sender ausschließlich auf sehr kindgerechte Anime gesetzt wurde.\n\nSpace Adventure Cobra gilt als der Anime mit dem höchsten Kultstatus in Frankreich, Realverfilmungen und Fortsetzungen als Koproduktion sind geplant. 2004 wurde \"Ghost in the Shell 2: Innocence\" bei den Internationalen Filmfestspielen von Cannes 2004 nominiert. Wie in den USA hatten Anime Einfluss auf die heimische Zeichentrickindustrie in Frankreich. \"Totally Spies!\" und \"Wakfu\" sind ästhetisch an Anime angelehnt.\n\n\nItalien.\nIn Italien war die Resonanz auf Anime durchwegs positiv. Seit \"Goldorak\" wurde beinahe jedes Genre und Format von Japan übernommen. In Italien wurden die meisten Anime außerhalb Japans im Fernsehen und Kino gezeigt. Während in Deutschland in den 70ern und 80ern nur knapp 20 Serien gezeigt wurden, waren es in Italien bereits über 200. Der Grund für diese Massenimporte war, dass Italien bereits 1976 das Fernsehen privatisierte und daraus eine Vielfalt an Sendern hervorging. Auch waren Anime die preiswertesten Zeichentrickproduktionen. Wie auch in Frankreich war \"Goldrake (Grendizer)\" sehr erfolgreich und erhielt mehrere italienische Comic-Adaptionen. Für weibliches Publikum wurde 1978 die Serie \"Candy Candy\" aus Japan importiert. Koproduktionen mit Japan wie \"Calimero\", \"Z wie Zorro\" und \"Die Abenteuer des Sherlock Holmes\" sind entstanden. Eine Vielzahl der Sendungen, die in Kinderprogrammen der großen Sender liefen (Rai und Mediaset), wurden konsequent bearbeitet. So hat man Gewalt und freizügige Szenen geschnitten – aber auch Zensur und Veränderungen im Dialog wurden vorgenommen. Thematiken wie der Tod, sexuelle Anspielungen, japanische kulturelle Inhalte, sowie drastische Bilder und Zustände wurden sehr kindgerecht und abgeflacht aufbereitet. Durch die Thematik der Serie \"Detektiv Conan\" haben sich aber solche Dialogumschreibungen wieder gelegt, und diese werden inzwischen auch in anderen Serien nicht mehr verwendet. In den 70ern, 80ern und 90ern sind verschiedene Serien unverändert auf verschiedenen Lokalsendern gelaufen, jedoch geriet \"Fist of the North Star\" in starke Kritik, weshalb fortan auf diesen kleineren Sendern auf Anime verzichtet wurde. 1999 begann mit MTV Italy die erste Ausstrahlung von Anime explizit für ein älteres Publikum zu einer passenden Sendezeit.\n\nIn Italien sind speziell Anime für jüngere Mädchen beliebter als in vielen anderen Ländern, speziell \"Rock ’n’ Roll Kids\" ist für vier Staffeln als Realserie umgesetzt worden. Der mitunter populärste Anime ist \"Lupin III\". Italien war Mitfinanzierer einer neuen Serie des Franchises, \"Lupin Sansei\".\n\n\nSpanien.\nVerschiedene Animeserien sind in Spanien zunächst auf dem öffentlich-rechtlichen Sender \"Televisión Español\" gelaufen. Der erste war wie in anderen Ländern \"Kimba, der weiße Löwe\", 1969. jedoch geriet \"Saint Seiya\" in die Kritik und wurde abgesetzt. Auch Koproduktionen wie \"Um die Welt mit Willy Fog\", \"D’Artagnan und die drei MuskeTiere\" und \"Roy, the Little Cid\" sind entstanden. Auch viele Serien des \"World Masterpiece Theater\" wurden gezeigt und einige von lokalen Künstlern adaptiert. So erschien ein spanischer Comic zu \"Heidi\". Ebenso wurde \"Mazinger Z\" adaptiert. Mit dem Aufkommen des Privatfernsehens im Jahre 1990 startete der Sender Telecinco. Er setzte \"Saint Seiya\" fort und importierte fast 100 weitere Animeserien. Genau wie in Frankreich und Italien hat sich die Wahrnehmung für Anime weit vor Deutschland und den USA entwickelt. Jedoch kamen viele dieser Serien in die Kritik aufgrund von Gewaltdarstellungen oder auch wegen freizügigeren Szenen (kurze Röcke bei \"Sailor Moon\" oder Nacktheit bei \"Ranma 1/2\") und wurden 1999 zeitweilig mit Disneycartoons ersetzt.\n\nDie zwei kulturell bedeutendsten Anime in Spanien sind \"Mazinger Z\" und \"Shin Chan\". Zu \"Mazinger Z\" gibt es eine Statue in der Stadt Tarragona, und \"Shin Chan\" hatte mitunter eine größere Beliebtheit im Fernsehen als manche Hauptnachrichtensendungen.\n\n\nRezeption des Mediums.\n\nWissenschaftliche Auseinandersetzung.\nDer Medienhistoriker Jonathan Clements weist darauf hin, dass Anime auf unterschiedliche Weisen rezipiert werden und wurden: lange Zeit nur als Ereignis, das im Kino oder im Fernsehen stattfand. Seit den 1980ern auch als Produkt, als Kaufmedium zum erwerben und besitzen. Die Art, wie Anime erlebt wurde, verändert die Wirkung des Mediums auf den Zuschauer. Anime als Ereignis ist insbesondere in Japan noch heute von Bedeutung, teils auch in abgewandelter Form wie bei Konzert-„Auftritten “der in 3D animierten Figur Miku Hatsune. Auch die Sicht von Journalisten und Wissenschaft auf das Medium wird davon beeinflusst, welche Arten der Rezeption dabei beachtet werden. In der Auseinandersetzung mit dem Medium spielt eine Reihe von Quellen eine Rolle. Dazu zählen die selbstgeschriebenen Chroniken von Unternehmen und von Einzelpersonen, Jahrbücher, digitale und analoge Datenbanken, Magazinsammlungen und Kataloge (vor allem der \"Animage\" und \"Newtype\"), sowie schließlich Originalmaterial an Werken und aus deren Produktion. Letztere sind jedoch nur noch unvollständig erhalten, da aus der Frühzeit bis in die Nachkriegszeit viele Materialien verloren gegangen und nur Berichte, Aufträge, Katalogeinträge oder Werbung dazu erhalten sind. Die Einblicke, die man in das Medium gewinnen kann, können sich dabei besonders bei den Quellen von Studios und Künstler, stark unterscheiden, da viele ihre persönliche Sicht auf die Entwicklung von Anime und ihren Beitrag darin pflegten und verbreiteten. Auch gehen manche Berichte auf mündliche Überlieferung oder Aussagen zurück, die Jahre oder Jahrzehnte nach den Ereignissen niedergeschrieben wurden.\n\n\nFanszene.\nJapanische Animationsfilme haben weltweit eine Fangemeinde, die sich in großen Teilen mit der von Mangas überschneidet. Viele Veranstaltungen widmen sich daher beiden Medien. Eine Fanszene entwickelte sich zunächst in Japan und ab den 1980er Jahren in kleinerem Maße in den USA und Frankreich. Die japanische Fanszene entstand ab den 1970er Jahren, vor allem im Zusammenhang mit Science-Fiction-Serien und dem SF-Fandom. 1978 startete mit \"Animage\" das erste Anime-Magazin in Japan, das sich an ein breites Publikum richtete. Ihm folgten viele weitere, darunter 1985 das für die Szene ebenso bedeutsame \"Newtype\". Im folgenden Jahrzehnt wurden diese Fans auch in der Anime-Industrie selbst aktiv. Die Szene emanzipierte sich von der Science-Fiction-Szene und es kam der Begriff Otaku für Anime-Fans auf, der bis heute sowohl als Selbstbezeichnung als auch spöttische, herabwürdigende Fremdbezeichnung verwendet wird. Ein wichtiges Porträt dieser Fans war die Kurzserie \"Otaku no Video\" des von Fans gegründeten Studios Gainax. Mit zunehmender Verbreitung und Popularität von Animes wie auch Mangas nach der Veröffentlichung von \"Akira\" im Westen und umso mehr nach dem Erfolg von Fernsehserien, darunter \"Sailor Moon\" und \"Dragon Ball\", entwickelte sich auch in Nordamerika und Europa eine größere Fangemeinde. Die Entwicklung dieser Fangemeinde wurde auch befördert durch die mit dem Internet aufkommenden, neuen Kommunikationsformen wie Chats und Foren. Als die deutsche Fanszene um das Jahr 2000 herum wuchs, war sie noch sehr jung. Einer Umfrage von Sozioland aus dem Jahr 2005 sowie einer Untersuchung des französischen \"Centre d'Études et de Recherches Internationales\" zufolge waren die meisten zwischen 14 und 25 Jahren alt. Nur wenige waren über 25, spielten jedoch in der Fanszene eine wichtige Rolle, gründeten die ersten Magazine und Veranstaltungen. 70 bis 85 Prozent der Befragten waren weiblich.\n\nWährend gelegentliche Zuschauer meist synchronisierte Animes konsumieren, werden in der Kern-Fanszene vor allem im Westen eher die untertitelten Fassungen vorgezogen, da diese näher am Original liegen und meist leichter und für eine deutlich größere Zahl an Werken verfügbar sind. Vor allem in den 1990er Jahren waren die synchronisierten Fassungen auch deutlich stärker nachbearbeitet und kulturell dem Zielland angepasst. Insbesondere das Bedürfnis, Werke in möglichst originaler Form zu sehen, spielt daher eine große Rolle beim Bevorzugen von Untertiteln. Damit verbunden entsteht oft auch ein Interesse an japanischer Sprache sowie das Verwenden japanischer (Slang-)Begriffe in der Konversation unter Fans.\n\nDas Medium lädt, wie auch Manga, in großem Maße zu eigener Kreativität ein. In der Fanszene ist die Auseinandersetzung mit den Werken und deren Fortsetzung in Form von Dōjinshi (Fan-Manga), Fanfiction, Fanart oder das Verkleiden als Figur aus einem Anime (Cosplay) weit verbreitet. Dieses große Maß an Interaktion zwischen Medium und Fans kann als ein weiteres wichtiges Merkmal von Anime gesehen werden. Gegenstand nicht weniger Fan-Werke sind von den Fans erdachte Liebesgeschichten zwischen den Charakteren, die im Original nicht vorkommen – häufig auch gleichgeschlechtliche Liebe. Für Geschichten über Liebe zwischen Männern etablierte sich die Genrebezeichnung Yaoi. Manche unter den Fans haben auch besonders starke Zuneigung oder Liebe zu einzelnen Charakteren oder Charakter-Merkmalen, was unter dem Begriff Moe zusammengefasst wird, der auch an dieses Phänomen angelehnte Genremerkmale und Darstellungsmittel bezeichnen kann. Die Anime- und Manga-Fanszene kann als in eine breitere Kultur moderner Japan-Mode eingebettet gesehen werden, bestehend aus J-Pop und Visual Kei, japanischem Essen, Mode, Karaoke und Computerspielen. Bedeutende Veranstaltungen, auf denen sich Fans vorrangig treffen, sind Anime- und Manga-Conventions. Diese Conventions bieten Verkaufsstände, Workshops, Autogrammstunden, Konzerte oder Videoabende und Cosplay-Wettbewerbe. Eine der weltweit größten Conventions ist die Japan Expo in Frankreich mit über 230.000 Besuchern. Darüber hinaus finden viele weitere Veranstaltungen in Europa und Nordamerika statt. Daneben ist Anime auch bei Veranstaltungen zu Japan, Animationsfilm, Comic oder Literatur ein Thema, so finden sich in Deutschland beim Japantag oder der Frankfurter Buchmesse Veranstaltungen zum japanischen Animationsfilm.\n\nDa bis in die 1990er Jahre hinein alle Animes nur mit Blick auf den heimischen Markt entstanden – und viele auch heute noch – ist für ihr Verständnis oft Wissen über die japanische Kultur notwendig. Der Erfolg der Serien außerhalb Japans war für deren Produzenten daher oft unerwartet. Laut Antonia Levi macht gerade diese zusätzliche Ebene, der damit verbundene Aufwand und die Möglichkeit etwas über eine andere Kultur zu lernen bei der Rezeptionauch den Reiz des Mediums für seine westlichen Fans aus, die das „Japanischsein“ von Anime wertschätzen. Die Attraktivität des Medium für die Fans, so Ralf Vollbrecht, liege im deutlichen Unterschied zum „westlichen“ Zeichentrick. Statt „kindlich-kindisch“ zu sein, würde das Körperliche und sexuelle Attraktivität stärker betont und die Geschichten seien auch bei fantastischen Stoffen in ihren Themen nah an den Lebenswelten – insbesondere Entwicklungsthemen werden oft angesprochen – und es gibt ein hohes Identifikationspotenzial für die vor allem junge Zielgruppe und Fangemeinde. Auch Alexander Zahlten betont, der Reiz vieler Serien gerade für jugendliches Publikum läge in der Thematisierung von Transformation des Selbst und der Begegnung mit Anderen, und bringt dies in Zusammenhang mit dem Ende der bipolaren Weltordnung nach dem Ende des Kalten Krieges, das eine Verunsicherung und Sorge um das Selbst gebracht habe. Gerade für Mädchen biete Anime Geschichten mit besonderem Identifikationmöglichkeiten, so Dinah Zank. Vor allem einige Genres von Animes sprechen in Themenwahl und Design ein weibliches Publikum an und bietet diesem auch Charaktere wie weibliche Kriegerinnen, die Stereotype aufbrechen. Dabei sprechen solche Werke auch männliche Zuschauer an, auch weil sie die Flucht in eine andersartige, „sichere, fantasievolle und idealisierte“ Mädchenwelt bieten.\n\n\n\n"}
{"id": "82", "url": "https://de.wikipedia.org/wiki?curid=82", "title": "Actionfilm", "text": "Actionfilm\n\nDer Actionfilm (von engl. \"action\": Tat, Handlung, Bewegung) ist ein Filmgenre des Unterhaltungskinos, in welchem der Fortgang der äußeren Handlung von zumeist spektakulär inszenierten Kampf- und Gewaltszenen vorangetrieben und illustriert wird. Es geht eher um stimulierende Aktionen als um inhaltliche Zusammenhänge, empathisches Miterleben der Gefühlswelt der Protagonisten oder künstlerisch-ästhetische Bildwelten. Hauptbestandteile von Actionfilmen sind daher meist aufwendig gedrehte Stunts, Nahkampf-Szenen, Schießereien, Explosionen und Verfolgungsjagden.\n\n\nGeschichte.\n\nUrsprung.\nDer Actionfilm ist seit den 1960er-Jahren ein eigenständiges Filmgenre, doch seine Konventionen sind bereits seit dem Beginn der Filmgeschichte bekannt. Künstler aus dem Vaudeville wie Buster Keaton ließen ihr Können in artistischer Bewegung in Verbindung mit Tricktechnik in ihr Filmschaffen einfließen.\n\nDer Actionfilm als eigenes Genre hat seinen Ursprung im Kriminalfilm, in dem in den 1950er-Jahren Aktion und explizite Darstellung von physischer Gewalt zunehmend an Bedeutung gewann, etwa in Stanley Kubricks \"Die Rechnung ging nicht auf\" (1956). Alfred Hitchcock präsentierte in \"Der unsichtbare Dritte\" (1959) erstmals eine geschlossene filmische Welt, die ausschließlich als Herausforderung für die physische Aktion der Hauptfigur dient. Dieses Konzept der geschlossenen Actionwelt, die rein zum Ausleben von Körperakrobatik und zur Demonstration spektakulärer Gewaltanwendungstechniken existiert, fand seine Fortsetzung in den Filmen der James-Bond-Reihe und in Fernsehserien wie \"Kobra, übernehmen Sie\".\n\nDieser von realistischer Darstellung und moralischer Wertung weit entfernten Illusionstendenz stehen die Regisseure der Bewegung des New Hollywood gegenüber, die in offener Form Aktion und Gewaltanwendung inszenierten. Sie reagierten auf gesellschaftliche und politische Entwicklungen wie die Protestbewegung und den Vietnamkrieg und suchten den Kontext der Darstellung zu Fragen der Moral, etwa zu den Folgen von Gewaltanwendung auf den menschlichen Körper. Beispiele für diese realistischere und ernüchternde Herangehensweise sind Arthur Penns \"Bonnie und Clyde\" (1967) und Sam Peckinpahs \"The Wild Bunch – Sie kannten kein Gesetz\" (1969).\n\n\nHochphase 1970–1990er.\nMit den Bruce-Lee-Filmen fand eine Ära der Überbetonung physischer Kräfte und des Körperkultes im Actionfilm ihren Anfang. Stilmittel wie Zeitlupe und Tonverfremdungen führten zur Entwicklung und Definition des Subgenres des Martial-Arts-Films. In den 1980er Jahren beherrschte der Actionfilm das Mainstreamkino mit Stars wie Arnold Schwarzenegger und Sylvester Stallone, die durch Bodybuilding den Körperkult auf einen Höhepunkt führten. Neben wenigen humorvollen Verfilmungen wie Indiana Jones beherrschten überwiegend reaktionäre Themen wie Rachephantasien und das stereotype Aufbauen von Feindbildern das Actionkino.\n\nIn den 1990er Jahren wurde das Genre zunehmend ironisiert und spiegelte sich selbst, etwa in Filmen wie \"Last Action Hero\" (John McTiernan, 1993) und \"True Lies\" (James Cameron, 1994). McTiernans \"Stirb-langsam\"-Reihe (1988 bis 2013) brach ebenfalls ironisch mit dem Heldenbild des Actionfilms und ließ ihren Protagonisten, dargestellt von Bruce Willis, entmystifiziert als leidensfähigen Jedermann gegen das Böse siegen. Stars wie Jackie Chan vereinnahmten den Stunt als Teil der künstlerischen Darstellung und zogen einen Teil ihrer Popularität aus der Tatsache, auch gefährliche Action grundsätzlich selbst zu bewerkstelligen. Mit The Rock – Fels der Entscheidung oder Con Air wurden für das Action-Genre relativ anspruchsvolle Werke geschaffen, die sich vom aufkommenden Direct-to-Video-Billigtrend abhoben.\n\n\nWeiterentwicklung.\nZudem gewannen ab den mittleren 1990ern aufwendige digitale Spezialeffekte und Stunts gegenüber einfachen Kämpfen und Schusswechseln an Bedeutung, z.&nbsp;B. in der -Reihe mit Tom Cruise oder xXx - Triple X mit Vin Diesel. Viele Elemente des Actionfilms wurden bereits Ende der 1970er in der mit Krieg der Sterne beginnenden ersten Trilogie von Star Wars und in etwas geringerem Maße auch Star Trek in die Science Fiction übernommen. Ab 2000 wurde der Superheldenfilm erneut populär, welcher durch das Batman-Reboot oder Marvels Avengers, die mit enormen tricktechnischen und finanziellen Mitteln produziert wurden. Spätestens in den 2010ern verschwanden klassische Actionfilme als Blockbuster weitgehend aus dem Kino, und Fortsetzungen wie Stirb Langsam 5 oder dem nachempfundene Filme wie White House Down waren nur durchschnittlich erfolgreich. Eine Ausnahme stellt aber z.&nbsp;B. die The Expendables-Trilogie dar, die als Ensemble-Filme um die populären Schauspieler Schwarzenegger, Stallone, Willis, Jason Statham, Dolph Lundgren und andere als eine Art Hommage inszeniert wurde.\n\n\nWeibliche Protagonisten.\nWeibliche Hauptfiguren waren bis Ende der 1990er Jahre selten, eine Ausnahme stellten aber die von Sigourney Weaver verkörperte \"Ellen Ripley\" der Alien-Reihe und die von Linda Hamilton verkörperte \"Sarah Connor\" in Terminator dar. Später folgten Brigitte Nielsen (Red Sonja), Kate Beckinsale (Underworld), Uma Thurman (Kill Bill), Michelle Rodriguez (SWAT - Die Spezialeinheit) oder Halle Berry (Catwoman), welche meist Seite an Seite mit einem männlichen Filmpartner agierten. Mit Wonder Woman (Gal Gadot) oder Rogue One (Felicity Jones) wurden weitere Heldinnen geschaffen.\n\n\nMotive und Darstellungsformen.\nDie Bewegung, Grundmotiv des Films, dient im Actionfilm in erster Linie Schauzwecken und hat weniger erzählerische Funktionen. Oft werden im Actionfilm in der Art einer Nummernrevue geschlossene Sequenzeinheiten aneinandergereiht, die der Zurschaustellung unterschiedlichster bewegungsgetriebener Konflikt- oder Duellsituationen dienen, etwa Shootouts, Verfolgungsjagden, Körperkämpfe oder Explosionen. Subjekte der Aktion sind speziell im US-amerikanischen Actionfilm häufig sich verfolgende Fahrzeuge, etwa in \"Brennpunkt Brooklyn\" (William Friedkin, 1971), \"Bullitt\" (Peter Yates, 1968) oder \"Dirty Harry\" (Don Siegel, 1971). Der dargestellten Gewalt wird häufig in wirklichkeitsfremder Weise der Realitätsbezug genommen. Filmische Mittel wie Konvergenzmontage und Parallelmontage strukturieren diese Nummern, etwa um einen Spannungsbogen in einer Last-Minute-Rescue aufzulösen.\n\nIn den Plots geht es meist um den Kampf zwischen Gut und Böse, die Identifikationsfigur ist häufig ein physisch starker männlicher Held (oder eine weibliche Heldin, siehe beispielsweise \"\"), der/die in der Regel eindeutige moralische Prinzipien vertritt, die den ethischen und weltanschaulichen Grundlagen der westlichen Kultur entsprechen (Gut gegen Böse, Beschützen der Schwachen, Gerechtigkeit, Sühne für erlittenes Unrecht, Verteidigung und Bewahrung der vertrauten Lebensweise usw.). Häufig fließen erzählerische Elemente aus verwandten Genres in den Actionfilm ein, unter anderem aus dem Abenteuerfilm, dem Kriegsfilm, dem Kriminalfilm, dem Psychothriller, dem Horrorfilm und dem Science-Fiction-Film.\n\n\n"}
{"id": "83", "url": "https://de.wikipedia.org/wiki?curid=83", "title": "Al Pacino", "text": "Al Pacino\n\nAlfredo James „Al“ Pacino (* 25. April 1940 in New York) ist ein US-amerikanischer Schauspieler, Filmregisseur und Filmproduzent. Er gilt für viele Kritiker und Zuschauer als einer der herausragenden Charakterdarsteller des zeitgenössischen amerikanischen Films und Theaters. So ist er seit den 1970er Jahren in zahlreichen Filmklassikern zu sehen.\n\nIm Laufe seiner Karriere wurde er unter anderem mit dem Oscar, dem Golden Globe Award, dem Tony Award und der National Medal of Arts ausgezeichnet. Seine bekanntesten Rollen sind die des \"Michael Corleone\" in der von Francis Ford Coppola inszenierten \"Der Pate\"-Trilogie und als Gangster \"Tony Montana\" in \"Scarface\".\n\n\nLeben.\n\nKindheit und Jugend.\nAl Pacino, geboren in Manhattan, ist der Sohn von Salvatore Pacino, geboren in der sizilianischen Stadt Corleone, und von Rose Gerard, der Tochter eines italienischen Einwanderers und einer italienisch-amerikanischen Mutter, die in New York geboren wurde. Seine Eltern ließen sich scheiden, als er zwei Jahre alt war. Nach der Scheidung zogen Al und seine Mutter in die Bronx, und Pacino wuchs bei seinen sizilianischen Großeltern, die aus der Heimatstadt seines Vaters eingewandert waren, in der New Yorker South Bronx auf.\n\nSein Vater Salvatore, der nach Covina zog, arbeitete als Versicherungsagent und besaß ein Restaurant, „Pacino’s Lounge“. Das \"Pacino’s\" wurde in wirtschaftlich schweren Zeiten in den frühen 1990er Jahren geschlossen; heute trägt es den Namen \"Citrus Grill.\" Salvatore Pacino starb am 1. Januar 2005 im Alter von 82 Jahren.\n\nAl Pacino ist der Stiefsohn der Schauspielerin und Maskenbildnerin Katherin Kovin-Pacino und hat vier Schwestern: Josette, eine Lehrerin, die Zwillinge Roberta und Paula sowie Desiree, die sein Vater in seiner vierten Ehe adoptierte.\n\nMit 17 Jahren wurde Pacino der Schule verwiesen und ging fortan auf die \"Manhattan School of Performing Arts\". Nebenher arbeitete er in kleineren Theatern als Platzanweiser und Kartenabreißer.\n\n\nSchauspielkarriere.\nPacino interessierte sich schon als Kind für die Schauspielerei. Er verfeinerte sein Talent an zwei renommierten New Yorker Schauspielschulen, in Herbert Berghofs HB Studio und später bei Lee Strasberg im The Actors Studio. Dort spielte er in mehreren erfolgreichen Theaterstücken wie in seinem Debütstück \"The Connection\" und in \"The Indian Wants the Bronx\", für das er mit einem Obie-Award ausgezeichnet wurde.\n\n\nFilmschauspieler.\nPacino wurde durch den späteren Filmproduzenten Martin Bregman bei einem Off-Broadway-Auftritt entdeckt. 1969 wirkte er in seiner ersten Hollywood-Produktion \"Ich, Natalie\" mit. 1971 erhielt er neben Kitty Winn eine Rolle in dem Film \"Panik im Needle Park,\" die ihm den Weg für die Rolle des Michael Corleone in Francis Ford Coppolas \"Der Pate\" (1972) ebnete und ihm 1973 seine erste Oscar-Nominierung einbrachte.\n\nNach \"Hundstage\" wurde es stiller um Pacino. Erst in den 1980er Jahren brachte er sich durch Filme wie Brian De Palmas \"Scarface\" (1983) und \"Sea of Love – Melodie des Todes\" (1989) wieder ins Gespräch. Nach einer erneuten Zusammenarbeit mit Coppola in \"Der Pate III\" (1990) folgte der Thriller \"Heat\" (1995) mit Schauspielkollege Robert De Niro. Die männliche Hauptrolle in dem Film \"Pretty Woman\" lehnte er ab.\n\nSeine Darstellung des AIDS-kranken Schwulenhassers Roy Cohn in der Miniserie \"Engel in Amerika\" (2003) brachte ihm zahlreiche Preise ein und wurde von der Kritik hoch gelobt.\n\nPacino ist dafür bekannt, seine Rollen bis zum Extrem auszufüllen. Während sein Spiel in den 1970er Jahren – insbesondere in \"Der Pate\" – dabei zumeist minimalistisch und zurückhaltend war, änderte es sich mit seinem Comeback in den 1980er Jahren radikal: Pacinos exaltierte Darstellungen in Filmen wie \"Scarface, Im Auftrag des Teufels, An jedem verdammten Sonntag\" oder auch \"Der Duft der Frauen\" wurden von Kritikern oftmals als Overacting bezeichnet. Für einen Großteil des Publikums zeichnet ihn allerdings genau diese Art und Weise als einen der größten Charakterdarsteller der Gegenwart aus. Viele seiner Filme, darunter auch \"Glengarry Glen Ross\", der zunächst floppte, zählen heute zu den Besten ihres Genres.\n\n\nTheaterarbeit.\nNeben seiner Karriere als Filmschauspieler arbeitet er weiterhin regelmäßig an verschiedenen Theatern – sowohl als Darsteller wie auch als Regisseur und Produzent. Für seine Rollen in den Bühneninszenierungen von \"The Basic Training Of Pavlo Hummel\" von David Rabe und \"Does A Tiger Wear A Necktie?\" von Don Petersen erhielt er jeweils einen Tony Award.\n\nAls langjähriges Mitglied von David Wheelers \"Experimental Theatre Company\" in Boston stand er unter anderem in \"Richard III.\" und Bertolt Brechts \"Der aufhaltsame Aufstieg des Arturo Ui\" auf der Bühne. In New York und London spielte Pacino in David Mamets \"American Buffalo,\" in New York war er der Titelheld in \"Richard III.\" und spielte den Mark Anton in \"Julius Cäsar\". Außerdem stand er im \"Square Theatre\" in New York in Oscar Wildes \"Salome\" auf der Bühne und wirkte in der Uraufführung von Ira Levins Theaterstück \"Chinese Coffee\" mit.\nIn der Theatersaison 2010/2011 spielte Pacino in der \"Shakespeare in the Park\"-Produktion \"Der Kaufmann von Venedig\" den \"Shylock\". Mit der Inszenierung gewann Heather Lind, die als Shylocks Tochter \"Jessica\" auftrat, einen Theatre World Award für ein herausragendes Broadway-Debüt.\n\n\nEigene Projekte.\nPacinos erstes eigenständiges Projekt war 1996 \"Looking for Richard,\" eine dokumentarische und künstlerische Filmstudie über den Charakter von Shakespeares \"Richard III.\", bei dem er Regie führte, die Produktion übernahm, das Drehbuch schrieb und die Hauptrolle spielte.\n\nPacino war auch Produzent, Hauptdarsteller und Koautor des Independent-Kurzfilms \"The Local Stigmatic\", einer Verfilmung des gleichnamigen Theaterstücks von Heathcote Williams, das sowohl im New Yorker Museum of Modern Art als auch im \"Public Theatre\" aufgeführt wurde.\n\n\nPrivatleben.\nAl Pacino war nie verheiratet. Er hat drei Kinder, eine Tochter mit Jan Tarrant und Zwillinge (Tochter und Sohn, * 2001) mit Beverly D’Angelo. Bis in die 1980er Jahre war er mit Marthe Keller liiert, mit der er sieben Jahre zusammen wohnte.\n\n\nSynchronstimme.\nAl Pacino wurde im Lauf der Jahrzehnte von verschiedenen deutschen Sprechern synchronisiert. Nachdem er im ersten Jahrzehnt seiner Karriere in der Regel von Lutz Mackensy gesprochen wurde (u.&nbsp;a. \"Der Pate I und II, Serpico, Hundstage\"), übernahm mit \"Scarface\" (1983) Frank Glaubrecht die Synchronisation des Schauspielers. Seit 1995 ist Glaubrecht alleiniger Sprecher (\"Heat, City Hall, Insomnia\" etc.) und er kann mittlerweile als Pacinos Standardstimme bezeichnet werden.\n\nIn den frühen 1990er Jahren wurde Pacino auch von Joachim Kemmer \"(Dick Tracy)\", Gottfried Kramer \"(Der Pate III)\" und Klaus Kindler \"(Carlito’s Way)\" gesprochen.\n\n"}
{"id": "88", "url": "https://de.wikipedia.org/wiki?curid=88", "title": "Alkohole", "text": "Alkohole\n\nAlkohole (, eigentlich: feines Antimonpulver) sind organische chemische Verbindungen, die eine oder mehrere an unterschiedliche aliphatische Kohlenstoffatome gebundene Hydroxygruppen (–O–H) besitzen.\n\nDer Unterschied zwischen Alkoholen und anderen Verbindungen mit OH-Gruppen (z.&nbsp;B. Enole, Halbacetale oder Carbonsäuren) als Teil der funktionellen Gruppe ist, dass in Alkoholen jedes Kohlenstoffatom, das eine OH-Gruppe trägt, sp-hybridisiert sein muss und außer der Hydroxygruppe nur noch an Kohlenstoff- oder Wasserstoffatomen gebunden sein darf. Nur dieser Bindungszustand entspricht dem Oxidationszustand eines normalen Alkanols.\n\nWenn die Hydroxygruppe an ein nicht-sp-hybridisiertes Kohlenstoffatom gebunden ist, das Teil eines aromatischen Ringes ist, so werden diese Verbindungen als Phenole bezeichnet und zählen nicht zu den Alkoholen.\n\nWährend Alkohole schwächer sauer sind als Wasser und mit einem pKs-Wert von ca. 16 zu den „sehr schwachen Säuren“ zählen, gehören normale Phenole mit einem pKs-Wert von 10 bereits zu den „schwachen Säuren“.\n\n\nNomenklatur und Einteilung.\nDer Name einfacher Alkohole ergibt sich als Zusammensetzung aus dem Namen des ursprünglichen Alkans und der Endung . Zusätzlich wird die Position der OH-Gruppe durch eine vorangestellte Zahl verdeutlicht, zum Beispiel . Eine veraltete, bis 1957 gültige Bezeichnung für Alkohole ist – nach einem Vorschlag von Hermann Kolbe – \"Carbinole\".\n\nDie Stoffgruppe der Alkohole wird nach verschiedenen Kriterien (Zahl der Nichtwasserstoffnachbarn, Wertigkeit, Vorhandensein von Doppel-/Dreifachbindungen und Kettenlänge) eingeteilt.\n\n\nZahl der Nichtwasserstoffnachbarn.\nMan unterscheidet Alkohole nach der Zahl der C- und H-Atome an dem C-Atom der funktionellen Gruppe, an das auch die Hydroxygruppe gebunden ist. Bei primären Alkoholen sind an dieses C-Atom neben einem C-Atom zwei H-Atome, bei sekundären Alkoholen neben zwei C-Atomen ein H-Atom und bei tertiären Alkoholen neben drei C-Atomen kein Wasserstoffatom gebunden. Ein Sonderfall ist der Alkohol mit nur einem C-Atom, das Methanol, das neben der Hydroxygruppe nur drei Wasserstoffatome am C-Atom der funktionellen Gruppe trägt.\n\nWertigkeit der Alkohole.\nIst mehr als eine Hydroxygruppe an verschiedenen C-Atomen in einem Alkoholmolekül vorhanden, wird deren Anzahl durch Einfügen einer der Anzahl der Hydroxygruppen entsprechenden griechischen Silbe (-di-, -tri- usw.) vor der Endung angegeben und man spricht von mehrwertigen Alkoholen. Ein ist das (Trivialname Ethylenglycol), ein das (Trivialname Glycerin). Die Zahl vor der Endung gibt die Position der funktionellen Gruppe(n) an. Dies gilt auch für einwertige Alkohole, zum Beispiel (Trivialname Isopropanol).\n\n\nDoppel- bzw. Dreifachbindungen.\nIn Bezug auf das Vorhandensein von Doppel- bzw. Dreifachbindungen in der Kette der C-Atome unterscheidet man Alkanole (abgeleitet von Alkanen), Alkenole (abgeleitet von Alkenen) und Alkinole (abgeleitet von Alkinen). Für den Fall, dass die OH-Gruppe an ein sp-hybridisiertes Kohlenstoffatom gebunden ist, hat man es mit einem anderen Oxidationszustand und damit mit einer anderen Stoffgruppe zu tun, nämlich mit den meist instabilen Enolen.\n\n\nKettenlänge.\nÜber die Kettenlänge werden Alkohole ebenfalls unterschieden. Die Bezeichnung \"Fettalkohole\" verwendet man für Alkohole mit endständiger primärer mit gerader Kette und einer Länge von sechs (Hexanol) bis hin zu 22 (Behenylalkohol) Kohlenstoffatomen. Sie werden meist durch Reduktion der aus Fettsäuren gewonnen. Die höheren primären Alkohole mit 24 bis 36 Kohlenstoffatome bezeichnet man als Wachsalkohole.\n\n\nPhysikalische Eigenschaften.\nNiedrigmolekulare Alkohole sind Flüssigkeiten, die einen charakteristischen Geruch und einen brennenden Geschmack besitzen. Höhere Alkohole sind meist feste Verbindungen mit nur schwach ausgeprägtem Geruch. Aufgrund von intermolekularen Wasserstoffbrückenbindungen besitzen die Alkohole im Vergleich zu Kohlenwasserstoffen gleicher Molekülmasse relativ hohe Schmelz- und Siedepunkte. Wichtigstes gemeinsames Merkmal der Alkohole ist die Hydrophilie. Diese Eigenschaft nimmt mit zunehmender Länge des Alkylrestes ab und mit der Anzahl der Hydroxygruppen zu. Besonders die kurzkettigen Alkohole werden aufgrund ihres amphiphilen Charakters oft als Lösungsmittel verwendet.\n\n\nHohe Siedepunkte.\nSauerstoff ist elektronegativer als Wasserstoff und Kohlenstoff, d.&nbsp;h., er zieht Elektronen stärker an als diese. Das führt zu einer unsymmetrischen Verteilung der Elektronen entlang der , man spricht von einer polaren Bindung, es bildet sich ein molekularer Dipol aus. Diese Dipole können untereinander Wasserstoffbrückenbindungen ausbilden, die die Anziehung der einzelnen Moleküle untereinander drastisch verstärken.\nDies führt für Alkohole zu relativ hohen Siedepunkten gegenüber den um eine Methyleneinheit verlängerten Homologen ihrer Stammverbindung, die eine annähernd gleiche molarer Masse besitzen. So hat beispielsweise das unpolare Ethan (CH) (M&nbsp;=&nbsp;30) einen Siedepunkt von −89&nbsp;°C, während Methanol (CHOH) (M&nbsp;=&nbsp;32) diesen erst bei 65&nbsp;°C erreicht.\n\nZusammenfassend:\n\n\nHydrophilie.\nDie OH-Gruppe ist ebenfalls in der Lage, Wasserstoffbrückenbindungen mit Wasser einzugehen. Sie erhöht damit die Hydrophilie, die Wasserlöslichkeit, der Verbindung. Organische Alkylreste selbst sind nicht wasserlöslich, also hydrophob. Die Wasserlöslichkeit sinkt daher mit der Größe des organischen Anteils und steigt mit der Zahl der Hydroxygruppen. Die Propanole und \"tert\"-Butanol sind bei Raumtemperatur noch in jedem Verhältnis mit Wasser mischbar, alle langkettigeren Alkohole lösen sich nur noch in zunehmend kleinen Mengen. Größere Mengen gelöster anorganischer Salze können auch bei den kurzkettigen Alkoholen eine Phasentrennung bewirken („Salzfracht“).\n\nZusammenfassend:\n\n\nAcidität und Deprotonierung.\nMit einem pK-Wert (Säurestärke) von etwa 16 sind Alkohole schwächer sauer als Wasser und reagieren somit in wässriger Lösung näherungsweise neutral. Die Acidität von Alkoholen nimmt in der Reihe von Methanol über primäre, sekundäre und tertiäre Alkohole ab. Es ist möglich, Alkohole mit starken Basen wie z.&nbsp;B. Hydridanionen oder durch Reaktion mit Natrium unter Entwicklung von Wasserstoff zu deprotonieren. Die dabei entstehenden \"Alkoholate\" können dann als stark nucleophile Anionen für weitere Reaktionen eingesetzt werden.\nEs ist auch möglich, Alkohole in gewissem Umfang mit starken Säuren zu protonieren:\n\nSpektroskopie.\nIm IR-Spektrum von Alkoholen ist deutlich die breite Bande der O–H-Valenzschwingung im Bereich von 3200–3650&nbsp;cm zu erkennen. Die Breite des Peaks wird durch Wasserstoffbrückenbindungen mit Wassermolekülen verursacht und ist in Spektren von wasserfreien Alkoholen in einem engeren Bereich von 3620–3650&nbsp;cm zu finden.\n\n\nChemische Eigenschaften.\n\nReaktion mit konzentrierter Schwefelsäure.\nUnterhalb von 140&nbsp;°C bildet sich der Ester der Schwefelsäure.\n\nBei etwa 140&nbsp;°C findet die Kondensationsreaktion zu einem Ether statt.\n\nOberhalb von 170&nbsp;°C werden primäre Alkohole zu Alkenen dehydratisiert. (Eliminierung)\n\n\nSelenoxid-Eliminierung.\nDie Selenoxid-Eliminierung ist eine milde Variante der Eliminierung.\n\n\nVeresterung.\nMit Carbonsäuren reagieren Alkohole unter Wasserabgabe zu Estern, diese Reaktion wird auch Veresterung genannt. Diese Reaktion wird durch Säuren katalysiert.\n\n\nOxidation.\nPrimäre Alkohole lassen sich zu Aldehyden und Carbonsäuren, sekundäre Alkohole zu Ketonen oxidieren. Tertiäre Alkohole lassen sich nicht weiter oxidieren, es sei denn unter Zerstörung des Kohlenstoffgerüsts.\n\nZur Oxidation von primären Alkoholen zur Carbonsäure können Chrom(VI)-haltige Oxidationsmittel eingesetzt werden, wie sie z.&nbsp;B. bei der Jones-Oxidation Anwendung finden. Als chromfreies, weniger giftiges Reagenz steht wässriges Rutheniumtetroxid zur Verfügung.\n\nDie Oxidation eines primären Alkohols kann unter Verwendung bestimmter Chrom(VI)-Verbindungen wie dem Collins-Reagenz auch nur bis zur Stufe des Aldehyds erfolgen. Entscheidend ist, dass wasserfreie Lösungsmittel eingesetzt werden. Ist kein Wasser anwesend, kann keine Hydratisierung zum geminalen Diol des Aldehyds (Aldehydhydrate) stattfinden.\n\nDa lösliche Chromate sehr giftig sind, sowie karzinogene und mutagene Eigenschaften besitzen, wurden alternative Methoden zur Oxidation von Alkoholen entwickelt. Eine häufig zur Anwendung kommende Methode ist die Swern-Oxidation mit aktiviertem Dimethylsulfoxid. Fast alle Methoden eignen sich ebenfalls für die Oxidation sekundärer Alkohole zu Ketonen. Die folgende Aufzählung liefert eine Übersicht der wichtigsten Methoden.\n\nOxidation zur Carbonsäure/zum Keton:\n\nOxidation zum Aldehyd/zum Keton:\n\n\n\nAcetalbildung.\nMit Aldehyden reagieren Alkohole in Gegenwart saurer Katalysatoren zu Halbacetalen bzw. Acetalen.\n\n\nVerwendung.\nViele Alkohole sind wichtige Lösungsmittel, die sowohl in der Industrie, als auch im Haushalt eingesetzt werden; die mengenmäßig wichtigsten sind Methanol, Ethanol, 2-Propanol und \"n\"-Butanol. Im Jahr 2011 wurden weltweit etwa 6,4&nbsp;Mio.&nbsp;Tonnen dieser alkoholischen Lösungsmittel nachgefragt.\n\n\nNachweis.\n\nAlcotest.\nDer Umsatz von Alkoholen mit Dichromaten in schwefelsaurer Lösung ist geeignet, um Alkohole quantitativ nachzuweisen und wurde früher in den Alcotest-Röhrchen eingesetzt:\n\nDas Nachweisprinzip beruht auf dem Farbumschlag von gelb-orange (saure Dichromatlösung) nach grün (Chrom(III)-Ionen) und kann spektralphotometrisch gemessen werden.\n\n\nCertest.\nEine weitere Möglichkeit besteht in der Umsetzung mit Ammoniumcer(IV)-nitrat. Hierbei wird eine konzentrierte Lösung von Ammoniumcer(IV)-nitrat mit einer verdünnten Lösung der unbekannten Substanz versetzt. Enthält die unbekannte Substanz Alkohol-Gruppen, färbt sich das Gemisch rot (manchmal auch grün). Enthält die Substanz Phenole, fällt ein brauner Niederschlag aus.\nDer Grund für diese Farbreaktion ist eine Komplexbildung, genauer gesagt eine Ligandensubstitution, bei der ein Alkohol/Phenol mit dem Sauerstoffatom am Cer(IV) koordiniert. Durch die Veränderung der Ligandensphäre verändert sich die Farbe des Cer(IV) von hellgelb zu rot/grün/braun. Leicht oxidierbare Alkohole/Phenole können einen negativen Nachweis ergeben, indem sie das Cer(IV) zu Cer(III) reduzieren.\n\n\nLucas-Probe.\nDer Nachweis des Substitutionsgrades eines Alkohols, also ob es sich dabei um einen primären, sekundären oder tertiären Alkohol handelt, erfolgt über nucleophile Substitution der OH-Gruppe gegen Chlorid durch die Lucas-Probe. Die Substitution hat zur Folge, dass sich die entstehende Substanz nicht mehr in Wasser löst und damit eine eigene Phase ausbildet.\nDabei ist die Geschwindigkeit dieser Phasenbildung entscheidend:\n\n\nVoraussetzung für diesen Test ist, dass sich der ursprüngliche Alkohol in Wasser löst. Auch darf keine andere unter den Reaktionsbedingungen substituierbare Gruppe vorliegen.\n\n\nSpektroskopie und Derivatisierung.\nDie eindeutige Identifizierung eines unbekannten Alkohols erfolgt entweder spektroskopisch oder durch Synthese eines charakteristischen Derivates, das einen Schmelzpunkt hat, der von den Schmelzpunkten gleicher Derivate ähnlicher Alkohole gut zu unterscheiden ist. Oftmals werden sie über Ester der oder der identifiziert. Hierzu wird die zu analysierende Substanz in Gegenwart geringer Mengen Schwefelsäure umgesetzt. Die Schmelzpunkte dieser Derivate sind in der Regel scharf.\n\nDie Derivate der besitzen in der Regel höhere Schmelzpunkte als die der . Sie werden dann bevorzugt gewählt, wenn der Schmelzpunkt mit der zu niedrig ist und keine genaue Bestimmung mehr möglich wird.\n\n"}
{"id": "89", "url": "https://de.wikipedia.org/wiki?curid=89", "title": "The Visitors", "text": "The Visitors\n\nThe Visitors ist das achte und letzte Studioalbum der schwedischen Popgruppe ABBA. Es wurde erstmals am 30. November 1981 in Schweden veröffentlicht. Die Aufnahmen im Studio dauerten von März bis November 1981. Etwa gleichzeitig mit der LP wurde \"One of Us\" als Single herausgegeben und zum letzten internationalen Charterfolg der Gruppe, während die nachfolgende Singleauskopplung \"Head over Heels\" floppte. \"The Visitors\" war zudem die erste veröffentlichte Audio-CD der Musikgeschichte und wurde am 17. August 1982 vorgestellt.\nIm Vergleich zu den Vorgängeralben von ABBA weist dieses einige musikalische Eigenheiten auf, die zuvor in den Produktionen der Gruppe kaum Anwendung fanden. So wird kein einziges Stück auf der gesamten LP von den beiden ABBA-Sängerinnen Agnetha Fältskog und Anni-Frid Lyngstad durchgehend gemeinsam gesungen. Die Lead Vocals wurden in jedem der neun Lieder auf eine der beiden aufgeteilt. Zudem wurden viele Elemente, die bis dato charakteristisch für die Musik der Gruppe gewesen waren, durch neue ersetzt. So handeln die Texte von Trennung, Abschiedschmerz und Kriegsangst. \n\n\nEntstehung.\nBenny Andersson und Björn Ulvaeus begannen im Februar 1981 mit dem Komponieren für ein neues Studioalbum. Die am 12. Februar 1981 öffentlich bekannt gegebene Scheidung der beiden ABBA-Mitglieder Andersson und Lyngstad überschattete dabei die Arbeit innerhalb der Band. Die Aufnahmen für drei neue Stücke begannen am 16. März 1981 in den Polar Music Studios in Stockholm. Darunter befand sich mit \"When All Is Said And Done\" eine Pop-Ballade, deren Text von einer zerbrochenen Beziehung handelt und sich auf die Trennung von Lyngstad und Andersson beziehen sollte. Dementsprechend übernahm Lyngstad die Lead Vocals und ließ in ihre stimmliche Darbietung auch die emotionalen Aspekte mit einfließen, die angesichts ihrer damaligen Privatsituation gegeben waren. \n\nAuch \"Slipping Through My Fingers\" gehörte zu den ersten drei eingespielten Songs. Ulvaeus hatte die Idee für den Text, nachdem er seine Tochter Linda dabei beobachtet hatte, wie sie zur Schule ging. Er handelt von den gemischten Gefühlen der Eltern beim Heranwachsen ihrer Kinder und der damit verbundenen Einsamkeit und Nostalgie. Hier übernahm Agnetha Fältskog den Leadgesang und erzählte später, der Song habe sich „sehr echt angefühlt“. Eine symbolische Kulisse der Küche und des Frühstückstisches, die im Text des Songs vorkommen, befindet sich heute im ABBA-Museum auf Djurgården in Stockholm.\n\nEin weiterer Song, dessen Grundspuren im März 1981 eingespielt wurde, war \"Two for the Price of One\", bei dem zur Abwechslung Ulvaeus den Hauptgesang übernahm. Dieser meinte später, der Song hätte trotz des trivialen Textes ein Hit werden können, hätten Fältskog und Lyngstad die Lead Vocals gesungen. Etwa zur selben Zeit wurde das bisherige analoge Aufnahmegerät im Tonstudio durch ein digitales mit 32 Spuren ersetzt. Zwischen 27. und 29. April 1981 wurden \"Slipping Through My Fingers\" und \"Two for the Price of One\" für das Fernsehspecial \"Dick Cavett Meets ABBA\" live aufgezeichnet, neben Darbietungen einiger älterer ABBA-Songs. \n\nAm 26. Mai wurde ein erster Versuch unternommen, \"Like an Angel Passing Through My Room\" aufzunehmen. Diese erste Demoversion trug den Arbeitstitel \"Twinkle Twinkle Little Star\" und wurde von Ulvaeus gesungen. Eine weitere Alternative zur selben Melodie entstand mit \"Another Morning Without You\", bei der Lyngstad solo sang. Bis zum Ende der Aufnahmesessions wurden weitere Varianten mit verschiedenen Instrumentierungen und Lead Vocals ausprobiert. Eine Version, die beide ABBA-Sängerinnen beinhaltete, wurde später verworfen und durch eine Variante ersetzt, in der Lyngstad vollständig alleine singt. Für die Deluxe Edition des Albums wurde 2012 ein neun-minütiges Medley zusammengestellt, das die Entstehung von \"Like an Angel Passing Through My Room\" über mehrere Alternativversionen dokumentiert.\n\nAm 2. September 1981 war Aufnahmebeginn für \"I Let the Music Speak\" und \"Head over Heels\". Besonders bei letzterem hatten die Musiker Probleme. Da die neue digitale Studiotechnik sehr präzise arbeitete, konnte Toningenieur Michael B. Tretow nur noch mäßig mit Verzögerung und Verschiebung der Spuren experimentieren, was die Aufnahmen „trockener“ und „kälter“ wirken ließ. So wirkte \"Head over Heels\" trotz eingängiger Melodie und ausgefeiltem Arrangement „wie eingefroren“, zählt aber dennoch zu den schwungvolleren und fröhlicheren Songs des Albums. Währenddessen zeugte \"I Let the Music Speak\" wieder einmal von den Ambitionen der beiden Komponisten, ein Musical zu schreiben. Parallel wurde \"Should I Laugh or Cry\" produziert, das später nicht auf der Titelliste des Albums erschien, weil es als „nicht stimmig genug“ angesehen wurde. Stattdessen wurde es im Dezember 1981 als B-Seite veröffentlicht.\n\nAm 15. Oktober 1981 gingen die Aufnahmen mit \"Soldiers, One of Us\" und das für das Album namensgebende Lied \"The Visitors\" zu Ende. Der Text des ersteren handelte in Zusammenhang mit dem Kalten Krieg von Kriegsangst und der Furcht vor einer möglichen neuen faschistischen Bewegung. \"One of Us\" handelte wiederum von den Folgen einer zerbrochenen Beziehung und wurde am 21. Oktober begonnen. Das Stück wurde später als erste Single aus dem Album ausgekoppelt und avancierte zum letzten internationalen Hit der Gruppe. Der Titelsong wurde ab 22. Oktober 1981 aufgenommen. Der Text von \"The Visitors\" handelt von Überwachung und Besuchen der Sicherheitspolizei bei Dissidenten in der Sowjetunion. Mitte November waren die Aufnahmen und Nachbearbeitungen von allen Songs abgeschlossen.\n\n\nAlbum-Cover.\nDas Album-Cover wurde von Rune Söderqvist zusammen mit dem Fotografen Lars Larsson entworfen. Söderqvist stellte die Gruppe im kalten, ungeheizten Atelier des Malers Julius Kronberg im Freilichtmuseum Skansen, Stockholm, in einer düsteren Stimmung dar. Im Hintergrund hing ein großes Gemälde von Julius Kronberg, das den griechischen Gott Eros darstellt. Das Cover zeigte die Gruppe zum ersten Mal nicht mehr als Gemeinschaft, sondern jedes Mitglied für sich allein. Diese düstere Stimmung innerhalb der Gruppe spiegelte sich sowohl in den Melodien als auch in den Texten vieler Songs wider. Das Cover ist laut \"The Making of The Visitors\" auch das Resultat einer gewissen Erschöpfung aufgrund des jahrelangen Zusammenseins als Gruppe.\n\nTitelliste.\n\nOriginal-LP.\nSeite 1:\n\nSeite 2:\n\n\n\n\n\n\n"}
{"id": "93", "url": "https://de.wikipedia.org/wiki?curid=93", "title": "Aluminium", "text": "Aluminium\n\nAluminium (im angloamerikanischen Sprachraum vielfach auch Aluminum) ist ein chemisches Element mit dem Elementsymbol Al und der Ordnungszahl 13. Im Periodensystem gehört Aluminium zur dritten Hauptgruppe und zur 13.&nbsp;IUPAC-Gruppe, der Borgruppe, die früher als \"Gruppe der Erdmetalle\" bezeichnet wurde. Es gibt zahlreiche Aluminiumverbindungen.\n\nAluminium ist ein silbrig-weißes Leichtmetall. In der Erdhülle ist es, nach Sauerstoff und Silicium, das dritthäufigste Element und in der Erdkruste das häufigste Metall.\n\nIn der Werkstofftechnik werden mit „Aluminium“ alle Werkstoffe auf Basis des Elementes Aluminium verstanden. Dazu zählt Reinaluminium (mindestens 99,0 % Al), Reinstaluminium (min 99,7 % Al) und insbesondere die Aluminiumlegierungen, die bis zu mit Stahl vergleichbare Festigkeiten besitzen – bei nur einem Drittel seiner Dichte.\n\nEntdeckt wurde Aluminium, das in der Natur fast ausschließlich in Form von chemischen Verbindungen vorkommt, im frühen 19.&nbsp;Jahrhundert. Im frühen 20.&nbsp;Jahrhundert setzte die industrielle Massenproduktion ein.\n\nDie Gewinnung erfolgt in Aluminiumhütten ausgehend von dem Mineral Bauxit zunächst im Bayer-Verfahren, mit dem Aluminiumoxid gewonnen wird, und anschließend im Hall-Héroult-Prozess einer Schmelzflusselektrolyse, bei der Aluminium gewonnen wird. 2016 wurden weltweit 115 Mio. Tonnen Aluminiumoxid (AlO) produziert. Daraus hat man 54,6&nbsp;Mio. Tonnen Primäraluminium gewonnen.\n\nDas Metall ist sehr unedel und reagiert an frisch angeschnittenen Stellen bei Raumtemperatur mit Luft und Wasser zu Aluminiumoxid. Dies bildet aber sofort eine dünne, für Luft und Wasser undurchlässige Schicht (Passivierung) und schützt so das Aluminium vor Korrosion. Reines Aluminium weist eine geringe Festigkeit auf; bei Legierungen ist sie deutlich höher. Die elektrische und thermische Leitfähigkeit ist hoch, weshalb Aluminium für leichte Kabel und Wärmetauscher verwendet wird.\n\nEines der bekanntesten Produkte ist Alufolie. Weitere sind Bauteile in Fahrzeugen und Maschinen, elektrische Leitungen, Rohre, Dosen und Haushaltsgegenstände. Das Aluminiumrecycling erreicht weltweit Raten von etwa 40 %.\n\n\nGeschichte.\n1782 vermutete Lavoisier als erster, dass es sich bei der 1754 von Marggraf aus einer Alaunlösung gewonnenen Alaunerde (\"alumina\", abgeleitet von lateinisch ‚Alaun‘) um das Oxid eines bislang unbekannten Elements handle. Dessen Darstellung glückte schließlich 1825 dem Dänen Hans Christian Ørsted durch Reaktion von Aluminiumchlorid (AlCl) mit Kaliumamalgam, wobei Kalium als Reduktionsmittel diente:\n\nDavy, der sich lange Zeit ebenfalls an der Darstellung des neuen Elements versucht hatte, führte ab 1807 die Namensvarianten \"alumium\", \"aluminum\" und \"aluminium\" ein, von welchen die letzten beiden im Englischen nebeneinander fortbestehen.\n\n1827 gelang es Friedrich Wöhler mit der gleichen Methode wie Ørsted, jedoch unter Verwendung metallischen Kaliums als Reduktionsmittel, reineres Aluminium zu gewinnen. Henri Étienne Sainte-Claire Deville verfeinerte den Wöhler-Prozess im Jahr 1846 und publizierte ihn 1859 in einem Buch. Durch diesen verbesserten Prozess stieg die Ausbeute bei der Aluminiumgewinnung, und in der Folge fiel der Preis des Aluminiums, der zuvor höher als jener von Gold gewesen war, innerhalb von zehn Jahren auf ein Zehntel.\n\n1886 wurde unabhängig voneinander durch Charles Martin Hall und Paul Héroult das nach ihnen benannte Elektrolyseverfahren zur Herstellung von Aluminium entwickelt: der Hall-Héroult-Prozess. 1889 entwickelte Carl Josef Bayer das nach ihm benannte Bayer-Verfahren zur Isolierung von reinem Aluminiumoxid aus Bauxiten. Aluminium wird noch heute nach diesem Prinzip großtechnisch hergestellt.\n\nAm Ende des 19.&nbsp;Jahrhunderts stand das Metall in solchem Ansehen, dass man daraus gefertigte Metallschiffe auf den Namen Aluminia taufte.\n\n\nVorkommen.\nAluminium ist mit einem Anteil von 7,57 Gewichtsprozent nach Sauerstoff und Silicium das dritthäufigste Element der Erdkruste und damit das häufigste Metall. Allerdings kommt es aufgrund seines unedlen Charakters praktisch ausschließlich in gebundener Form vor. Die größte Menge befindet sich chemisch gebunden in Form von Alumosilicaten, in denen es in der Kristallstruktur die Position von Silicium in Sauerstoff-Tetraedern einnimmt. Diese Silicate sind zum Beispiel Bestandteil von Ton, Gneis und Granit.\n\nSeltener wird Aluminiumoxid in Form des Minerals Korund und seiner Varietäten Rubin (rot) und Saphir (farblos, verschiedenfarbig) gefunden. Die Farben dieser Kristalle beruhen auf Beimengungen anderer Metalloxide. Korund hat mit fast 53 Prozent den höchsten Aluminiumanteil einer Verbindung. Einen ähnlich hohen Aluminiumanteil haben die noch selteneren Minerale Akdalait (etwa 51 Prozent) und Diaoyudaoit (etwa 50 Prozent). Insgesamt sind bisher (Stand: 2017) 1156 aluminiumhaltige Minerale bekannt.\n\nDas einzige wirtschaftlich wichtige Ausgangsmaterial für die Aluminiumproduktion ist Bauxit. Vorkommen befinden sich in Südfrankreich (Les Baux), Guinea, Bosnien und Herzegowina, Ungarn, Russland, Indien, Jamaika, Australien, Brasilien und den Vereinigten Staaten. Bauxit enthält ungefähr 60 Prozent Aluminiumhydroxid (Al(OH) und AlO(OH)), etwa 30 Prozent Eisenoxid (FeO) und Siliciumdioxid (SiO).\n\nBei der Herstellung unterscheidet man \"Primäraluminium\", auch \"Hüttenaluminium\" genannt, das aus Bauxit gewonnen wird, und \"Sekundäraluminium\" aus Aluminiumschrott. Die Wiederverwertung benötigt nur etwa 5 Prozent der Energie der Primärgewinnung.\n\n\nAluminium als Mineral.\nInfolge der Passivierung kommt Aluminium in der Natur sehr selten auch elementar (gediegen) vor. Erstmals entdeckt wurde Aluminium 1978 durch B. V. Oleinikov, A. V. Okrugin, N. V. Leskova in Mineralproben aus der Billeekh Intrusion und dem Dyke \"OB-255\" in der Republik Sacha \"(Jakutien)\" im russischen Föderationskreis Ferner Osten. Insgesamt sind weltweit bisher rund 20 Fundorte (Stand 2019) für gediegen Aluminium bekannt, so unter anderem in Aserbaidschan, Bulgarien, der Volksrepublik China (Guangdong, Guizhou, Jiangsu und Tibet) und in Venezuela. Zudem konnte gediegen Aluminium in Gesteinsproben vom Mond, das die Sonde der Luna-20-Mission vom Krater Apollonius mitbrachte, nachgewiesen werden.\n\nAufgrund der extremen Seltenheit hat gediegen Aluminium zwar keine Bedeutung als Rohstoffquelle, als gediegen vorkommendes Element ist Aluminium dennoch von der International Mineralogical Association (IMA) als eigenständiges Mineral anerkannt (Interne Eingangs-Nr. der IMA: \"1980-085a\"). Gemäß der Systematik der Minerale nach Strunz (9.&nbsp;Auflage) wird Aluminium unter der System-Nummer \"1.AA.05\" (Elemente – Metalle und intermetallische Verbindungen – Kupfer-Cupalit-Familie – Kupfergruppe) eingeordnet. In der veralteten 8. Auflage der Strunz’schen Mineralsystematik ist Aluminium dagegen noch nicht aufgeführt. Nur im zuletzt 2018 aktualisierten „Lapis-Mineralienverzeichnis“, das sich aus Rücksicht auf private Sammler und institutionelle Sammlungen noch an dieser Form der System-Nummerierung orientiert, erhielt das Mineral die System- und Mineral-Nr. \"I/A.3-05\". Die vorwiegend im englischsprachigen Raum verwendete führt das Element-Mineral unter der System-Nr. 01.01.01.05.\n\nIn der Natur kommt gediegen Aluminium meist in Form körniger Mineral-Aggregate und Mikronuggets vor, kann in seltenen Fällen aber auch tafelige Kristalle bis etwa einen Millimeter Größe entwickeln. Frische Mineralproben sind von metallisch glänzender, silberweißer Farbe. An der Luft dunkeln die Oberflächen durch Oxidierung nach und wirken grau. Auf der Strichtafel hinterlässt Aluminium einen dunkelgrauen Strich.\n\nJe nach Fundort enthält Aluminium oft Fremdbeimengungen von anderen Metallen (Cu, Zn, Sn, Pb, Cd, Fe, Sb) oder tritt eingewachsen in beziehungsweise mikrokristallin verwachsen mit Hämatit, Ilmenit, Magnetit, Moissanit und Pyrit beziehungsweise Jarosit auf.\n\nTypmaterial, das heißt Mineralproben aus der Typlokalität des Minerals, wird im Geologischen Museum der Akademie der Wissenschaften in Jakutsk in der russischen Teilrepublik Sacha (Jakutien) aufbewahrt.\n\n\nGewinnung.\n\nPrimäraluminium (Herstellung aus Mineralien).\nCa. 2/3 des europäischen Aluminiumbedarfs wird durch Primäraluminium gedeckt. Primäraluminium wird elektrolytisch aus einer Aluminiumoxidschmelze hergestellt. Da diese aus den auf der Erde allgegenwärtigen Alumosilicaten nur schwer isoliert werden kann, erfolgt die großtechnische Gewinnung aus dem relativ seltenen, silikatärmeren Bauxit. Zur Gewinnung von reinem Aluminiumoxid aus Silikaten gibt es seit langem Vorschläge, deren Anwendung allerdings nicht wirtschaftlich möglich ist.\n\nDas im Erz enthaltene Aluminiumoxid/-hydroxid-Gemisch wird zunächst mit Natronlauge aufgeschlossen (Bayer-Verfahren, Rohrreaktor- oder Autoklaven-Aufschluss), um es von Fremdbestandteilen wie Eisen- und Siliciumoxid zu befreien, und wird dann überwiegend in Wirbelschichtanlagen (aber auch in Drehrohröfen) zu Aluminiumoxid (AlO) gebrannt.\n\nDer trockene Aufschluss (Deville-Verfahren) hat dagegen keine Bedeutung mehr. Dabei wurde feinstgemahlenes, ungereinigtes Bauxit zusammen mit Soda und Koks in Drehrohröfen bei rund 1200&nbsp;°C kalziniert und das entstehende Natriumaluminat anschließend mit Natronlauge gelöst.\n\nDie Herstellung des Metalls erfolgt in Aluminiumhütten durch Schmelzflusselektrolyse von Aluminiumoxid nach dem Kryolith-Tonerde-Verfahren (Hall-Héroult-Prozess). Zur Herabsetzung des Schmelzpunktes wird das Aluminiumoxid zusammen mit Kryolith geschmolzen (Eutektikum bei 963&nbsp;°C). Bei der Elektrolyse entsteht an der den Boden des Gefäßes bildenden Kathode Aluminium und an der Anode Sauerstoff, der mit dem Graphit (Kohlenstoff) der Anode zu Kohlenstoffdioxid und Kohlenstoffmonoxid reagiert. Die Graphitblöcke, welche die Anode bilden, brennen so langsam ab und werden von Zeit zu Zeit ersetzt. Die Graphitkathode (Gefäßboden) ist gegenüber Aluminium inert. Das sich am Boden sammelnde flüssige Aluminium wird mit einem Saugrohr abgesaugt.\n\nAufgrund der hohen Bindungsenergie durch die Dreiwertigkeit des Aluminiums und der geringen Atommasse ist der Prozess recht energieaufwendig. Pro produziertem Kilogramm Rohaluminium müssen 12,9&nbsp;bis 17,7&nbsp;Kilowattstunden an elektrischer Energie eingesetzt werden. Eine Reduzierung des Strombedarfs ist nur noch in geringem Ausmaß möglich, weil die Potentiale für energetische Optimierungen weitgehend erschlossen sind. Aluminiumherstellung ist daher nur wirtschaftlich, wenn billige Elektroenergie zur Verfügung steht, beispielsweise neben Wasserkraftwerken, wie in Rheinfelden oder (ehemals) in Ranshofen unweit des Inns.\n\nDie nachfolgende Tabelle zeigt die Aluminiumproduktion und die maximal mögliche Produktionsleistung der Hüttenwerke nach Ländern.\n\n\nSekundäraluminium (Herstellung durch Aluminium-Recycling).\nUm Aluminium zu recyceln, werden Aluminiumschrotte und „Krätzen“ in Trommelöfen eingeschmolzen. „Krätze“ ist ein Abfallprodukt bei der Verarbeitung von Aluminium und bei der Herstellung von Sekundäraluminium. Krätze ist ein Gemisch aus Aluminiummetall und feinkörnigen Oxidpartikeln und wird beim Schmelzen von Aluminium bei 800&nbsp;°C aus dem Aluminiumoxid der normalen Aluminiumkorrosion und als Oxidationsprodukt (Oxidhaut) beim Kontakt von flüssigem Aluminium mit Luftsauerstoff gebildet. Damit beim Aluminiumgießen keine Aluminiumoxidpartikel in das Gussteil gelangen, wird die Krätze durch \"Kratz\"vorrichtungen von der Oberfläche des Metallbads abgezogen.\n\nUm die Bildung von Krätze zu verhindern, wird die Oberfläche der Schmelze mit Halogenidsalzen (rund zwei Drittel NaCl, ein Drittel KCl und geringe Mengen Calciumfluorid CaF) abgedeckt (siehe dazu Aluminiumrecycling). Dabei entsteht als Nebenprodukt Salzschlacke, die noch ca. 10 Prozent Aluminium enthält, die, entsprechend aufbereitet, als Rohstoff für mineralische Glasfasern dient.\n\nAllerdings wird an der Herstellung von Sekundäraluminium kritisiert, dass beim Recycling pro Tonne jeweils 300 bis 500 Kilogramm Salzschlacke, verunreinigt mit Dioxinen und Metallen, entstehen; deren mögliche Wiederverwertung ist aber Stand der Technik.\n\n\nEigenschaften.\n\nPhysikalische Eigenschaften.\n\nMikrostruktur.\nAluminium erstarrt ausschließlich in einem kubisch flächenzentrierten Raumgitter in der . Der Gitterparameter beträgt bei Reinaluminium 0,4049&nbsp;nm (entspricht 4,05&nbsp;Å) bei 4 Formeleinheiten pro Elementarzelle.\n\nLeerstellen kommen mit einer Dichte von 1,3 × 10 bei 500&nbsp;°C vor, bei Raumtemperatur sind es nur noch 10. Durch Abschrecken können größere Leerstellendichten bei Raumtemperatur vorkommen, was für einige Eigenschaften von Aluminiumwerkstoffen von Bedeutung ist, da die Leerstellen die Diffusion begünstigen. Durch Umformen bei Raumtemperatur kann die Leerstellendichte auf 10 erhöht werden. Die Versetzungs&shy;dichte liegt bei 10, einem für Metalle typischen Bereich, und führt zur guten Umformbarkeit von Aluminium. Stapelfehler konnten bei Aluminium nicht nachgewiesen werden, was mit der hohen Stapelfehlerenergie von 103 bis 200 (10 J/cm²) erklärt wird. Dies führt dazu, dass die Festigkeitsteigerung beim Kaltwalzen und -schmieden nur gering ausfällt und manche Aluminiumwerkstoffe sogar anschließend zur Entfestigung neigen.\n\n\nDichte.\nMit einer Dichte von 2,6989&nbsp;g/cm³ (etwa ein Drittel von Stahl) ist Aluminium ein typisches Leichtmetall, was es als Werkstoff für den Leichtbau interessant macht. Die Dichte der Legierungen weicht meist nur um etwa +3 % bis −2 % ab. Spezielle Legierungen mit Lithium haben eine 15 % geringere Dichte. Aluminium zählt somit zu den leichtesten Werkstoffen, übertroffen nur noch von Magnesium.\n\n\nMechanische Eigenschaften.\nAluminium ist ein relativ weiches und zähes Metall. Die Zugfestigkeit von absolut reinem Aluminium liegt bei 45&nbsp;N/mm², die Streckgrenze bei 17&nbsp;N/mm² und die Bruchdehnung bei 60 %, während bei handelsüblich reinem Aluminium die Zugfestigkeit bei 90&nbsp;N/mm² liegt, die Streckgrenze bei 34&nbsp;N/mm² und die Bruchdehnung bei 45 %. Die Zugfestigkeit seiner Legierungen liegt dagegen bei bis zu 710&nbsp;N/mm² (Legierung 7068). Sein Elastizitätsmodul liegt bei etwa 70&nbsp;GPa, einem häufig angegebenen Wert. Für Reinaluminium wird ein Wert von 66,6 GPa angegeben, die Werte schwanken jedoch von 60 bis 78 GPa. Der G-Modul liegt bei 25,0 kN/mm², die Querkontraktionszahl (Poissonzahl) bei 0,35.\n\n\nThermische Eigenschaften.\nDie Schmelztemperatur liegt bei 660,2&nbsp;°C und die Siedetemperatur bei 2470&nbsp;°C. Die Schmelztemperatur ist deutlich niedriger als die von Kupfer (1084,6&nbsp;°C), Gusseisen (1147&nbsp;°C) und Eisen (1538&nbsp;°C), was Aluminium zu einem guten Gusswerkstoff macht.\n\nBei einer Sprungtemperatur von 1,2&nbsp;K wird reines Aluminium supraleitend.\n\nDie Wärmeleitfähigkeit liegt mit 235 W/(K m) relativ hoch. Die Wärmeleitfähigkeit von Kupfer liegt zwar etwa doppelt so hoch, dafür ist die Dichte etwa viermal größer, weshalb Aluminium für Wärmetauscher in Fahrzeugen genutzt wird. Der Wärmeausdehnungskoeffizient ist durch den recht niedrigen Schmelzpunkt mit 23,1&nbsp;µm·m·K recht hoch.\n\nDie Schwindung, also die Volumenabnahme beim Erstarren liegt bei 7,1 %.\n\n\nElektrische Eigenschaften.\nDa thermische und elektrische Leitfähigkeit bei Metallen von denselben Mechanismen dominiert werden, ist Aluminium mit formula_2 auch ein sehr guter elektrischer Leiter. In der Rangfolge der Elemente mit der größten spezifischen Leitfähigkeit steht Aluminium wie auch bei der Wärmeleitfähigkeit hinter Silber, Kupfer und Gold an vierter Stelle. Durch die Kombination von hohem spezifischem Leitwert, geringer Dichte, hoher Verfügbarkeit und (im Vergleich zu anderen Materialien) geringen Kosten ist Aluminium in der Elektrotechnik – speziell in der Energietechnik, wo große Leiterquerschnitte benötigt werden – neben Kupfer zum wichtigsten Leitermaterial geworden.\n\n\nMagnetische Eigenschaften.\nAluminium ist paramagnetisch, wird also von Magneten angezogen, der Effekt ist jedoch sehr schwach ausgeprägt. Die Magnetische Suszeptibilität liegt bei Raumtemperatur bei 0,62 × 10 m³/kg, womit Aluminium praktisch gesehen unmagnetisch ist.\n\n\nChemische Eigenschaften.\nDas reine Leichtmetall Aluminium hat aufgrund einer sich sehr schnell an der Luft bildenden dünnen Oxidschicht ein stumpfes, silbergraues Aussehen. Diese passivierende Oxidschicht macht reines Aluminium bei pH-Werten von 4 bis 9 sehr korrosionsbeständig, sie erreicht eine Dicke von etwa 0,05&nbsp;µm.\n\nDiese Oxidschicht schützt vor weiterer Oxidation, ist aber bei der elektrischen Kontaktierung und beim Löten hinderlich. Sie kann durch elektrische Oxidation (Eloxieren) oder auf chemischem Weg verstärkt werden.\n\nDie Oxidschicht kann mittels Komplexbildungsreaktionen aufgelöst werden. Einen außerordentlich stabilen und wasserlöslichen Neutralkomplex geht Aluminium in neutraler chloridischer Lösung ein. Folgende Reaktionsgleichung veranschaulicht den Vorgang:\n\nDies geschieht vorzugsweise an Stellen, wo die Oxidschicht des Aluminiums bereits geschädigt ist. Es kommt dort durch Bildung von Löchern zur Lochfraßkorrosion. Kann die chloridische Lösung dann an die freie Metalloberfläche treten, so laufen andere Reaktionen ab. Aluminium-Atome können unter Komplexierung oxidiert werden:\n\nLiegen in der Lösung Ionen edlerer Metalle vor, so werden sie reduziert und am Aluminium abgeschieden. Auf diesem Prinzip beruht die Reduktion von Silberionen, die auf der Oberfläche von angelaufenem Silber als Silbersulfid vorliegen, hin zu Silber.\n\nAluminium reagiert heftig mit wässriger Natriumhydroxidlösung (NaOH) (und etwas weniger heftig mit wässriger Natriumcarbonatlösung) unter Bildung von Wasserstoff. Diese Reaktion wird in chemischen Rohrreinigungsmitteln ausgenutzt.\nDie Reaktion von Aluminium mit NaOH läuft in zwei Schritten ab: der Reaktion mit Wasser und die Komplexierung des Hydroxids zu Natriumaluminat.\n\nBei der Reaktion mit Wasser\nentsteht zunächst Aluminiumhydroxid.\n\nIn der Regel wird anschließend die Oberfläche getrocknet, dabei wird das Hydroxid in das Oxid umgewandelt:\n\nDies passiert jedoch nicht bei der Reaktion von Aluminium in wässriger Natronlauge.\n\nNun folgt der 2. Schritt, die Komplexierung des Hydroxids zu Natriumaluminat:\nDurch die Komplexierung wird das gallertartige Hydroxid wasserlöslich und kann von der Metalloberfläche abtransportiert werden. Dadurch ist die Aluminiumoberfläche nicht mehr vor dem weiteren Angriff des Wassers geschützt und Schritt 1 läuft wieder ab.\n\nMit dieser Methode lassen sich – ebenso wie bei der Reaktion von Aluminium mit Säuren – pro zwei Mol Aluminium drei Mol Wasserstoffgas herstellen.\n\nMit Brom reagiert Aluminium bei Zimmertemperatur unter Flammenerscheinung. Hierbei ist zu beachten, dass das entstehende Aluminiumbromid mit Wasser unter Bildung von Aluminiumhydroxid und Bromwasserstoffsäure reagiert.\n\nMit Quecksilber bildet Aluminium ein Amalgam. Wenn Quecksilber direkt mit Aluminium zusammenkommt, d.&nbsp;h., wenn die Aluminiumoxidschicht an dieser Stelle mechanisch zerstört wird, frisst Quecksilber Löcher in das Aluminium; unter Wasser wächst dann darüber Aluminiumoxid in Gestalt eines kleinen Blumenkohls. Daher wird Quecksilber in der Luftfahrt als Gefahrgut und „ätzende Flüssigkeit“ gegenüber Aluminiumwerkstoffen eingestuft.\n\nMit Salzsäure reagiert Aluminium sehr heftig unter Wasserstoffentwicklung, von Schwefelsäure wird es langsam aufgelöst. In Salpetersäure wird es passiviert.\n\nIn Pulverform (Partikelgröße kleiner 500&nbsp;µm) ist Aluminium vor allem dann, wenn es nicht phlegmatisiert ist, aufgrund seiner großen Oberfläche sehr reaktiv. Aluminium reagiert dann mit Wasser unter Abgabe von Wasserstoff zu Aluminiumhydroxid. Feinstes, nicht phlegmatisiertes Aluminiumpulver wird auch als Pyroschliff bezeichnet. Nicht phlegmatisierter Aluminiumstaub ist sehr gefährlich und entzündet sich bei Luftkontakt explosionsartig von selbst.\n\n\nIsotope.\nIn der Natur kommt ausschließlich das Isotop Al vor; Aluminium gehört damit zu den Reinelementen. Dieses Isotop, das stabil ist und im Kern 14 Neutronen und 13 Protonen enthält, absorbiert keine Neutronen, weshalb Aluminium in Kernreaktoren genutzt wird. Alle anderen Isotope werden künstlich erzeugt und sind radioaktiv. Das stabilste dieser Isotope ist Al mit einer Halbwertszeit von einer Million Jahren. Durch Elektroneneinfang oder Beta-Zerfall entsteht daraus Mg, durch Einfangen eines Neutrons und anschließenden Gamma-Zerfall Al. Die Isotope Al bis Al (Außer Al und Al) haben Halbwertszeiten zwischen wenigen Sekunden und einigen hundert Sekunden. Al zerfällt mit einer Halbwertszeit von nur 0,13 Sekunden.\n\n\nAluminiumlegierungen.\nAluminiumlegierungen sind Legierungen, die überwiegend aus Aluminium bestehen. Für andere Legierungen, die Aluminium enthalten, siehe Abschnitt #Weitere Anwendungen.\n\nAluminium kann mit zahlreichen Metallen legiert werden, um bestimmte Eigenschaften zu fördern oder andere, ungewünschte Eigenschaften zu unterdrücken. Bei einigen Legierungen ist die Bildung der schützenden Oxidschicht (Passivierung) stark gestört, wodurch die daraus gefertigten Bauteile teils korrosionsgefährdet sind. Nahezu alle hochfesten Aluminiumlegierungen sind von dem Problem betroffen.\n\nEs gibt Aluminiumknetlegierungen, die zur Weiterverarbeitung durch Walzen, Schmieden und Strangpressen gedacht sind und Gusswerkstoffe. Diese werden in Gießereien verwendet.\n\nIm Allgemeinen werden Aluminiumlegierungen in die zwei große Gruppen der Knet- und Gusslegierungen eingeteilt:\n\nAußerdem wird unterschieden zwischen naturharten Legierungen – welche sich durch eine Wärmebehandlung nicht härten lassen – und aushärtbaren:\n\n\nWirtschaftliche Bedeutung.\nAluminium ist nach Stahl der zweitwichtigste metallische Werkstoff. 2016 wurden weltweit 115 Mio. Tonnen produziert.\n\nDer Aluminiumpreis bewegte sich am Weltmarkt seit 1980 um den Wert von 2000 Dollar pro Tonne (Reinheit von 99,7 %). Er ist jedoch relativ volatil, 2016 fiel er auf um die 1500 Dollar pro Tonne, während er 2017 wieder bei annähernd 2000 Dollar lag.\n\nIm August 2020 verhängten die USA 10 % Einfuhrzoll auf Aluminium aus Kanada um die inländische Produktion zu schützen.\n\n\nVerwendung.\n\nKonstruktionswerkstoff allgemein.\nAluminium weist eine hohe spezifische Festigkeit auf. Verglichen mit Stahl sind Bauteile aus Aluminium bei gleicher Festigkeit etwa halb so schwer, weisen jedoch ein größeres Volumen auf. Deshalb wird es gern im Leichtbau verwendet, also dort, wo es auf geringe Masse ankommt, die zum Beispiel bei Transportmitteln zum geringeren Treibstoffverbrauch beiträgt, vor allem in der Luft- und Raumfahrt. Im Kraftfahrzeugbau gewann es aus diesem Grund an Bedeutung; hier standen früher der hohe Materialpreis, die schlechtere Schweißbarkeit sowie die problematische Dauerbruchfestigkeit und die Verformungseigenschaften bei Unfällen (geringes Energieaufnahmevermögen in der sogenannten Knautschzone) im Wege. Die Haube des Washington-Denkmals, ein 3&nbsp;kg schweres Gussstück, galt bis 1884 als eines der größten Aluminiumwerkstücke. Beim Bau von kleinen und mittleren Schiffen und Booten wird die Korrosionsbeständigkeit von Aluminium gegenüber Salzwasser geschätzt. Der Fahrzeugbau (inklusive Schiffen, Flugzeugen und Schienenfahrzeugen) machte 2010 mit ca. 35 Prozent den größten Anteil an der weltweiten Verwendung von Aluminium aus.\n\nIn Aluminiumlegierungen werden Festigkeiten erreicht, die denen von Stahl nur wenig nachstehen. Daher ist die Verwendung von Aluminium zur Gewichtsreduzierung überall dort angebracht, wo Materialkosten eine untergeordnete Rolle spielen. Insbesondere im Flugzeugbau und in der Weltraumtechnik sind Aluminium und Duraluminium weit verbreitet. Der größte Teil der Struktur heutiger Verkehrsflugzeuge wird aus Aluminiumblechen verschiedener Stärken und Legierungen genietet.\n\n\nFahrzeugbau.\nBei Fahrzeugen spielt deren Masse eine Rolle: Je leichter ein Fahrzeug ist desto geringer ist der Treibstoffverbrauch. In Deutschland werden knapp 50 % des Aluminiums im Fahrzeugbau verwendet (Stand: 2015).\n\nBei Autos werden Aluminiumwerkstoffe verwendet für verschiedene Motor&shy;komponenten – darunter der Motorblock, die Zylinderkolben für die spezielle Kolbenlegierungen existieren, die Zylinderköpfe – wo vor allem die geringe Wärmeausdehnung und Korrosionsanfäligkeit sowie die hohe Warmfestigkeit ausschlaggebend sind; zusammen mit der guten Gießbarkeit, da diese Komponenten üblicherweise gegossen werden. Weitere Anwendungen bei Fahrzeugen sind für Gehäuse von Getrieben, als Wärmeabschirmung und als Wärmetauscher – bei den letzten beiden in Form von Reinaluminium. Im Fahrwerk wird Aluminium genutzt als Schmiedeteile für Hinterachsen, Achsträger, Querlenker und Räder. In der Karosserie wird Aluminium verwendet für Türen, Motorhauben, Stoßfänger und Kotflügel, sowie in der Rohwagenstruktur.\n\nBei Nutzfahrzeugen wird Aluminium angewandt für Bordwände, Ladebordwände, Aufbauten, zur Ladungssicherung, Druckluftbehälter, Treibstofftanks und als Unterbauschutz. Der Leichtbau mit Aluminium wird bei Nutzfahrzeugen stark durch die gesetzliche Maximallast pro Achse beeinflusst: Bei geringerem Fahrzeuggewicht ist eine höhere Nutzlast möglich.\n\nAuch bei Schienenfahrzeugen wird reichlich Aluminium verwendet. Voraussetzung waren dafür zwei wichtige andere Entwicklungen: Bestimmte Schweißverfahren die für Aluminiumwerkstoffe geeignet sind (WIG-Schweißen / MIG-Schweißen) in den 1950ern und das Strangpressen von Großprofilen. Die Verwendung von Aluminium hat die gesamte Bauweise von Schienenfahrzeugen verändert. Bis etwa 1970 waren Konstruktionen aus Stahlrohren üblich, danach vermehrt verschweißte Profile aus Aluminium.\n\nBereits in der Anfangsphase der Luftfahrt wurden Aluminiumwerkstoffe genutzt, 1903 beispielsweise Magnalium für die Beschläge eines Flugzeuges, das noch größtenteils aus Holz, Draht und Tuch bestand. Das erste flugfähige Ganzmetallflugzeug stammt aus dem Jahre 1915, bestand allerdings aus Stahlblechen in Schalenbauweise. Die entscheidende Entwicklung zur Verwendung von Aluminium im Flugzeugbau stammt von 1906 von Alfred Wilm, der mit dem Duraluminium eine aushärtbare Aluminium-Kupfer-Legierung fand, die sehr hohe Festigkeiten aufweist und sich daher ausgezeichnet für den Leichtbau eignet. Genutzt werden für Flugzeuge AlCu und AlZnMg. Die Gesamtmasse von Flugzeugen geht zu 60 % auf Aluminium zurück. Die Verbindung der aus Blechen gestanzten, geschnittenen oder getriebenen, aus dem Vollen gefrästen oder aus Profilen bestehenden Werkstücke erfolgt meist durch Nieten, da die meistverwendeten Werkstoffe schlecht schweißbar sind.\n\nBei Sport- und Alltagsrädern, hat Aluminium bei vielen Bauteilen Stahl abgelöst. Bei Felgen gab es im Rennsport auch Holzfelgen, bevor sich Alufelgen durchsetzten – diese sind griffiger für Bremsbacken, verschleißen jedoch dabei. Alurahmenrohre wurden zuerst – um 1970 – mit Epoxidkleber gefügt, später geschweisst. Bei Gepäcksträgern und Seitenständern kommt Alu als Draht, Guss und Rohr vor. Lenker, Vorbau, Bremsgriffe, Kurbeln und Ketten-Schaltwerk sind seit langem typisch aus Alu. Kotschützer sind häufig aus kunststoffumhülltem Alu.\n\n\nElektrotechnik.\n\nElektrische Leitungen.\nAluminium ist ein guter elektrischer Leiter. Es weist nach Silber, Kupfer und Gold die vierthöchste elektrische Leitfähigkeit aller Metalle auf. Ein Leiter aus Aluminium hat bei gegebenem elektrischen Widerstand eine kleinere Masse, aber ein größeres Volumen als ein Leiter aus Kupfer. Daher wird meistens dann Kupfer als elektrischer Leiter verwendet, wenn das Volumen eine dominante Rolle spielt, wie bei den Wicklungen in Transformatoren. Aluminium hat dann als elektrischer Leiter Vorteile, wenn das Gewicht eine wesentliche Rolle spielt, beispielsweise bei den Leiterseilen von Freileitungen. Aus dem Grund der Gewichtsreduktion werden in Flugzeugen wie dem Airbus A380 Aluminiumkabel verwendet.\n\nAluminium wird unter anderem zu Stromschienen in Umspannwerken und zu stromführenden Gussteilen verarbeitet. Für Elektroinstallationen gibt es kupferkaschierte Aluminiumkabel, der Kupferüberzug ist zur Verbesserung der Kontaktgabe. In diesen Anwendungsbereichen sind primär Rohstoffpreise entscheidend, da Aluminium preisgünstiger als Kupfer ist. Für Oberleitungen bei elektrischen Bahnen ist es dagegen aufgrund seiner schlechten Kontakt- und Gleiteigenschaften ungeeignet, in diesem Bereich wird trotz des höheren Gewichts primär Kupfer eingesetzt.\n\nBeim Kontaktieren unter Druck ist Aluminium problematisch, da es zum Kriechen neigt. Außerdem überzieht es sich an Luft mit einer Oxidschicht. Nach längerer Lagerung oder Kontakt mit Wasser ist diese isolierende Schicht so dick, dass sie vor der Kontaktierung beseitigt werden muss. Vor allem im Kontakt mit Kupfer kommt es zu Bimetallkorrosion. Bei ungeeigneten Kontaktierungen in Klemmen kann es bei Aluminiumleitern in Folge zu Ausfällen und Kabelbränden aufgrund sich lösender Kontakte kommen. Crimpverbindungen mit passenden Hülsen und Werkzeugen sind jedoch sicher. Als Zwischenlage zwischen Kupfer und Aluminium können Verbindungsstücke aus Cupal die Kontaktprobleme vermeiden.\n\nHervorzuheben ist das geringe Absinken der spezifischen elektrischen Leitfähigkeit von Aluminium bei Zusatz von Legierungsbestandteilen, wohingegen Kupfer bei Verunreinigungen eine deutliche Verringerung der Leitfähigkeit zeigt.\n\n\nElektronik.\nDie Elektronikindustrie setzt Aluminium aufgrund der guten Verarbeitbarkeit und der guten elektrischen und Wärme-Leitfähigkeit ein.\n\nIn integrierten Schaltkreisen wurde bis in die 2000er Jahre ausschließlich Aluminium als Leiterbahnmaterial eingesetzt. Bis in die 1980er Jahre wurde es als Material für die Steuerelektrode (Gate) von Feldeffekttransistoren mit Metall-Isolator-Halbleiter-Struktur (MOSFET beziehungsweise MOS-FET) verwendet. Neben dem geringen spezifischen Widerstand sind für die Verwendung die gute Haftung auf und geringe Diffusion in Siliciumoxiden (Isolationsmaterial zwischen den Leiterbahnen) sowie die einfache Strukturierbarkeit mithilfe von Trockenätzen ausschlaggebend. Seit Anfang der 2000er Jahre wird Aluminium jedoch zunehmend durch Kupfer als Leiterbahnmaterial ersetzt, auch wenn dafür aufwendigere Strukturierungsverfahren (vgl. Damascene- und Dual-Damascene-Prozess) und Diffusionsbarrieren notwendig sind. Der höheren Fertigungsaufwand wird durch den geringeren spezifischen Widerstand, der im Fall von kleinen Strukturen bei Aluminium viel früher signifikant ansteigt und anderen Eigenschaften (z.&nbsp;B. Elektromigrationverhalten) überwogen und die Aluminium-Prozesse konnte die gestiegenen Anforderungen (Taktfrequenz, Verlustleistung) in mit hohen Frequenzen arbeitenden Schaltkreisen nicht mehr genügen.\n\nAluminium wird jedoch weiterhin in mikroelektronischen Produkten verwendet, so wird es wegen seiner guten Kontaktierbarkeit durch andere Metalle in den letzten Leiterbahnebenen eingesetzt, um den elektrischen Kontakt zu den bei der Flip-Chip-Montage eingesetzten Lotkügelchen herzustellen. Ähnlich verhält es sich bei Leistungshalbleitern, bei denen in der Regel alle Leiterbahnebenen aus Aluminium bestehen. Allgemein und insbesondere bei Leistungshalbleitern wird das Material für Bonddrähte (Verbindungsdrähte zwischen Chip und Gehäuseanschluss) verwendet.\n\nMit der Einführung der High-k+Metal-Gate-Technik hat Aluminium nach gut 25 Jahren Abstinenz auch im Bereich des Gates an Bedeutung gewonnen und wird neben anderen als Material zur Einstellung der Austrittsarbeit eingesetzt.\n\n\nVerpackung und Behälter.\nIn der Verpackungsindustrie wird Aluminium zu Getränke- und Konservendosen sowie zu Aluminiumfolie verarbeitet. Dabei macht man sich die Eigenschaft der absoluten Barrierewirkung gegenüber Sauerstoff, Licht und anderen Umwelteinflüssen zunutze. Ausschlaggebend für die Verwendung von Aluminium als Verpackung ist nicht die geringe Dichte, sondern die gute Verarbeitbarkeit durch Walzen und die Ungiftigkeit. Dünne Folien werden in Stärken von sechs Mikrometern hergestellt und dann zumeist in Verbundsystemen eingesetzt, beispielsweise in Tetra Paks. Kunststofffolien können durch Bedampfen mit Aluminium mit einer dünnen Schicht versehen werden, welche dann eine hohe (aber nicht vollständige) Barrierefunktion aufweist. Grund dieser Barrierewirkung ist nicht das reine Aluminium, sondern die Passivschicht aus Böhmit. Wird diese verletzt, so kann Gas ungehindert durch den Werkstoff Aluminium strömen. Genutzt werden meist Reinaluminium, AlMn (Legierungen mit Mangan) und AlMg (Legierungen mit Magnesium).\n\nAus Aluminium werden Kochtöpfe und andere Küchengeräte, wie die klassische italienische Espressokanne, sowie Reise- und Militär-Geschirr hergestellt.\n\nAluminium wird für eine Vielzahl von Behältern und Gehäusen verarbeitet, da es sich gut durch Umformen bearbeiten lässt. Gegenstände aus Aluminium werden häufig durch eine Eloxalschicht vor Oxidation und Abrieb geschützt.\n\nDruckgasflaschen aus Aluminium sind eher selten, denn Taucher brauchen Ballast und im Bereich Feuerwehr wurden Leichtstahlflaschen durch solche aus Faserverbund ersetzt, zunehmend ohne Alu-Liner.\n\n2017 entfielen 17 % der europäischen Aluminiumverwendung auf Verpackungen.\n\n\nOptik und Lichttechnik.\nAluminium wird aufgrund seines hohen Reflexionsgrades als Spiegelbeschichtung von Oberflächenspiegeln, unter anderem in Scannern, Kraftfahrzeug-Scheinwerfern und Spiegelreflexkameras aber auch in der Infrarotmesstechnik eingesetzt. Es reflektiert im Gegensatz zu Silber Ultraviolettstrahlung. Aluminium-Spiegelschichten werden meist durch eine Schutzschicht vor Korrosion und Kratzern geschützt.\n\n\nArchitektur und Bauwesen.\n\nBetonherstellung.\nAluminiumpulver und Aluminiumpasten werden zur Herstellung von Porenbeton eingesetzt. Man verwendet Verbindungen wie Aluminiumhydroxysulfat, Aluminiumdihydroxyformiat oder amorphes Aluminiumhydroxid als alkalifreie Spritzbetonbeschleuniger.\n\n\nKonstruktions- und Funktionswerkstoffe.\nAluminium wird als Konstruktionswerkstoff für tragende Teile von Bauwerken und als Funktionswerkstoff als dekorative, korrisionsbeständige Teile verwendet. Neben der Witterungsbeständigkeit ist vor allem die gute Verarbeitbarkeit ausschlaggebend, insbesondere bei handwerklicher Fertigung. Das Baugewerbe ist der Hauptabnehmer für Aluminiumprofile. Genutzt wird Aluminium hauptsächlich für Fensterrahmen, Türen und Elemente von Fassaden. Besonders bekannt ist die Fassade des Imperial War Museums in Manchester. Genutzt werden vor allem die Aluminium-Mangan-Legierungen die geringe Festigkeit und gute Korrosionsbeständigkeit haben. Teilweise wird Aluminium für den Brückenbau angewandt, wo sonst der Stahlbau vorherrscht. Für den konstruktiven Ingenieurbau werden Legierungen mit höherer Festigkeit genutzt, darunter AlMg und AlSi.\nBleche und Verbundplatten aus Aluminiumlegierungen erreichen Brandschutzklassen von 'nicht brennbar' bis 'normal entflammbar'. Ein Wohnungsbrand entwickelt im Vollbrand 1000&nbsp;°C Hitze was ungeachtet der Brandschutzklasse Löcher in die Aluminiumlegierung brennt, die zwischen 600&nbsp;°C und 660&nbsp;°C nach unten fließt oder tropft.\n\nWeitere Anwendungen.\nIn der Raketentechnik besteht der Treibstoff von Feststoffraketen zu maximal 30 Prozent aus Aluminiumpulver, das bei seiner Verbrennung viel Energie freisetzt. Aluminium wird in Feuerwerken (s.&nbsp;a. Pyrotechnik) verwendet, wo es je nach Körnung und Mischung für farbige Effekte sorgt. Auch in Knallsätzen findet es oft Verwendung.\n\nBei der Aluminothermie wird Aluminium zur Gewinnung anderer Metalle und Halbmetalle verwendet, indem das Aluminium zur Reduktion der Oxide genutzt wird. Ein wichtiges Verfahren der Aluminothermie ist die Thermitreaktion, bei der Aluminium mit Eisen(III)-oxid umgesetzt wird. Bei dieser stark exothermen Reaktion entstehen Temperaturen bis zu 2500&nbsp;°C und flüssiges Eisen, das zum aluminothermischen Schweißen genutzt wird, z.&nbsp;B. zum Fügen von Bahngleisen. Weitere Anwendungen der Reduktionswirkung von Aluminium werden für Laborzwecke ermöglicht, indem Aluminiumamalgam verwendet wird.\n\nAluminium dient als Pigment für Farben (Silber- oder Goldbronze). Farbig eloxiert ist es Bestandteil vieler Dekorationsmaterialien wie Flitter, Geschenkbänder und Lametta. Zur Beschichtung von Oberflächen wird es beim Aluminieren verwendet.\n\nMit Aluminium werden Heizelemente von Bügeleisen und Kaffeemaschinen umpresst.\n\nBevor es gelang, Zinkblech durch Titanzusatz als so genanntes Titanzink verarbeitbar zu machen, wurde Aluminiumblech für Fassaden- und Dachelemente (siehe Leichtdach) sowie Dachrinnen eingesetzt.\n\nWegen seiner hohen Wärmeleitfähigkeit wird Aluminium als Werkstoff für stranggepresste Kühlkörper und wärmeableitende Grundplatten verwendet. Aluminium-Elektrolytkondensatoren verbauen Aluminium als Elektrodenmaterial und Gehäusewerkstoff, weiters wird es zur Herstellung von Antennen und Hohlleitern verwendet.\n\nAluminium kommt in einigen Legierungen vor. Neben den Aluminiumlegierungen die überwiegend aus Aluminium bestehen, kommt es noch vor in den Kupferlegierungen Aluminiumbronze, Aluminiummessing, Isabellin, zu etwa gleichen Teilen Al und Kupfer in der Devardaschen Legierung, als Hauptlegierungselement für Magnesiumlegierungen sowie in Alnico und Sendust, zwei Eisenlegierungen mit besonderen magnetischen Eigenschaften. In vielen Titanlegierungen kommt ebenfalls Aluminium vor, insbesondere in Ti-6Al-4V, der Sorte die etwa 50 % aller Titanlegierungen ausmacht. Dort ist Aluminium mit sechs Massenprozent enthalten.\n\n\nVerarbeitung.\nBei der Verarbeitung wird unterschieden, ob es sich um Gusslegierungen handelt oder um Knetlegierungen:\n\nDanach werden die Einzelteile durch Schweißen, Nieten, Löten und ähnliche Verfahren verbunden.\n\n\nGießen.\nDas Gießen von Aluminium wird als Aluminiumguss bezeichnet. Es gehört aufgrund seines vergleichsweise geringen Schmelzpunktes von 660&nbsp;°C (Gusseisen etwa 1150&nbsp;°C, Stahl 1400&nbsp;°C bis 1500&nbsp;°C) und seiner guten Gießbarkeit zu den häufig in der Gießerei verwendeten Werkstoffen. AlSi, spezielle Gusslegierungen mit Silicium, haben sogar Schmelzpunkte um 577&nbsp;°C. Der Massenanteil von Aluminium aller in Gießereien erzeugten Produkte beträgt etwa 11 % (Gusseisen 76 %, Stahlguss 9 %) und ist damit in der Gießerei das mit Abstand wichtigste Nichteisenmetall (NE-Metalle) noch vor Kupfer mit 1,5 %. Der Anteil am NE-Metallguss von Aluminium beträgt etwa 87 %. In Deutschland wurden 2011 etwa 840.000 Tonnen Aluminium in Gießereien verarbeitet; Etwa 76 % des Nichteisenmetall-Gusses wird von der Automobilbranche abgenommen.\n\nAus dem niedrigen Schmelzpunkt folgt ein geringer Energieeinsatz beim Schmelzvorgang sowie eine geringere Temperaturbelastung der Formen. Aluminium eignet sich grundsätzlich für alle Gussverfahren, insbesondere für Druckguss beziehungsweise Aluminiumdruckguss, mit denen kompliziert geformte Teile gefertigt werden können. In der Gießerei werden besondere Aluminiumgusslegierungen verarbeitet, größtenteils die Aluminium-Silicium-Legierungen. In den Hüttenwerken werden dagegen meist Knetlegierungen erzeugt, die zur Weiterbearbeitung durch Walzen, Schmieden und Fließpressen gedacht sind. Diese werden in den Hüttenwerken vergossen zu Barren (Blockguss) oder zu Rundbarren, die theoretisch endlos sein können (Strangguss). Seit den 1930er Jahren kommt der Strangguss vermehrt zum Einsatz. Dafür gibt es spezielle Anlagen die bis zu 96 Rundbarren gleichzeitig herstellen können mit Gießlängen zwischen 3 und 7 Metern teils bis zu 10 Metern. Die Durchmesser liegen bei 75 bis 700&nbsp;mm. Bleche werden manchmal hergestellt durch Gießen direkt auf eine Walze, die die Schmelze kühlt. Das Rohblech wird danach direkt kaltgewalzt ohne Warmwalzen, was Kosten von bis zu 60 % spart.\n\n\nUmformende Verfahren.\nEtwa 74 Prozent des Aluminiums wird durch Umformen bearbeitet. Hierzu zählt unter anderem das Walzen, Schmieden, Strangpressen und Biegen.\n\nRein- und Reinstaluminium lässt sich wegen der niedrigen Festigkeit gut umformen und verfestigt sich bei Kaltumformung, wobei große Formänderungen möglich sind. Die Verfestigung lässt sich durch Rekristallisationsglühen beseitigen. Knetlegierungen mit AlMg und AlMn erreichen ihre höhere Festigkeit durch die Legierungselemente und durch Kaltverformung. Die aushärtbaren Legierungen AlMgSi, AlZnMg, AlCuMg und AlZnMgCu scheiden bei Umformung festigkeitssteigernde Phasen aus; sie lassen sich relativ schwierig umformen.\n\nGegossene Barren werden häufig durch Walzen weiterverarbeitet, entweder zu dicken Platten die anschließend durch Fräsen zu Endprodukten werden, zu Blechen die durch Stanzen und Biegen weiterverarbeitet werden oder zu Folien. Beim Walzen ändert sich die Mikrostruktur der Werkstoffe: Kleine kugelförmige Bestandteile die häufig nach dem Gießen vorliegen, werden plattgedrückt und in die Länge gezogen. Das Gefüge wird dadurch einerseits feiner und gleichmäßiger, andererseits aber auch Richtungsabhängig. Die Kapazität einer Aluminium-Warmwalzanlage liegt bei etwa 800.000 Tonnen pro Jahr. Verarbeitet werden Barren mit bis zu 30 Tonnen Masse. Sie haben Abmessungen von bis zu 8,7 Metern Länge, 2,2 Metern Breite und 60&nbsp;cm Dicke. Noch größere Barren können technisch verarbeitet werden, die Gefügequalität nimmt dann aber ab. Nach dem Warmwalzen liegt der Werkstoff meist mit Dicken von etwa 20 bis 30&nbsp;mm vor. Anschließend folgt das Kaltwalzen auf Enddicke. Kaltwalzwerke haben Kapazitäten von 300.000 bis 400.000 Jahrestonnen. Verbundwerkstoffe können durch Walzplattieren hergestellt werden. Dabei wird ein- oder zweiseitig eine Schicht aus einem anderen Werkstoff aufgebracht. Häufig wird auf korrosionsanfälliges Kernmaterial eine Schicht aus korrosionsbeständigem Reinaluminium aufgebracht.\n\nAluminium lässt sich durch Strangpressen in komplizierte Konstruktionsprofile formen; hierin liegt ein großer Vorteil bei der Fertigung von Hohlprofilen (z.&nbsp;B. für Fensterrahmen, Stäbe, Balken), Kühlkörper&shy;profilen oder in der Antennentechnik. Die Herstellung von Halbzeug oder Bauteilen geschieht aus Vormaterial wie Walzbarren, Blech oder Zylindern. Aluminiumlegierungen lassen sich deutlich besser strangpressen als andere Werkstoffe, weshalb ein großer Teil des Aluminiums mit diesem Verfahren verarbeitet wird. Dabei wird das Ausgangsmaterial durch ein hohles Werkzeug gepresst. Es entsteht Endlosmaterial das in der gewünschten Länge abgesägt wird. Es können auch komplizierte Querschnitte hergestellt werden, beispielsweise Hohlprofile oder welche mit Hinterschneidungen. Der Querschnitt ist allerdings über die Länge konstant. Mit hochfesten Legierungen sind große Mindestwanddicken erforderlich und das Pressen dauert lange, weshalb eher die mittelfesten, aushärtbaren Legierungen bevorzugt werden. Die Aushärtung wird meist direkt im Anschluss durchgeführt. Beim Strangpressen wird der Werkstoff auf Temperaturen von etwa 450 bis 500&nbsp;°C erwärmt um die Umformbarkeit zu erhöhen, was gleichzeitig zum Lösungsglühen genutzt wird. Direkt nach dem Strangpressen wird das Werkstück durch Luft oder Wasser stark abgekühlt und so abgeschreckt, was zu höheren Festigkeiten führt.\n\nEin Mischverfahren aus Gießen und Schmieden ist Cobapress, welches speziell für Aluminium ausgelegt ist und häufig in der Automobilbranche genutzt wird. Moderne Walzwerke sind sehr teuer, aber auch produktiv.\n\n\nSpanende Verfahren.\nZum Zerspanen zählt das Drehen, Bohren und Fräsen. Aluminiumwerkstoffe sind gut spanbar. Ihre genauen Eigenschaften hängen jedoch von der Legierung und Gefügezustand ab. Zu beachten ist, dass die bei der Bearbeitung auftretenden Temperaturen schnell im Bereich des Schmelzpunktes liegen können. Bei gleichen Schnittparametern wie bei Stahl resultiert bei Aluminium allerdings eine geringere mechanische und thermische Belastung. Als Schneidstoff wird oft Hartmetall für untereutektische oder Diamant für die stark verschleißenden übereutektischen Legierungen verwendet. Insbesondere die Bearbeitung von eloxierten Werkstücken erfordert harte Werkzeuge, um Verschleiß durch die harte Eloxalschicht zu vermeiden. Die beim Schleifen von Aluminium entstehenden Schleifstäube können zu einem erhöhten Explosionsrisiko führen.\n\n\nSchweißen und Löten.\nGrundsätzlich sind alle Aluminium-Werkstoffe zum Schweißen geeignet, wobei jedoch reines Aluminium zu Poren in der Schweißnaht neigt. Außerdem neigt die Aluminiumschmelze zu Reaktionen mit der Atmosphäre, weshalb fast immer unter Schutzgas geschweißt wird. Gut geeignet sind das MIG- und Plasmaschweißen sowie das WIG-Schweißen. Bei Letzterem wird bei Nutzung von Wechselstrom das Edelgas Argon als Schutzgas verwendet, und bei Gleichstrom Helium.\n\nFür das Laserschweißen eignen sich sowohl Kohlendioxid- als auch Festkörperlaser, allerdings nicht für alle Legierungen. Wegen der hohen Wärmeleitfähigkeit erstarrt die Schmelze sehr schnell, sodass die Schweißnaht zu Poren und Rissen neigt. Das Widerstandspunktschweißen erfordert, verglichen mit Stahl, höhere elektrische Ströme und kürzere Schweißzeiten sowie teilweise spezielle Geräte, da die handelsüblichen Schweissgeräte für Stahl nicht dafür geeignet sind. Für das Elektronenstrahlschweißen eignen sich alle Legierungen, jedoch neigen Magnesium und Zinn zum Verdampfen während des Schweißvorgangs. Lichtbogenhandschweißen wird nur noch selten verwendet, meist zur Gussnachbesserung. Löten gestaltet sich wegen der sich bildenden Oxidschicht an Luft schwierig. Genutzt werden sowohl Hart- als auch Weichlöten mit speziellen Flussmitteln. Alternativ kann Aluminium ohne Flussmittel mit Ultraschall gelötet werden, dabei wird die Oxidschicht mechanisch während des Lötvorganges aufgebrochen.\n\n\nAluminium in Natur und Organismen.\n\nAluminium im menschlichen Körper.\nAluminium ist kein essentielles Spurenelement und gilt für die menschliche Ernährung als entbehrlich. Im menschlichen Körper befinden sich durchschnittlich etwa 50 bis 150 Milligramm Aluminium. Diese verteilen sich zu ungefähr 50 Prozent auf das Lungengewebe, zu 25 Prozent auf die Weichteile und zu weiteren 25 Prozent auf die Knochen. Aluminium ist damit ein natürlicher Bestandteil des menschlichen Körpers.\n\n99 bis 99,9 Prozent der üblicherweise in Lebensmitteln aufgenommenen Menge von Aluminium (10 bis 40&nbsp;mg pro Tag) werden unresorbiert über den Kot wieder ausgeschieden. Chelatbildner \"(Komplexbildner)\" wie Citronensäure können die Resorption auf 2 bis 3 Prozent steigern. Die Aufnahme von Aluminiumsalzen über den Magen-Darm-Trakt ist gering; sie variiert aber in Abhängigkeit von der chemischen Verbindung und ihrer Löslichkeit, dem pH-Wert und der Anwesenheit von Komplexbildnern. Man schätzt, dass 1&nbsp;‰ beziehungsweise 3&nbsp;‰ des in der Nahrung beziehungsweise im Trinkwasser erhaltenen Aluminiums im Magen-Darm-Trakt absorbiert werden.\n\nVon dort gelangt es in zahlreiche Gewebe und ins Blut. Im Blut ist Al überwiegend (zu etwa 80 %) an Transferrin gebunden. 16 Prozent liegen als [Al(PO)(OH)], 1,9 Prozent als Citrat-Komplex, 0,8 Prozent als Al(OH) und 0,6 Prozent als [Al(OH)] vor. Das Blut Neugeborener enthält bereits Aluminiumionen, die aus dem maternalen Kreislauf stammen. Die Serumkonzentrationen von etwa 6–10 μg/l entspricht in etwa der von Erwachsenen. Durch das Blut gelangen wasserlösliche Aluminiumsalze auch in das Gehirn: Die Passage an der Blut-Hirn-Schranke geschieht durch Endozytose mittels Transferrin-Rezeptor und durch aktiven, ATP-abhängigen Transport des Citrates. Dies wurde tierexperimentell mittels radioaktiv markierten Aluminiums des Isotops Al, das in der Natur nicht vorkommt, nachgewiesen.\n\nDie Eliminierung von in den Organismus gelangten wasserlöslichen Aluminiumsalzen erfolgt innerhalb weniger Tage vorwiegend durch die Nieren über den Urin, weniger über den Kot. Die Halbwertszeit im Blut beträgt hierbei 8 Stunden. Bei Dialysepatienten mit einer eingeschränkten Nierenfunktion besteht daher ein erhöhtes Risiko einer Akkumulation im Körper (Gehirn, Knochen) mit toxischen Effekten, etwa Knochenerweichungen und Schäden des Zentralnervensystems; zusätzlich sind Dialysepatienten aufgrund für sie notwendiger pharmazeutischer Produkte (Phosphatbinder) einer höheren Aluminiumzufuhr ausgesetzt. Aluminium, das nicht über die Nieren ausgeschieden wird, gelangt in die Knochen. Dort wird es vergleichsweise sehr langsam eliminiert (Halbwertszeit mehrere Jahre), so dass man durch Modelschätzungen annimmt, dass etwa 1–2 % der resorbierten Dosis sich im Körper anhäufen. In einem Leben häufen sich etwa 35 bis 50 mg Aluminium im Körper an.\n\n\nPflanzen.\nAluminium in Form verschiedener Salze (Phosphate, Silikate) ist Bestandteil vieler Pflanzen und Früchte, denn gelöste Al-Verbindungen werden durch Regen aus den Böden von den Pflanzen aufgenommen, bei Säurebelastung der Böden infolge sauren Regens ist dies vermehrt der Fall (siehe dazu Waldschäden).\n\nEin großer Teil des Bodens auf der Welt ist chemisch sauer. Liegt der pH-Wert unter 5,0, werden Al-Ionen von den Wurzeln der Pflanzen aufgenommen. Dies ist bei der Hälfte des bebaubaren Lands auf der Welt der Fall. Die Ionen schädigen insbesondere das Wurzelwachstum der Feinwurzeln. Wenn die Pflanze nicht Aluminium-tolerant ist, steht sie dann unter Stress. Zahlreiche Enzyme und signalübertragende Proteine sind betroffen; die Folgen der Vergiftung sind noch nicht vollständig bekannt. In sauren metallhaltigen Böden ist Al das Ion mit dem größten Potenzial zur Schädigung. Von der Modellpflanze \"Arabidopsis\" sind Transgene bekannt, die deren Aluminium-Toleranz heraufsetzen und auch bei Kulturpflanzen sind tolerante Sorten bekannt.\n\nDer saure Regen hat beispielsweise in Schweden in den 1960er Jahren die Seen übersäuert, wodurch mehr Al-Ionen in Lösung gingen und empfindliche Fische verendeten. In Norwegen wurde dieser Zusammenhang bei einem Forschungsprojekt in den 1970er Jahren festgestellt.\n\nBei pH-Werten über 5,0 ist Aluminium als polymeres Hydroxykation an der Oberfläche von Silicaten gebunden. Bei pH-Werten von 4,2 bis 5 steigt Anteil von mobilen Kationen.\n\nBei Erhöhung der Schwefelsäurekonzentration durch sauren Regen bildet sich Aluminiumhydroxysulfat:\n\n\nIn Lebensmitteln.\nDie meisten Lebensmittel enthalten Aluminium in Spurenmengen. Unverarbeitete pflanzliche Lebensmittel enthalten durchschnittlich weniger als 5&nbsp;mg/kg in der Frischmasse. Dabei streuen die Werte aufgrund unterschiedlicher Sorten, Anbaubedingungen und Herkunft in erheblichem Maße. So weisen beispielsweise Salat und Kakao deutlich höhere Durchschnittswerte auf. Zwischen 5 und 10 mg/kg finden sich in Brot, Kuchen, Backwaren, einer Vielzahl von mehlhaltigen Speisen, einigen Gemüsearten oder Würsten. Schwarzer Tee kann Gehalte von bis zu 1042&nbsp;mg/kg in der Trockenmasse aufweisen. Jedoch ist dort das Aluminium an schlecht absorbierbaren Polyphenolen gebunden, so dass eine Aufnahme im Magen-Darm-Trakt erschwert wird. Einen hohen Aluminiumgehalt weisen Kräuter und Gewürze auf, beispielsweise Thymianblätter. Im europäischen Vergleich zeigen sich Schwankungen, was vermutlich auf eine unterschiedlich hohe Aluminiumgrundbelastung und Verwendung von aluminumhaltigen Zusatzstoffen zurückzuführen ist.\n\nBeim Kochen oder Aufbewahren in Aluminiumgeschirr oder in Alufolie kann es (außer bei sauren Lebensmitteln) nach einer Schätzung zu einer maximalen zusätzlichen Aufnahme von 3,5&nbsp;mg/Tag/Person kommen. Bei sauren Lebensmitteln wie Sauerkraut oder Tomaten können aufgrund der Säurelöslichkeit wesentlich höhere Werte erreicht werden. Das Bundesinstitut für Risikobewertung (BfR) rät von der Zubereitung und Lagerung von insbesondere sauren und salzigen Lebensmitteln in unbeschichteten Aluminiumgefäßen oder Alufolie ab. Hohe Belastungen fallen beispielsweise dann an, wenn Fisch oder Fleischgerichten mit Zitrone oder anderen sauren Zutaten in Aluminiumschalen oder -folien angerichtet und über längere Zeit hoch erhitzt werden.\n\nTrink- und Mineralwässer weisen mit durchschnittlich 0,2–0,4&nbsp;mg/l im Gegensatz zur Nahrung geringe Gehalte auf und stellen somit nur einen kleinen Beitrag zur täglichen Aluminium-Aufnahme. Die Trinkwasserverordnung legt einen Grenzwert von 0,2&nbsp;mg/l fest. Trinkwasser darf in Deutschland, Österreich und der Schweiz keine höheren Werte aufweisen.\n\nNach einer Schätzung nimmt der erwachsene Europäer im Durchschnitt zwischen 1,6 und 13&nbsp;mg Aluminium pro Tag über die Nahrung auf. Dies entspricht einer wöchentlichen Aufnahme von 0,2 bis 1,5&nbsp;mg Aluminium pro kg Körpergewicht bei einem 60&nbsp;kg schweren Erwachsenen. Die großen Unsicherheiten beruhen auf den unterschiedlichen Ernährungsgewohnheiten und der variablen Gehalte an Aluminium in den Lebensmitteln, auch innerhalb eines Landes aufgrund verschiedener Erhebungen. Falls Säuglinge mit Fertignahrung ernährt werden, kann die Aluminiumkonzentration im Blut bei 15 μg/l liegen. Eine mögliche gesundheitliche Schädigung ist nicht bekannt.\n\nDie Europäische Behörde für Lebensmittelsicherheit (Efsa) legt eine tolerierbare wöchentliche Aufnahme (TWI) von 1&nbsp;mg Aluminium pro kg Körpergewicht fest, vorher lag diese bei 7 mg Al pro kg KW. Wegen der möglichen Akkumulation im Körper bevorzugt die Efsa den TWI im Gegensatz zur tolerierbarer täglicher Aufnahme (TDI).\n\nAluminium ist als Lebensmittelzusatzstoff unter der Bezeichnung ausschließlich als Farbmittel für Überzüge von Zuckerwaren und als Dekoration von Kuchen und Keksen erlaubt. Weiterhin ist Aluminium zum Färben von Arzneimitteln und Kosmetika zugelassen. Bei der Untersuchung von Laugengebäck (Brezeln, Stangen, Brötchen) aus Bäckereien wurde Aluminium nachgewiesen, das in das Lebensmittel gelangt, wenn bei der Herstellung von Laugengebäck Aluminiumbleche verwendet werden.\n\nWährend Bier in Aluminiumfässern transportiert wird, hat sich für den Weintransport der Werkstoff Aluminium nicht durchgesetzt. Ein kurzfristiger Kontakt schadet nicht, doch können nach längerem Kontakt Weinfehler in Geruch und Geschmack oder als Trübung auftreten, vor allem beim offenen Stehen an der Luft.\n\n\nToxizität.\nBei eingeschränkter Nierenfunktion und bei Dialyse-Patienten führt die Aufnahme von Aluminium zu progressiver Enzephalopathie (Gedächtnis- und Sprachstörungen, Antriebslosigkeit und Aggressivität) durch Untergang von Hirnzellen und zu fortschreitender Demenz, zu Osteoporose (Arthritis) mit Knochenbrüchen und zu Anämie (weil Aluminium dieselben Speichereiweiße wie Eisen besetzt). Dies wurde in den 1970er Jahren bei langjährigen Hämodialysepatienten durch starke Aluminiumzufuhr beobachtet \"(„Dialysis Encephalopathy Syndrome“)\".\n\nSpeziell im Hinblick auf die Verwendung in Deodorants und Lebensmittel-Zusatzstoffen werden die gesundheitlichen Auswirkungen von Aluminium kontrovers diskutiert. So wurde Aluminium mehrfach kontrovers als Faktor im Zusammenhang mit der Alzheimer-Krankheit in Verbindung gebracht.\n\nLaut einer Studie des Bundesinstituts für Risikobewertung (BfR) vom Juli 2007 wurde im allgemeinen Fall zum Zeitpunkt der Erstellung der Studie aufgrund der vergleichsweise geringen Menge kein Alzheimer-Risiko durch Aluminium aus Bedarfsgegenständen erkannt; jedoch sollten vorsorglich keine sauren Speisen in Kontakt mit Aluminiumtöpfen oder -folie aufbewahrt werden. Das Bundesinstitut für Risikobewertung (BfR) hat im Jahr 2020 eine neue gesundheitliche Risikobewertung zu aluminiumhaltigen Antitranspirantien erstellt. Das Ergebnis: Gesundheitliche Beeinträchtigungen durch den regelmäßigen Gebrauch von Aluminiumchlorohydrat-haltigen Antitranspirantien sind nach gegenwärtigem wissenschaftlichen Kenntnisstand unwahrscheinlich. Bei der Risikobewertung von Aluminium ist es jedoch grundsätzlich wichtig, die Gesamtaufnahme über die verschiedenen Eintragspfade wie Lebensmittel oder aluminiumhaltige Produkte für den Lebensmittelkontakt zu betrachten.\n\nDie britische Alzheimer-Gesellschaft mit Sitz in London vertritt den Standpunkt, dass die bis 2008 erstellten Studien einen kausalen Zusammenhang zwischen Aluminium und der Alzheimer-Krankheit nicht überzeugend nachgewiesen haben. Dennoch gibt es einige Studien, wie die PAQUID-Kohortenstudie in Frankreich, mit einer Gesundheitsdatenauswertung von 3777 Personen im Alter ab 65 Jahren seit 1988 bis zur Gegenwart, in welchen eine Aluminium-Exposition als Risikofaktor für die Alzheimer-Krankheit angegeben wird. Demnach wurden viele senile Plaques mit erhöhten Aluminium-Werten in Gehirnen von Alzheimer-Patienten gefunden. Es ist jedoch unklar, ob die Aluminium-Akkumulation eine Folge der Alzheimer-Krankheit ist, oder ob Aluminium in ursächlichem Zusammenhang mit der Alzheimer-Krankheit zu sehen ist. Die Deutsche Alzheimer Gesellschaft sieht zwar keinen überzeugenden Zusammenhang zwischen Aluminium-Aufnahme und Alzheimer-Krankheit, gibt aber schädliche Wirkungen von Aluminium auf das Gehirn zu und stellt fest, dass bisher (Stand 2013) keine Klärung der Frage existiert, ob Aluminium als ein Auslöser der Alzheimer-Krankheit zu sehen ist. Abschließend wird geraten, dass sich Dialysepatienten vergewissern sollten, dass nur aluminiumfreie Flüssigkeiten zur Blutreinigung eingesetzt werden und dass aluminiumhaltige, die Magensäure bindende Arzneimittel nur nach ärztlicher Anordnung eingenommen werden sollten.\n\nAluminium gehört zu den nicht essentiellen Spurenelementen, bei der Toxizität kommt es im Wesentlichen auf die Menge an: 10 µg/l Aluminium im Blut gilt als Normalwert, Werte über 60 µg/l sprechen für übermäßige Belastung und Werte über 200 µg/l im Blut gelten als toxisch. Tierexperimentelle Studien mittels Al zeigen, dass sich die Serumkonzentration von Aluminium durch eine adjuvierte Impfung nur um wenige Promille erhöht (von etwa 5 µg/l auf 5,04 µg/l).\n\n\nAspekte der Ökobilanz.\nDie Herstellung von Aluminium ist sehr energieaufwendig. Allein für die Schmelzflusselektrolyse zur Gewinnung eines Kilogramms Aluminium werden je nach Errichtungsdatum und Modernität der Anlage zwischen 12,9 und 17,7&nbsp;kWh elektrische Energie benötigt. Bei der Stromerzeugung für die Produktion von einem Kilogramm Aluminium werden im deutschen Kraftwerkspark 8,4&nbsp;kg CO freigesetzt, im weltweiten Durchschnitt etwa 10&nbsp;kg. Es ist aber auch zu bedenken, dass aufgrund des Kostenfaktors Energie die Elektrolyse verstärkt an Orten erfolgt, an denen auf billige, CO-emissionsarme Wasserkraft zurückgegriffen werden kann, wie in Brasilien, Kanada, Venezuela oder Island. Allerdings ist auch bei Verwendung von Elektrizität aus vollständig regenerativen Energien die Produktion von Aluminium nicht CO-frei, da der bei der Schmelzflusselektrolyse entstehende Sauerstoff mit dem Kohlenstoff der Elektroden zu CO reagiert. Die Verbrauchswerte für Roh-Aluminium erhöhen sich durch Transport- und Verarbeitungsanteile für das Wiederaufschmelzen, Gießen, Schleifen, Bohren sowie Polieren auf 16,5&nbsp;kg CO pro kg Aluminium-Konsumgut.\n\nDie europaweite Recyclingrate von Aluminium liegt bei 67 Prozent. In Österreich gelangen (laut einer Studie aus dem Jahr 2000) 16.000&nbsp;Tonnen Aluminium pro Jahr \"über Verpackungen\" in den Konsum, ebenso gelangen 16.000&nbsp;Tonnen Aluminium ohne Wiederverwertung in den Hausmüll (dabei sind auch die Aluminiumhaushaltsfolien eingerechnet, die nicht als „Verpackung“ gelten). 66 Prozent der Verpackungen im Restmüll sind Aluminium[getränke]dosen. Diese liegen nach der Müllverbrennung in der Asche noch metallisch vor und machen in Europa durchschnittlich 2,3 Prozent der Asche aus. In der EU werden durchschnittlich 70 Prozent des in der Bodenasche enthaltenen Aluminiums zurückgewonnen.\n\nDurch den Abbau des Erzes Bauxit werden große Flächen in Anspruch genommen, die erst nach einer Rekultivierung wieder nutzbar werden. Um eine Tonne Aluminium herzustellen, werden vier Tonnen Bauxit benötigt. Dies erzeugt zehn Tonnen Abraum. Zudem entstehen bei der Herstellung des Aluminiumoxids nach dem Bayer-Verfahren ca. drei Tonnen von eisenreichem alkalischen Rotschlamm, der kaum wiederverwertet wird und dessen Deponierung oder sonstige „Entsorgung“ große Umweltprobleme aufwirft (siehe entsprechende Abschnitte unter \"Rotschlamm\" und \"Bauxitbergbau in Australien\").\n\nPositiv ist hingegen die gute Wiederverwendbarkeit von Aluminium hervorzuheben, wobei die Reststoffe streng getrennt erfasst und gereinigt werden müssen (Aluminiumrecycling, Recycling-Code-41 (ALU)). Aluminium ist dabei besser rezyklierbar als Kunststoffe, wegen Downcycling bei nicht sortenreiner Erfassung jedoch etwas schlechter wiederverwertbar als Stahl. Beim Aluminiumrecycling wird nur 5 Prozent der Energiemenge der Primärproduktion benötigt.\nDurch Leichtbau mit Aluminiumwerkstoffen (beispielsweise Aluminiumschaum, Strangpressprofile) wird Masse von beweglichen Teilen und Fahrzeugen gespart, was zur Einsparung von Treibstoff führen kann.\n\nAluminium ist durch seine Selbstpassivierung korrosionsbeständiger als Eisen und erfordert daher weniger Korrosionsschutzmaßnahmen.\n\n\nNachweis.\nAluminiumsalze weist man durch Glühen mit verdünnter Kobaltnitratlösung (Co(NO)) auf der Magnesia-Rinne nach. Dabei entsteht das Pigment Thénards Blau, ein Cobaltaluminiumspinell mit der Formel CoAlO. Es wird auch Kobaltblau oder Cobaltblau, Dumonts Blau, Coelestinblau, Cobaltaluminat oder – nach dem Entdecker des Pigments, Josef Leithner – Leithners Blau genannt.\n\n\nNachweis mittels Kryolithprobe.\nDie Probelösung wird alkalisch gemacht, um Aluminium als Aluminiumhydroxid Al(OH) zu fällen. Der Niederschlag wird abfiltriert und mit einigen Tropfen Phenolphthalein versetzt, dann gewaschen, bis keine Rotfärbung durch Phenolphthalein mehr vorhanden ist. Wenn anschließend festes Natriumfluorid (NaF) auf den Niederschlag gestreut wird, verursachen Hydroxidionen, die bei der Bildung von Kryolith Na[AlF] freigesetzt werden, eine erneute Rotfärbung des Phenolphthaleins.\n\n\nNachweis als fluoreszierender Morinfarblack.\nDie Probe wird mit Salzsäure (HCl) versetzt und eventuell vorhandenes Aluminium somit gelöst. Anschließend wird die Probelösung mit Kaliumhydroxid (KOH) stark alkalisch gemacht. Gibt man nun einige Tropfen der Probelösung zusammen mit der gleichen Menge Morin-Lösung auf eine Tüpfelplatte und säuert anschließend mit konzentrierter Essigsäure (\"Eisessig\", CHCOOH) an, so ist unter UV-Strahlung (λ&nbsp;=&nbsp;366&nbsp;nm) eine grüne Fluoreszenz beobachtbar. Der Nachweis ist dann sicher, wenn diese Fluoreszenz bei Zugabe von Salzsäure wieder verschwindet.\n\nGrund hierfür ist, dass Al(III) in neutralen sowie essigsauren Lösungen in Verbindung mit Morin eine fluoreszierende kolloidale Suspension bildet.\n\n\n\n\nLiteratur.\nZur Geschichte\n\nDeutsche Fachliteratur\n\nEnglische Fachliteratur\n\n"}
{"id": "95", "url": "https://de.wikipedia.org/wiki?curid=95", "title": "Antimon", "text": "Antimon\n\nAntimon [] (von lateinisch \"Antimonium\", vermutlich von arabisch „al-ithmîd(un)“ (, Antimonsulfid bzw. Stibnit)) ist ein chemisches Element mit dem Elementsymbol Sb (von ‚(Grau-)Spießglanz‘) und der Ordnungszahl 51. Im Periodensystem steht es in der 5. Periode und der 5. Hauptgruppe, bzw. 15.&nbsp;IUPAC-Gruppe oder Stickstoffgruppe. In der stabilen Modifikation ist es ein silberglänzendes und sprödes Halbmetall.\n\n\nName, Geschichte.\nEs wird auch vermutet, dass der Name auf das spätgriechische \"anthemon\" (dt. \"Blüte\") zurückgeht. Damit sollten die stängelartigen Kristalle von Antimonsulfid (SbS) beschrieben werden, die büschelförmig erschienen und wie eine Blüte aussähen. Im 11. Jahrhundert findet sich der lateinische Begriff für die mineralische Arzneidroge \"antimonium\" zur innerlichen Therapie von Krankheiten dann bei Constantinus Africanus.\n\nIm 17.&nbsp;Jahrhundert ging der Name Antimon als Bezeichnung auf das Metall über. Die koptische Bezeichnung für das Schminkpuder Antimonsulfid ging über das Griechische in das Lateinische \"stibium\" über. Die vom schwedischen Mediziner und Chemiker Jöns Jakob Berzelius („Vater der modernen Chemie“) benutzte Abkürzung Sb wird noch heute als Elementsymbol genutzt.\n\nEine späte legendäre Volksetymologie, die von Samuel Johnson in seinem Wörterbuch verewigt wurde, besagt, dass der deutsche Mönch Basilius Valentinus die Beobachtung machte, dass Schweine durch die Aufnahme von Antimon schnell fett wurden. Er probierte dies auch an seinen Ordensbrüdern aus, woraufhin diese allerdings starben, sodass der Begriff „antimoine“ (\"antimönchisch\") geprägt wurde, aus dem später „Antimon“ entstanden sei.\n\nAls Typlokalität für gediegenes Antimon gilt die Silbermine in der schwedischen Gemeinde Sala im Västmanland. Allerdings war metallisches Antimon schon den Chinesen und Babyloniern bekannt. Einige seiner Verbindungen wurden schon in der Bronzezeit als Zuschlag zu Kupfer verwendet, um Bronze herzustellen (Funde von Velem-St. Vid in Ungarn).\n\n\nVorkommen.\nAntimon ist ein selten vorkommendes Element. Da es in der Natur auch gediegen (das heißt in elementarer Form) gefunden werden kann, wird es von der International Mineralogical Association (IMA) unter der System-Nr. 1.CA.05 als Mineral anerkannt.\n\nWeltweit konnte gediegenes Antimon bisher (Stand: 2011) an rund 300 Fundorten nachgewiesen werden. So unter anderem in mehreren Regionen von Australien; in den bolivianischen Departements La Paz und Potosí; Minas Gerais in Brasilien; Schwarzwald, Fichtelgebirge, Oberpfälzer Wald, Odenwald und im Harz in Deutschland; Seinäjoki in Finnland; mehreren Regionen von Frankreich; Lombardei, Piemont, Sardinien und Trentino-Südtirol in Italien; einigen Regionen von Kanada; einigen Regionen von Österreich; Ost- und Westsibirien und Ural in Russland; neben Västmanland noch Dalarna, Gästrikland, Närke, Södermanland, Värmland und Västerbotten in Schweden; in einigen Regionen der Slowakei; Böhmen und Mähren in Tschechien sowie in vielen Regionen der USA. Eine der weltweit bedeutendsten Lagerstätten für gediegen Antimon und Antimonerze ist der \"Murchison greenstone belt\" in der Murchison Range von Südafrika.\n\nBisher sind 264 Antimon-Minerale bekannt (Stand: 2010). Industriell genutzt wird überwiegend das Sulfid-Mineral Stibnit SbS (Grauspießglanz) mit einem Gehalt von maximal&nbsp;71,7 %&nbsp;Sb. Das Mineral mit dem höchsten Sb-Gehalt in einer chemischen Verbindung ist die natürliche Antimon-Arsen-Legierung Paradocrasit (max.&nbsp;92 %). Allerdings kommt sie mit nur drei Fundorten, im Gegensatz zum Stibnit (rund 2500 Fundorte), sehr viel seltener vor. Weitere Quellen für Antimon sind die Minerale Valentinit SbO (Weißspießglanz), Breithauptit NiSb (Antimonnickel, Nickelantimonid), Kermesit SbSO (Rotspießglanz) und SbS (Goldschwefel).\n\n\nGewinnung und Darstellung.\nTechnisch wird Antimon aus dem Antimonglanz gewonnen. Ein Verfahren beruht auf dem Abrösten und der Reduktion mit Kohlenstoff (Röstreduktionsverfahren):\n\nEine andere Möglichkeit besteht darin, die Reduktion mit Eisen durchzuführen (Niederschlagsverfahren):\n\nWeltweit wurden zu Beginn des 21.&nbsp;Jahrhunderts zwischen 110.000 und 160.000 Tonnen pro Jahr an Antimon gefördert. Seit 1900 hat sich damit die Fördermenge mehr als verzehnfacht.\n\n87 % der Antimonproduktion findet in China statt (Stand: 2015).\n\n\nEigenschaften.\n\nModifikationen.\nAntimon kann in drei verschiedenen Modifikationen auftreten, wobei metallisches bzw. \"graues\" Antimon die beständigste Modifikation ist.\n\nUnter Normalbedingungen kristallisiert Antimon trigonal in rhomboedrischer Aufstellung in der nach der Hermann-Mauguin-Symbolik beschriebenen Raumgruppe&nbsp; mit den Gitterparametern \"a\"&nbsp;=&nbsp;431&nbsp;pm und \"c\"&nbsp;=&nbsp;1127&nbsp;pm sowie sechs Formeleinheiten pro Elementarzelle.\n\nDurch Abschrecken von Antimondampf an kalten Flächen entsteht amorphes, \"schwarzes\" und sehr reaktives Antimon, welches sich durch Erhitzen wieder in metallisches Antimon umwandelt. Durch elektrolytische Herstellung entsteht \"explosives\" Antimon, das beim Ritzen explosionsartig aufglühend und funkensprühend in metallisches Antimon übergeht. Diese Form enthält jedoch immer etwas Chlor und kann nicht als Modifikation betrachtet werden. \"Gelbes\" Antimon ist ebenfalls keine eigenständige Modifikation, sondern eine hochpolymere chemische Verbindung mit Wasserstoff.\n\n\nPhysikalische Eigenschaften.\nMetallisches Antimon ist silberweiß, stark glänzend, blättrig-grobkristallin. Es lässt sich aufgrund seiner Sprödigkeit leicht zerkleinern. Elektrische und thermische Leitfähigkeit sind gering.\n\n\nChemische Eigenschaften.\nMit naszierendem Wasserstoff reagiert Antimon zum instabilen Antimonhydrid SbH. Von Luft und Wasser wird Antimon bei Raumtemperatur nicht angegriffen. Oberhalb des Schmelzpunkts verbrennt es in Luft mit bläulich-weißer Flamme zu Antimon(III)-oxid. In heißen konzentrierten Mineralsäuren löst es sich auf. Mit den Halogenen reagiert es schon bei Raumtemperatur heftig zu den entsprechenden Halogeniden.\n\nIn Verbindungen liegt Antimon überwiegend in den Oxidationsstufen +3 und +5 vor. In Metallantimoniden wie Kaliumantimonid KSb bildet es Sb-Ionen.\n\n\nIsotope.\nEs existieren zwei stabile Antimon-Isotope: Sb und Sb.\n\n\nVerwendung.\n\nLegierungen.\nDer überwiegende Teil des hergestellten Antimons wird zu Legierungen verarbeitet und zeigt dabei folgende Eigenschaften:\n\nWichtige Legierungen:\n\n\nMedizin.\nAntimon (bzw. ein aus Antimonerz gewonnenes Präparat) wurde im 16. und 17. Jahrhundert zu einem (iatrochemischen) „Leitarzneimittel“, war aber – wie auch andere paracelsische Medikamente – umstritten und in Frankreich zwischen 1615 und 1688 auch verboten.\n\nBrechweinstein wurde lange als brechreizerregendes Mittel verwendet (Antimonpille), heute wird es noch manchmal verwendet, um den Mageninhalt von Vögeln zu untersuchen.\n\nSowohl Schistosomiasis als auch Trypanosomen wurden beginnend Anfang des 19. Jahrhunderts mit Brechweinstein (Kaliumantimonyltartrat) bekämpft. Brechweinstein wurde hergestellt, indem man für einen Tag Wein in einem Antimonbecher lagerte, und diesen dann austrank. Inzwischen kommen effektivere und verträglichere Medikamente zur Anwendung.\n\nAntimonpräparate werden meist als weniger toxische pentavalente Formen zur medikamentösen Therapie der Leishmaniose und Schistosomiasis eingesetzt, allerdings in entwickelten Ländern nicht mehr als Mittel der ersten Wahl. Hierbei hemmt Antimon das Enzym Phosphofructokinase, das den geschwindigkeitsbestimmenden Schritt der Glykolyse darstellt.\n\n\n\nToxizität.\nAntimon kann bereits bei Ingestion von 200 bis 1200 mg tödlich sein. In der Toxikologie sind drei Antimon-Formen bekannt, von denen das gasförmige Antimonhydrid (Stiban, SbH) die gefährlichste Form ist, die eine massive Hämolyse induziert. Nach der Toxizität folgt Brechweinstein mit dreiwertigem („trivalentem“) Antimon, während fünfwertiges Antimon am wenigsten toxisch ist.\n\nDas trivalente Antimon wird innerhalb der ersten zwei Stunden nach der Einnahme zu 95 % in rote Blutkörperchen aufgenommen und damit vorwiegend in stark durchbluteten Organen angereichert. Die Exkretion erfolgt vorwiegend durch Bindung an Glutathion über die Galle mit entsprechend hohem enterohepatischen Kreislauf, und nur ein geringer Teil wird über die Nieren ausgeschieden. Kaliumantimonyltartrat wird zu 90 % innerhalb des ersten Tages nach Aufnahme ausgeschieden, die übrigen 10 % aufgrund einer langsameren Eliminationskinetik über 16 Tage.\n\nEs wird vermutet, dass Antimon ähnlich wie Arsen die Funktion des Pyruvatdehydrogenase-Komplexes hemmt und somit zu einem Mangel des intrazellulären Energieträgers Adenosintriphosphat (ATP) führt. Dabei kommt es zur Bildung von Chelatkomplexen zwischen dem Antimon und Thiol-Gruppen der entsprechenden Enzyme. Im Körper wirkt es in zahlreichen Organen toxisch, so im Verdauungstrakt, in der Leber, in den Nieren, im Herz und im Zentralnervensystem. Die höchste Konzentration erreicht Antimon in der Leber, wo es zu einer Hepatitis bis hin zum Leberversagen kommen kann. Am Herzen kommt es zu EKG-Veränderungen mit Inversion und Verminderung der T-Welle und verlängertem QT-Intervall. Ein akutes Nierenversagen kann zur temporären oder permanenten Hämodialyse führen.\n\nTherapeutisch erfolgt bei einer Antimon-Vergiftung neben unterstützenden Maßnahmen wie Infusionstherapie (sowohl zum Ausgleich des Flüssigkeitsverlustes durch das Erbrechen als auch zum Schutz der Nieren), und engmaschiger Überwachung der Vitalfunktionen und des EKGs die Gabe von Aktivkohle, N-Acetylcystein als Vorläufer des Glutathions zur vermehrten Sekretion und eines Chelatbildners, z.&nbsp;B. Dimercaprol.\n\nErgebnisse aus Untersuchungen deuten darauf hin, dass Antimonverbindungen Haut und Schleimhäute reizen. Diese Verbindungen lösen sich vermutlich aus Kunststoff und Textilien.\n\n\nSicherheitshinweise und Grenzwerte.\nVon den Antimonverbindungen sind seitens der EU Antimonfluorid als giftig (T) und die Chloride als ätzend (C) eingestuft, außerdem als umweltgefährlich (N); alle anderen Antimonverbindungen als gesundheitsschädlich (Xn) und umweltgefährlich (N). Antimon selbst ist dort nicht aufgeführt, laut Sicherheitsdatenblatt ist es als \"reizend\" gekennzeichnet.\n\nDie Internationale Agentur für Krebsforschung (IARC) stuft Antimon(III)-oxid als \"möglicherweise krebserzeugende Substanz\" ein.\n\nIn der EU gilt für Trinkwasser ein Grenzwert von 5&nbsp;µg/l. Untersuchungen von in PET-Flaschen abgefüllten Fruchtsäften (für die keine Richtlinien existieren) ergaben Antimonkonzentrationen bis zu 44,7&nbsp;µg/l in unverdünnten Saftkonzentraten.\n\nAntimon wurde 2016 von der EU gemäß der Verordnung (EG) Nr. 1907/2006 (REACH) im Rahmen der Stoffbewertung in den fortlaufenden Aktionsplan der Gemeinschaft (CoRAP) aufgenommen. Hierbei werden die Auswirkungen des Stoffs auf die menschliche Gesundheit bzw. die Umwelt neu bewertet und ggf. Folgemaßnahmen eingeleitet. Ursächlich für die Aufnahme von Antimon waren die Besorgnisse bezüglich Exposition von Arbeitnehmern, hoher (aggregierter) Tonnage, hohes Risikoverhältnis (Risk Characterisation Ratio, RCR) und weit verbreiteter Verwendung sowie der möglichen Gefahr durch krebsauslösende Eigenschaften. Die Neubewertung läuft seit 2018 und wird von Deutschland durchgeführt.\n\n\nNachweis.\nVorproben:\n\nFlammenfärbung: Flamme fahlblau, wenig charakteristische\nPhosphorsalzperle: Farblos (gestört durch alle Elemente, die eine farbige Perle erzeugen)\n\nNachweisreaktion:\n\nReduktion durch unedle Metalle, zum Beispiel Eisen, Zink oder Zinn.\n\nIn nicht zu sauren Lösungen reduzieren unedle Metalle Antimon-Kationen Sb(III), Sb(V) und Sb(III)/(V) zu metallischem Antimon:\n\nDie auf Antimon zu prüfende Substanz wird in salzsaure Lösung gegeben und mit Eisenpulver versetzt. Es entsteht ein schwarzer, flockiger Niederschlag aus metallischem Antimon in der Lösung oder direkt am Eisen. Auch der Nachweis an einem Eisennagel ist möglich. Dabei ist eine schwarze Ablagerung am Nagel ein Nachweis für Antimon, welches sich hier elementar niedergeschlagen hat.\n\nDie Marshsche Probe gestattet einen eindeutigen Nachweis von Antimon. Wenn die pyrolytisch abgeschiedene Substanz (dunkel glänzender Spiegel) sich nicht in ammoniakalischem Wasserstoffperoxid löst, sind Arsen und Germanium als mögliche Alternativen ausgeschlossen.\n\nDie hochempfindliche Bestimmung winziger Antimonspuren erfolgt durch die Hydridtechnik der Atomspektrometrie. Hierbei wird im Prinzip die Marshsche Probe mit der Atomabsorptionsspektrometrie gekoppelt. Die Matrixeffekte der Probelösung lassen sich dadurch sehr wirksam unterdrücken.\n\nEine weitere Methode besteht darin, eine wässrige Lösung, in der Antimonionen enthalten sind, mit Rhodamin-B-Lösung zu versetzen. Es bildet sich ein farbiger Komplex, der mit Isopropylether extrahierbar ist. Dieser Nachweis ist allerdings recht unspezifisch, da auch Gold-, Cadmium-, Gallium, Thallium-, Uran- und Wolfram-ionen farbige Komplexe bilden.\n\n\n\n\n\n\n\n\n\n"}
{"id": "96", "url": "https://de.wikipedia.org/wiki?curid=96", "title": "Argon", "text": "Argon\n\nArgon ( „untätig, träge“) ist ein chemisches Element mit dem Symbol Ar (bis 1957 nur A) und der Ordnungszahl 18. Im Periodensystem steht es in der 8.&nbsp;Hauptgruppe bzw. der 18.&nbsp;IUPAC-Gruppe und zählt daher zu den Edelgasen. Wie die anderen Edelgase ist es ein farbloses, äußerst reaktionsträges, einatomiges Gas. In vielen Eigenschaften wie Schmelz- und Siedepunkt oder Dichte steht es zwischen dem leichteren Neon und dem schwereren Krypton.\n\nArgon ist das häufigste auf der Erde vorkommende Edelgas, der Anteil an der Atmosphäre beträgt etwa 0,934 %. Damit ist Argon der dritthäufigste Bestandteil der Erdatmosphäre, nach Stickstoff und Sauerstoff. Dies ist großteils auf den Zerfall des Kaliumisotops K zurückzuführen, bei dem Ar entsteht.\n\nArgon war das erste Edelgas, das als Stoff entdeckt und gewonnen wurde, daher der Name, der im Grunde zu jedem Edelgas passt. Helium (von griechisch \"helios\" für „Sonne“) wurde vorher lediglich spektroskopisch im Sonnenlicht sowie in irdischen Proben nachgewiesen und Neon erst später entdeckt. Argon wurde 1894 von Lord Rayleigh und William Ramsay durch fraktionierte Destillation von flüssiger Luft gefunden. Als preiswertestes Edelgas wird Argon in großen Mengen als Schutzgas etwa beim Schweißen und in der Produktion von manchen Metallen, aber auch als Füllgas von Glühlampen verwendet.\n\n\nGeschichte.\nEinen ersten Hinweis auf das später entdeckte Argon fand Henry Cavendish, der 1783 die Reaktivität der Luft erforschte. Er erzeugte elektrische Entladungen in einer bestimmten Menge Luft, die mit Sauerstoff im Verhältnis von 5:3 angereichert war. Stickstoff und Sauerstoff reagierten miteinander und die entstandenen Stickoxide konnten ausgewaschen werden. Dabei blieb stets ein kleiner Rest nicht-reagierten Gases zurück. Cavendish erkannte jedoch nicht, dass es sich dabei um ein anderes Element handelte und setzte seine Experimente nicht fort.\n\nNachdem John William Strutt, 3. Baron Rayleigh 1892 die Dichte von aus Luft isoliertem Stickstoff bestimmt hatte, fiel ihm auf, dass aus Ammoniak gewonnener Stickstoff eine niedrigere Dichte aufwies. Es gab verschiedene Spekulationen zu diesem Befund; so meinte James Dewar, es müsse sich um ein N, also ein Stickstoff-Analogon zu Ozon handeln. Rayleigh wiederholte Cavendishs Experimente, indem er in einer luftgefüllten Glaskugel elektrische Funken erzeugte und so Stickstoff und Sauerstoff zur Reaktion brachte. Nach Bestätigung von Cavendishs Ergebnis eines unreaktiven Rückstandes untersuchte William Ramsay diesen ab 1894 durch Überleitung über heißes Magnesium genauer. Da Magnesium mit Stickstoff zum Nitrid reagiert, konnte er dem Gemisch weiteren Stickstoff entziehen. Dabei stellte er eine Erhöhung der Dichte fest und fand schließlich ein bislang unbekanntes, reaktionsträges Gas. Am 31. Januar 1895 gaben Ramsay und Rayleigh schließlich die Entdeckung des neuen Elements bekannt, das sie nach dem altgriechischen \"argos\", „träge“, \"Argon\" nannten. Als William Ramsay ab 1898 das aus der Luft isolierte Argon weiter untersuchte, entdeckte er darin drei weitere Elemente, die Edelgase Neon, Krypton und Xenon.\n\nErste technische Anwendungen fand das Gas in der Elektroindustrie: Es wurden unter anderem Gleichrichter auf der Basis der Glimmentladung in Argon hergestellt, die sogenannten \"Tungar-Röhren.\"\n\n\nVorkommen.\nArgon zählt im Universum zu den häufigeren Elementen, in seiner Häufigkeit ist es vergleichbar mit Schwefel und Aluminium. Es ist im Universum nach Helium und Neon das dritthäufigste Edelgas. Dabei besteht das primordiale Argon, das etwa in der Sonne oder Gasplaneten wie Jupiter gefunden wird, hauptsächlich aus den Isotopen Ar und Ar, während das dritte stabile Isotop, Ar, dort nur in geringer Menge vorkommt. Das Verhältnis von Ar zu Ar beträgt etwa 5,7.\n\nAuf der Erde ist Argon dagegen das häufigste Edelgas. Es macht 0,934 % des Volumens der Atmosphäre (ohne Wasserdampf) aus und ist damit nach Stickstoff und Sauerstoff der dritthäufigste Atmosphärenbestandteil. Die Zusammensetzung des terrestrischen Argons unterscheidet sich erheblich von derjenigen des primordialen Argons im Weltall. Es besteht zu über 99 % aus dem Isotop Ar, das durch Zerfall des Kaliumisotops K entstanden ist. Die primordialen Isotope sind dagegen nur in geringen Mengen vorhanden.\n\nDa das Argon durch den Kaliumzerfall in der Erdkruste entsteht, findet man es auch in Gesteinen. Beim Schmelzen von Gesteinen im Erdmantel gast das Argon, aber auch das bei anderen Zerfällen entstehende Helium aus. Es reichert sich daher vorwiegend in den Basalten der ozeanischen Erdkruste an. Aus den Gesteinen wird das Argon an das Grundwasser abgegeben. Daher ist in Quellwasser, vor allem wenn es aus größerer Tiefe kommt, Argon gelöst.\n\n\nGewinnung und Darstellung.\nDie Gewinnung des reinen Argons erfolgt ausschließlich aus der Luft, in der Regel im Rahmen der Luftverflüssigung im Linde-Verfahren. Das Argon wird dabei nicht in der Haupt-Rektifikationskolonne des Verfahrens von den Hauptluftbestandteilen getrennt, sondern in einer eigenen Argon-Kolonne. In dieser wird durch Rektifikation zunächst Rohargon hergestellt, das noch etwa 3–5 % Sauerstoff und 1 % Stickstoff enthält.\n\nAnschließend wird das Rohargon in weiteren Stufen gereinigt. Das Gasgemisch wird zunächst auf Raumtemperatur erwärmt und auf 4–6&nbsp;bar verdichtet. Um den restlichen Sauerstoff zu entfernen, wird danach Wasserstoff eingespritzt, der an Edelmetall-Katalysatoren mit dem Sauerstoff zu Wasser reagiert. Nachdem dieses entfernt wurde, wird in einer weiteren Kolonne das Argon, das sich am unteren Ende der Kolonne anreichert, vom restlichen Stickstoff getrennt, so dass Argon mit einer Reinheit von 99,9999 % (Argon 6.0) produziert werden kann.\n\nWeitere Quellen für die Gewinnung von Argon sind die Produktion von Ammoniak im Haber-Bosch-Verfahren sowie die Synthesegasherstellung, etwa zur Methanolproduktion. Bei diesen Verfahren, die Luft als Ausgangsstoff nutzen, reichern sich Argon und andere Edelgase im Produktionsprozess an und können aus dem Gasgemisch isoliert werden. Wie beim Linde-Verfahren werden auch hier die verschiedenen Gase durch Adsorption oder Rektifikation voneinander getrennt und so reines Argon gewonnen.\n\n\nEigenschaften.\n\nPhysikalische Eigenschaften.\nArgon ist bei Normalbedingungen ein einatomiges, farbloses und geruchloses Gas, das bei 87,15&nbsp;K (−186&nbsp;°C) kondensiert und bei 83,8&nbsp;K (−189,3&nbsp;°C) erstarrt. Wie die anderen Edelgase außer dem Helium kristallisiert Argon in einer kubisch dichtesten Kugelpackung mit dem Gitterparameter \"a\"&nbsp;=&nbsp;526&nbsp;pm bei 4&nbsp;K.\n\nWie alle Edelgase besitzt Argon nur abgeschlossene Schalen (Edelgaskonfiguration). Dadurch lässt sich erklären, dass das Gas stets einatomig vorliegt und die Reaktivität gering ist.\n\nMit einer Dichte von 1,784&nbsp;kg/m bei 0&nbsp;°C und 1013&nbsp;hPa ist Argon schwerer als Luft, es sinkt also ab. Im Phasendiagramm liegt der Tripelpunkt bei 83,8&nbsp;K und 689&nbsp;hPa, der kritische Punkt bei 150,86&nbsp;K, 4896&nbsp;kPa sowie einer kritischen Dichte von 0,536&nbsp;g/cm.\n\nIn Wasser ist Argon etwas löslich. In einem Liter Wasser können sich bei 0&nbsp;°C und Normaldruck maximal 53,6&nbsp;ml Argon lösen.\n\n\nChemische Eigenschaften.\nAls Edelgas reagiert Argon fast nicht mit anderen Elementen oder Verbindungen. Bislang ist nur das experimentell dargestellte Argonfluorohydrid HArF bekannt, das durch Photolyse von Fluorwasserstoff in einer Argonmatrix bei 7,5&nbsp;K gewonnen wird und anhand neuer Linien im Infrarotspektrum identifiziert wurde. Oberhalb von 27&nbsp;K zersetzt es sich. Nach Berechnungen sollten weitere Verbindungen des Argons metastabil sein und sich verhältnismäßig schwer zersetzen; diese konnten jedoch experimentell bislang nicht dargestellt werden. Beispiele hierfür sind das Chloranalogon des Argonfluorohydrides HArCl, aber auch Verbindungen, bei denen das Proton durch andere Gruppen ersetzt ist, etwa FArCCH als organische Argonverbindung und FArSiF mit einer Argon-Silicium-Bindung.\n\nArgon bildet einige Clathrate, in denen es physikalisch in Hohlräume eines umgebenden Kristalls eingeschlossen ist. Bei −183&nbsp;°C ist ein Argon-Hydrat stabil, jedoch ist die Geschwindigkeit der Bildung sehr langsam, da eine Umkristallisierung stattfinden muss. Ist das Eis mit Chloroform gemischt, bildet sich das Clathrat schon bei −78&nbsp;°C. Stabil ist auch ein Clathrat von Argon in Hydrochinon.\n\n\nIsotope.\nInsgesamt sind 23 Isotope sowie ein weiteres Kernisomer von Argon bekannt. Von diesen sind drei, nämlich die Isotope Ar, Ar und Ar, stabil und kommen in der Natur vor. Dabei überwiegt bei weitem Ar mit einem Anteil von 99,6 % am natürlichen irdischen Isotopengemisch. Ar und Ar sind mit einem Anteil von 0,34 % beziehungsweise 0,06 % selten. Von den instabilen Isotopen besitzen Ar mit 269&nbsp;Jahren und Ar mit 32,9&nbsp;Jahren die längsten Halbwertszeiten. Alle anderen Isotope besitzen kurze Halbwertszeiten im Bereich von unter 10 ps bei Ar bis 35,04 Tagen bei Ar.\n\nAr wird für die Altersbestimmung von Gesteinen genutzt (Kalium-Argon-Datierung). Dabei wird ausgenutzt, dass instabiles K, das in diesen enthalten ist, langsam zu Ar zerfällt. Je mehr Kalium zu Argon zerfallen ist, desto älter ist das Gestein. Das kurzlebige Isotop Ar kann zur Überprüfung von Gasleitungen verwendet werden. Durch das Durchleiten von Ar kann die Leistungsfähigkeit einer Belüftung oder Dichtigkeit einer Leitung festgestellt werden.\n\n\"→ Liste der Argon-Isotope\"\n\n\nBiologische Bedeutung.\nWie die anderen Edelgase hat Argon auf Grund der Reaktionsträgheit keine biologische Bedeutung und ist auch nicht toxisch. In höheren Konzentrationen wirkt es durch Verdrängung des Sauerstoffs erstickend. Bei Drücken von mehr als 24&nbsp;bar wirkt es narkotisierend.\n\n\nVerwendung.\nAls günstigstes und in großen Mengen verfügbares Edelgas wird Argon in vielen Bereichen verwendet. Die Produktion betrug 1998 weltweit etwa zwei Milliarden&nbsp;m³ bzw. zwei km³. Der größte Teil des Argons wird als Schutzgas verwendet. Es wird immer dann genutzt, wenn der billigere Stickstoff nicht anwendbar ist. Dazu zählen vor allem Schweißverfahren für Metalle, die mit Stickstoff bei hohen Temperaturen reagieren, etwa Titan, Tantal und Wolfram. Auch beim Metallinertgasschweißen und Wolfram-Inertgasschweißen, die etwa beim Schweißen von Aluminiumlegierungen oder hoch legierten Stählen angewendet werden, dient Argon als Inertgas. Weiterhin wird es in der Metallurgie als Schutzgas, etwa für die Produktion von Titan, hochreinem Silicium oder der Schmelzraffination sowie zum Entgasen von Metallschmelzen genutzt.\n\nArgon ist ein Lebensmittelzusatzstoff (E 938) und dient als Treib- und Schutzgas bei der Verpackung von Lebensmitteln und der Weinherstellung.\n\nArgon wird als gasförmiges Löschmittel vorwiegend für den Objektschutz, vor allem bei elektrischen und EDV-Anlagen eingesetzt und wirkt dabei durch Sauerstoffverdrängung. Für diesen Zweck wird reines Argon oder ein Gasgemisch zusammen mit Stickstoff verwendet.\n\nIn der Analytik wird Argon als Träger- und Schutzgas für die Gaschromatographie und das induktiv gekoppelte Plasma (ICP-MS, ICP-OES) verwendet.\n\nGlühlampen werden häufig mit Argon-Stickstoff-Gemischen gefüllt, weil eine Gasfüllung die Sublimation des Glühfadens vermindert. Argon hat dabei eine geringere Wärmeleitfähigkeit als leichtere Gase, ist aber preiswerter als andere schwerere und damit noch geringer wärmeleitende Gase wie Krypton oder Xenon. Ein Vorteil der geringeren Wärmeleitfähigkeit ist eine höhere mögliche Glühtemperatur und damit höhere Lichtausbeute. Ebenfalls wegen der geringen Wärmeleitfähigkeit wird es als Füllgas für Isolierglasscheiben verwendet. Auch in Gasentladungslampen dient Argon als Leuchtgas mit einer typischen violetten Farbe. Wird etwas Quecksilber dazugegeben, ändert sich die Farbe ins Blaue. Weiterhin ist Argon das Lasermedium in Argon-Ionen-Lasern.\n\nIm Bereich der Stahlerzeugung kommt Argon eine besonders wichtige Rolle im Bereich der Sekundärmetallurgie zu. Mit der Argon-Spülung kann die Stahllegierung entgast und gleichzeitig homogenisiert werden, speziell wird dabei der unerwünschte, gelöste Stickstoff aus der Schmelze entfernt.\n\nBeim Tauchen wird Argon – insbesondere bei der Nutzung des Helium&shy;haltigen Trimix als Atemgas – dazu verwendet, um Trockentauchanzüge zu füllen bzw. damit zu tarieren. Hierbei wird ebenfalls die geringe Wärmeleitfähigkeit des Gases genutzt, um das Auskühlen des Anzugträgers zu verzögern.\n\nSeit Mai 2014 ist Argon auf der Dopingliste der Welt-Anti-Doping-Agentur (WADA). Durch den bei der Inhalation von Argon entstehenden Sauerstoffmangel wird offensichtlich die Bildung von körpereigenem Erythropoetin (EPO) aktiviert. Aus demselben Grund ist auch Xenon auf der Dopingliste.\n\n"}
{"id": "97", "url": "https://de.wikipedia.org/wiki?curid=97", "title": "Arsen", "text": "Arsen\n\nArsen [] ist ein chemisches Element mit dem Elementsymbol As und der Ordnungszahl 33. Im Periodensystem der Elemente steht es in der 4. Periode und der 5. Hauptgruppe, bzw. 15.&nbsp;IUPAC-Gruppe oder Stickstoffgruppe. Arsen kommt selten gediegen vor, meistens in Form von Sulfiden. Es gehört zu den Halbmetallen, da es je nach Modifikation metallische oder nichtmetallische Eigenschaften zeigt.\n\nUmgangssprachlich wird auch das als Mordgift bekannte Arsenik meist einfach „Arsen“ genannt. Arsenverbindungen kennt man schon seit dem Altertum. Als mutagenes Klastogen können Arsenverbindungen als Gift wirken, welches Chromosomenaberrationen hervorrufen und somit karzinogene Wirkung besitzen kann.\n\nArsen wird zur Dotierung von Halbleitern und als Bestandteil von III-V-Halbleitern wie Galliumarsenid genutzt. Die organische Arsenverbindung Arsphenamin (\"Salvarsan\") galt trotz schwerer und schwerster Nebenwirkungen Anfang des 20. Jahrhunderts als Durchbruch in der Behandlung der Syphilis. Heute wird Arsentrioxid als letzte Behandlungsoption in der Therapie der Promyelozytenleukämie angewendet.\n\n\nGeschichte.\nDer Name Arsen geht auf zurück, der antiken Bezeichnung des Arsenminerals Auripigment. Sie findet sich schon bei Dioskurides im 1.&nbsp;Jahrhundert. Die griechische Bezeichnung scheint ihrerseits ihren Ursprung im Altpersischen \"(al-)zarnik\" (goldfarben, Auripigment, „Arsen“) zu haben und gelangte wohl durch semitische Vermittlung ins Griechische. Volksetymologisch wurde der Name fälschlicherweise vom gleichlautenden (alt- und neu-)griechischen Wort abgeleitet, das sich etwa mit männlich/stark übersetzen lässt. Erst seit dem 19.&nbsp;Jahrhundert ist die Bezeichnung Arsen gebräuchlich. Das Elementsymbol wurde 1814 von Jöns Jakob Berzelius vorgeschlagen.\n\nDer erste Kontakt von Menschen mit Arsen lässt sich aus dem 3.&nbsp;Jahrtausend v. Chr. nachweisen: In den Haaren der im Gletschereis erhaltenen Mumie des volkstümlich Ötzi genannten Alpenbewohners ließen sich größere Mengen Arsen nachweisen, was archäologisch als Hinweis darauf gedeutet wird, dass der betroffene Mann in der Kupferverarbeitung tätig war – Kupfererze sind oft mit Arsen verunreinigt. Im klassischen Altertum war Arsen in Form der Arsen-Sulfide Auripigment (AsS) und Realgar (AsS) bekannt, die etwa von dem Griechen Theophrastos, dem Nachfolger Aristoteles, beschrieben wurden. Auch der griechische Philosoph Demokrit hatte im 5. Jahrhundert v. Chr. nachweislich Kenntnisse über Arsenverbindungen. Der Leidener Papyrus X aus dem 3.&nbsp;Jahrhundert nach Chr. lässt darauf schließen, dass sie benutzt wurden, um Silber goldartig und Kupfer weiß zu färben. Der römische Kaiser Caligula hatte angeblich bereits im 1.&nbsp;Jahrhundert nach Chr. ein Projekt zur Herstellung von Gold aus dem (goldgelben) Auripigment in Auftrag gegeben. Die Alchimisten, die Arsen-Verbindungen nachweislich der Erwähnung im antiken Standardwerk \"Physica et Mystica\" kannten, vermuteten eine Verwandtschaft mit Schwefel und Quecksilber. Arsen(III)-sulfid kam als Malerfarbe und Enthaarungsmittel zum Einsatz sowie zur äußerlichen als auch inneren Behandlung von Lungenkrankheiten.\n\nIm Mittelalter wurde Arsenik (Arsen(III)-oxid) im Hüttenrauch (staubbeladenes Abgas metallurgischer Öfen) gefunden. Albertus Magnus beschrieb um 1250 erstmals die Herstellung von Arsen durch Reduktion von Arsenik mit Kohle. Er gilt daher als Entdecker des Elements, auch wenn es Hinweise darauf gibt, dass das elementare Metall schon früher hergestellt wurde. Paracelsus führte es im 16. Jahrhundert in die Heilkunde ein. Etwa zur gleichen Zeit wurden Arsenpräparate in der chinesischen Enzyklopädie \"Pen-ts'ao Kang-mu\" des Apothekers Li Shi-zhen beschrieben. Dieser Autor hebt insbesondere die Anwendung als Pestizid in Reisfeldern hervor.\n\nIm 17.&nbsp;Jahrhundert wurde das gelbe Auripigment bei niederländischen Malern als \"Königsgelb\" populär. Da sich das Pigment über längere Zeiträume hinweg in Arsen(III)-oxid umwandelt und von der Leinwand bröckelt, entstehen Schwierigkeiten bei der Restaurierung. Ab 1740 wurden Arsenpräparate in Europa mit Erfolg als Beizmittel im Pflanzenschutz eingesetzt. Diese Nutzung verbot man jedoch 1808 wegen ihrer hohen Giftigkeit wieder. Der Einsatz von Arsenzusätzen für den Bleiguss beruht auf der größeren Härte solcher Bleilegierungen, typische Anwendung sind Schrotkugeln. Obwohl die Giftigkeit und die Verwendung als Mordgift bekannt war, ist Arsen im beginnenden 19.&nbsp;Jahrhundert eines der bedeutendsten Asthmamittel. Grundlage sind anscheinend Berichte, in denen den Chinesen nachgesagt wurde, sie würden Arsen in Kombination mit Tabak rauchen, um Lungen zu bekommen, die stark wie Blasebälge seien. Ebenfalls bis ins 19. Jahrhundert fanden Arsenverbindungen äußerlich und innerliche Anwendungen bei bösartigen Geschwülsten, Hauterkrankungen und (etwa in Form der Fowlerschen Tropfen) bei Fieber.\n\nArsen wurde in Form von Kupferarsenaten in Farbmitteln wie dem Pariser Grün eingesetzt, um Tapeten zu bedrucken. Bei hoher Feuchtigkeit wurden diese Pigmente durch Schimmelpilzbefall in giftige flüchtige Arsenverbindungen umgewandelt, die nicht selten zu chronischen Arsenvergiftungen führten.\n\nDoch auch in Kriegen fand Arsen Verwendung: Im Ersten Weltkrieg wurden Arsenverbindungen in chemischen Kampfstoffen (Blaukreuz) oder Lewisit eingesetzt. Bei den Opfern bewirkten sie durch Angriff auf Haut und Lungen grausame Schmerzen und schwerste körperliche Schädigungen.\n\n\nVorkommen.\nArsen kommt in geringen Konzentrationen von bis zu 10&nbsp;ppm praktisch überall im Boden vor. Es ist in der Erdkruste ungefähr so häufig wie Uran oder Germanium. In der kontinentalen Erdkruste kommt Arsen mit durchschnittlich 1,7&nbsp;ppm vor, wobei es durch seinen lithophilen Charakter (= Silikat liebend) in der oberen Kruste angereichert ist (2&nbsp;ppm gegenüber 1,3&nbsp;ppm in der unteren Kruste); damit liegt Arsen in der Tabelle der häufigsten Elemente an 53.&nbsp;Stelle.\n\nArsen (\"Scherbenkobalt\") kommt in der Natur gediegen, das heißt in elementarer Form, vor und ist daher von der International Mineralogical Association (IMA) als eigenständiges Mineral anerkannt. Gemäß der Systematik der Minerale nach Strunz (9. Auflage) wird Arsen unter der System-Nr. 1.CA.05 (Elemente – Halbmetalle (Metalloide) und Nichtmetalle – Arsengruppen-Elemente) (8. Auflage: \"I/B.01-10\") eingeordnet. Die im englischsprachigen Raum ebenfalls geläufige Systematik der Minerale nach Dana führt das Element-Mineral unter der System-Nr. 01.03.01.01.\n\nWeltweit sind zurzeit (Stand: 2011) rund 330 Fundorte für gediegenes Arsen bekannt. In Deutschland wurde es an mehreren Fundstätten im Schwarzwald (Baden-Württemberg), im bayerischen Spessart und Oberpfälzer Wald, im hessischen Odenwald, in den Silberlagerstätten des Westerzgebirges (Sachsen), am Hunsrück (Rheinland-Pfalz) sowie im Thüringer Wald gefunden. In Österreich trat Arsen an mehreren Fundstätten in Kärnten, Salzburg und der Steiermark zutage. In der Schweiz fand sich gediegen Arsen in den Kantonen Aargau und Wallis.\n\nWeitere Fundorte sind in Australien, Belgien, Bolivien, Bulgarien, Chile, China, Finnland, Frankreich, Griechenland, Irland, Italien, Japan, Kanada, Kasachstan, Kirgisistan, Madagaskar, Malaysia, Marokko, Mexiko, Mongolei, Neuseeland, Norwegen, Österreich, Peru, Polen, Rumänien, Russland, Schweden, Slowakei, Spanien, Tschechien, Ukraine, Ungarn, im Vereinigten Königreich (Großbritannien) und in den Vereinigten Staaten (USA) bekannt.\n\nWeit häufiger kommt das Element allerdings in verschiedenen intermetallischen Verbindungen mit Antimon (Allemontit) und Kupfer (Whitneyit) sowie in verschiedenen Mineralen vor, die überwiegend der Klasse der Sulfide und Sulfosalze angehören. Insgesamt sind bisher (Stand: 2011) 565 Arsenminerale bekannt. Die höchsten Konzentrationen an Arsen enthalten dabei unter anderem die Minerale Duranusit (ca. 90 %), Skutterudit und Arsenolith (jeweils ca. 76 %), die allerdings selten zu finden sind. Weit verbreitet sind dagegen Arsenopyrit (\"Arsenkies\"), Löllingit, Realgar (\"Rauschrot\") und Auripigment (\"Orpiment\", \"Rauschgelb\"). Weitere bekannte Minerale sind Cobaltit (\"Kobaltglanz\"), Domeykit (\"Arsenkupfer\"), Enargit, Gersdorffit (\"Nickelarsenkies\"), Proustit (\"Lichtes Rotgültigerz\", \"Rubinblende\"), Rammelsbergit sowie Safflorit und Sperrylith.\n\nArsenate finden sich häufig in phosphathaltigen Gesteinen, da sie eine vergleichbare Löslichkeit aufweisen und das häufigste Sulfidmineral Pyrit kann bis zu einigen Massenprozent Arsen einbauen.\n\nArsen wird heutzutage als Nebenprodukt der Verhüttung von Gold-, Silber-, Zinn-, Kupfer-, Cobalt- und weiteren Buntmetallerzen sowie bei der Verarbeitung von Phosphatrohstoffen gewonnen. Die größten Produzenten im Jahr 2009 waren China, Chile, Marokko und Peru. Arsen ist nur schwer wasserlöslich und findet sich daher nur in geringen Spuren, etwa 1,6&nbsp;ppb (Milliardstel Massenanteilen) in Meeren und Ozeanen.\n\nIn der Luft findet man Arsen in Form von partikulärem Arsen(III)-oxid. Als natürliche Ursache dafür hat man Vulkanausbrüche identifiziert, die insgesamt jährlich geschätzte 3000 Tonnen in die Erdatmosphäre eintragen. Bakterien setzen weitere 20.000 Tonnen in Form organischer Arsenverbindungen wie Trimethylarsin frei. Ein großer Teil am freigesetzten Arsen entstammt der Verbrennung fossiler Brennstoffe wie Kohle oder Erdöl. Die geschätzten Emissionen, verursacht durch den Straßenverkehr und stationäre Quellen, betrugen 1990 in der Bundesrepublik Deutschland 120 Tonnen (20 Tonnen in den alten, 100 Tonnen in den neuen Bundesländern). Die Außenluftkonzentration von Arsen liegt zwischen 0,5 und 15 Nanogramm pro Kubikmeter.\n\n\nGewinnung und Darstellung.\nArsen fällt in größeren Mengen als Nebenprodukt bei der Gewinnung von Kupfer, Blei, Cobalt und Gold an. Dies ist die Hauptquelle für die kommerzielle Nutzung des Elements.\n\nEs kann durch thermische Reduktion von Arsen(III)-oxid mit Koks oder Eisen und durch Erhitzen von Arsenkies (FeAsS) oder Arsenikalkies (FeAs) unter Luftabschluss in liegenden Tonröhren gewonnen werden. Dabei sublimiert elementares Arsen, das an kalten Oberflächen wieder in den festen Aggregatzustand zurückkehrt.\n\nFür die Halbleitertechnik wird Arsen, dessen Reinheit über 99,99999 Prozent betragen muss, durch Reduktion von mehrfach destilliertem Arsen(III)-chlorid im Wasserstoffstrom hergestellt:\n\nFrüher wurde es auch durch Sublimation aus Lösungen in flüssigem Blei erzeugt. Dabei wird der Schwefel der Arsen-Erze durch das Blei in Form von Blei(II)-sulfid gebunden. Die hierbei erzielten Reinheiten von über 99,999 Prozent waren für Halbleiteranwendungen nicht ausreichend. Eine andere Möglichkeit besteht im Auskristallisieren bei hohen Temperaturen aus geschmolzenem Arsen oder in der Umwandlung in Monoarsan, einer anschließenden Reinigung sowie der Zersetzung bei 600&nbsp;°C in Arsen und Wasserstoff.\n\n\nEigenschaften.\nArsen bildet mit Stickstoff, Phosphor, Antimon und Bismut die 5. Hauptgruppe des Periodensystems und nimmt wegen seiner physikalischen und chemischen Eigenschaften den Mittelplatz in dieser Elementgruppe ein. Arsen hat eine relative Atommasse von 74,92159. Der Radius des Arsen-Atoms beträgt 124,5 Pikometer. In kovalent gebundenem Zustand ist er etwas kleiner (121 Pikometer). Aufgrund der Abgabe der äußeren Elektronen (Valenzelektronen) bei der Ionisierung reduziert sich der Radius beträchtlich auf 34 Pikometer (As; das äußerste p- und das äußerste s-Atomorbital bleiben unbesetzt) beziehungsweise 58 Pikometer (As; nur das p-Orbital ist unbesetzt). In chemischen Komplexverbindungen ist das As-Kation von vier Bindungspartnern (Liganden), As von sechs umgeben. Arsen tritt allerdings nur sehr selten in eindeutig ionischer Form auf.\n\nDer Wert für die Elektronegativität liegt nach Pauling auf der von 0 (Metalle) bis 4 (Nichtmetall) reichenden Skala bei 2,18 und ist damit mit dem Wert des Gruppennachbarn Phosphor vergleichbar. Der Halbmetall-Charakter des Arsens zeigt sich zudem darin, dass die benötigte Dissoziationsenergie von 302,7&nbsp;kJ/mol, also die Energie, die aufgebracht werden muss, um ein einzelnes Arsen-Atom aus einem Arsen-Festkörper herauszulösen, zwischen der des Nichtmetalls Stickstoff (473,02&nbsp;kJ/mol; kovalente Bindung) und des Metalls Bismut (207,2&nbsp;kJ/mol; metallische Bindung) liegt. Unter Normaldruck sublimiert Arsen bei einer Temperatur von 613&nbsp;°C, geht also aus dem festen Aggregatzustand direkt in die Gasphase über. Arsendampf ist zitronengelb und setzt sich bis ungefähr 800&nbsp;°C aus As-Molekülen zusammen. Oberhalb von 1700&nbsp;°C liegen As-Moleküle vor.\n\nArsen zeigt je nach Verbindungspartner Oxidationsstufen zwischen −3 und +5. Mit elektropositiven Elementen wie Wasserstoff oder Metallen bildet es Verbindungen, in denen es eine Oxidationsstufe von −3 einnimmt. Beispiele dafür sind Monoarsan (AsH) und Arsenkupfer (CuAs). In Verbindungen mit elektronegativen Elementen wie den Nichtmetallen Sauerstoff, Schwefel und Chlor besitzt es die Oxidationsstufe +3 oder +5; erstere ist dabei gegenüber den in derselben Hauptgruppe stehenden Elementen Stickstoff und Phosphor tendenziell bevorzugt.\n\n\nModifikationen.\nArsen kommt wie andere Elemente der Stickstoffgruppe in verschiedenen allotropen Modifikationen vor. Anders als beim Stickstoff, der in Form zweiatomiger Moleküle mit kovalenter Dreifachbindung vorkommt, sind die entsprechenden As-Moleküle instabil und Arsen bildet stattdessen kovalente Netzwerke aus.\n\n\nGraues Arsen.\nGraues oder metallisches Arsen ist die stabilste Form. Es hat eine Dichte von 5,73&nbsp;g/cm. Seine Kristalle sind stahlgrau, metallisch glänzend und leiten den elektrischen Strom.\n\nBetrachtet man den strukturellen Aufbau des grauen Arsens, dann erkennt man Schichten aus gewellten Arsen-Sechsringen, welche die Sesselkonformation einnehmen. Darin bilden die Arsen-Atome eine Doppelschicht, wenn man sich den Aufbau der Schicht im Querschnitt ansieht. Die Übereinanderlagerung dieser Doppelschichten ist sehr kompakt. Bestimmte Atome der nächsten darüberliegenden oder darunterliegenden Schicht sind von einem Bezugsatom fast ähnlich weit entfernt wie innerhalb der betrachteten Doppelschicht. Dieser Aufbau bewirkt, dass die graue Arsen-Modifikation wie die homologen Elemente Antimon und Bismut sehr spröde ist. Deswegen werden diese drei Elemente häufig auch als Sprödmetalle bezeichnet.\n\n\nGelbes Arsen.\nWird Arsen-Dampf, in dem Arsen gewöhnlich als As-Tetraeder vorliegt, schnell abgekühlt, so bildet sich das metastabile gelbe Arsen mit einer Dichte von 1,97&nbsp;g/cm. Es besteht ebenfalls aus tetraedrischen As-Molekülen. Gelbes Arsen ist ein Nichtmetall und leitet infolgedessen den elektrischen Strom nicht. Es kristallisiert aus Schwefelkohlenstoff und bildet kubische, stark lichtbrechende Kristalle, die nach Knoblauch riechen. Bei Raumtemperatur und besonders schnell unter Lichteinwirkung wandelt sich gelbes Arsen in graues Arsen um.\n\n\nSchwarzes Arsen.\nSchwarzes Arsen selbst kann seinerseits in zwei verschiedenen Formen vorkommen. \"Amorphes schwarzes Arsen\" entsteht durch Abkühlung von Arsen-Dampf an 100 bis 200&nbsp;°C warmen Oberflächen. Es besitzt keine geordnete Struktur, sondern liegt in einer amorphen, glasartigen Form vor, analog zum roten Phosphor. Die Dichte beträgt 4,7 bis 5,1&nbsp;g/cm. Oberhalb 270&nbsp;°C wandelt sich das schwarze Arsen in die graue Modifikation um. Wird glasartiges, amorphes schwarzes Arsen bei Anwesenheit von metallischem Quecksilber auf 100 bis 175&nbsp;°C erhitzt, so entsteht das metastabile \"orthorhombische schwarze Arsen\", das mit dem schwarzen Phosphor vergleichbar ist.\n\nNatürlich gebildetes orthorhombisches schwarzes Arsen ist in der Natur als seltenes Mineral Arsenolamprit bekannt.\n\n\nBraunes Arsen.\nBei der Reduktion von Arsenverbindungen in wässriger Lösung entstehen ähnlich wie beim Phosphor Mischpolymerisate. Bei diesen bindet ein Teil der freien Valenzen des Arsens Hydroxygruppen&nbsp;(–OH). Man nennt diese Form des Arsens \"braunes Arsen\".\n\n\nReaktionen.\nArsen reagiert heftig mit Oxidationsmitteln und Halogenen. So verbrennt Arsen an der Luft mit bläulicher Flamme zu einem weißen Rauch von giftigem Arsen(III)-oxid.\n\nOhne äußere Wärmezufuhr findet die Reaktion mit Chlor unter Feuererscheinung zu Arsen(III)-chlorid statt.\n\nEine weitere Oxidation ist möglich.\n\nAnaloge Reaktionsgleichungen gelten für die entsprechenden Reaktionen mit Fluor.\nStark oxidierende Säuren, wie konzentrierte Salpetersäure oder Königswasser, wandeln Arsen in Arsensäure um.\n\nIst die Oxidationsstärke weniger groß – etwa bei Verwendung von verdünnter Salpetersäure oder Schwefelsäure – entsteht Arsenige Säure.\n\nUnter sauren Bedingungen und bei Anwesenheit von nichtpassivierten unedlen Metallen, insbesondere Zink, reagiert Arsen mit dem gebildeten Wasserstoff zu Monoarsan.\n\nMit basischem Natriumhydroxid bildet sich das entsprechende Arsenitsalz.\n\n\nIsotope.\nVom Arsen sind künstlich hergestellte, radioaktive Isotope mit Massenzahlen zwischen 65 und 87 bekannt. Die Halbwertszeiten liegen zwischen 96 Millisekunden (As) und 80,3 Tagen (As). Natürlich vorkommendes Arsen besteht zu 100 Prozent aus dem Isotop As, es ist daher ein anisotopes Element. Der entsprechende Arsen-Kern besteht also aus genau 33 Protonen und 42 Neutronen. Physikalisch zählt man ihn daher zu den ug-Kernen (u steht hier für ungerade, g für gerade). Sein Kernspin beträgt 3/2.\n\n\nVerwendung.\nArsen wird Bleilegierungen zugesetzt, um ihre Festigkeit zu verbessern und das Blei gießbar zu machen. Vor allem die fein strukturierten Platten von Akkumulatoren könnten ohne Arsen nicht gegossen werden. Historisch war Arsen eine wichtige Zutat von Kupferlegierungen, die dadurch besser verarbeitbar wurden. Metallisches Arsen wurde früher gelegentlich zur Erzeugung mattgrauer Oberflächen auf Metallteilen verwendet, um eine Alterung vorzutäuschen.\n\nIn der Elektronik spielt es als mindestens 99,9999 Prozent reines Element für Gallium-Arsenid-Halbleiter, sogenannte III-V-Halbleiter (aufgrund der Kombination von Elementen aus der 3. und 5. Hauptgruppe des Periodensystems), sowie für Epitaxieschichten auf Wafern in Form von Indiumarsenidphosphid und Galliumarsenidphosphid eine wesentliche Rolle in der Herstellung von Hochfrequenzbauelementen wie Integrierten Schaltkreisen (ICs), Leuchtdioden (LEDs) beziehungsweise Laserdioden (LDs). Es gab Anfang 2004 weltweit nur drei Hersteller von hochreinem Arsen, zwei in Deutschland und einen in Japan.\n\nArsen wird in Form seiner Verbindungen in einigen Ländern als Schädlingsbekämpfungsmittel im Weinbau, als Fungizid (Antipilzmittel) in der Holzwirtschaft, als Holzschutzmittel, als Rattengift und als Entfärbungsmittel in der Glasherstellung verwendet. Der Einsatz ist umstritten, da die eingesetzten Arsenverbindungen (hauptsächlich Arsen(III)-oxid) giftig sind.\n\n\nArsen in Arzneimitteln.\nDie Verwendung arsenhaltiger Mineralien als Heilmittel ist bereits in der Antike durch Hippokrates und Plinius bezeugt. Sie wurden als Fiebermittel, als Stärkungsmittel und zur Therapie von Migräne, Rheumatismus, Malaria, Tuberkulose und Diabetes eingesetzt. Im 18.&nbsp;Jahrhundert wurde eine Mischung aus Kaliumarsenit und Lavendelwasser als Fowler’sche Lösung bekannt, die lange als medizinisches Wundermittel galt und als Fiebersenker, Heilwasser und sogar als Aphrodisiakum Anwendung fand. Kaliumarsenit war als Bestandteil der Fowler’schen Lösung bis in die 1960er Jahre in Deutschland als Mittel zur Behandlung der Psoriasis im Einsatz.\n\nConstantinus Africanus (1017–1087) empfahl eine Arsenapplikation zur Bekämpfung von Zahnschmerzen. Bereits um 2700 vor Christus soll die Anwendung von Arsen zur Behandlung eines schmerzenden Zahnes in der chinesischen Heilkunst beschrieben worden sein. In dem Mitte des 10. Jahrhunderts erschienenen Werk „Liber Regius“ empfahl der arabische Arzt Haly Abbas (ʿAli ibn al-ʿAbbās; † 944) ebenfalls den Einsatz von Arsenik zur Devitalisation der Pulpa. Arsen(III)-oxid wurde bis in die Neuzeit zur Devitalisation der Zahnpulpa verwendet und verschwand in den 1970er Jahren wegen der krebserregenden Wirkung, Entzündungen des Zahnhalteapparates, des Verlustes eines oder mehrerer Zähne einschließlich Nekrosen des umliegenden Alveolarknochens, Allergien und Vergiftungserscheinungen aus dem Therapiespektrum.\n\nEinen Aufschwung erlebten arsenhaltige bzw. Arsenverbindungen enthaltende Arzneimittel zu Beginn des 20.&nbsp;Jahrhunderts. Harold Wolferstan Thomas und Anton Breinl konnten 1905 beobachten, dass das arsenhaltige Präparat Atoxyl Trypanosomen, zu denen die Erreger der Schlafkrankheit gehören, abtötet. 1920 wurde eine Weiterentwicklung, das Tryparsamid, in der Zeit von 1922 bis 1970 im tropischen Afrika zur Therapie der Schlafkrankheit eingesetzt. Es war bedeutsam für die Eingrenzung dieser Epidemie in der ersten Hälfte des vorigen Jahrhunderts, konnte jedoch zur Erblindung führen. Das in den 1950er Jahren entwickelte Melarsoprol war über mehrere Jahrzehnte das Mittel der Wahl zur Behandlung der Schlafkrankheit und wird heute noch eingesetzt, da keine effektiven Nachfolgepräparate zur Verfügung stehen.\n\nEbenfalls angeregt durch die Trypanosomen-toxische Wirkung von Atoxyl entwickelte Paul Ehrlich das arsenhaltige Arsphenamin (Salvarsan). Das 1910 in die Therapie der Syphilis eingeführte Mittel stellte das erste auf theoretischen Vorüberlegungen beruhende, systematisch entwickelte, spezifisch wirkende Chemotherapeutikum dar und war Vorbild für die Entwicklung der bis heute verwendeten Sulfonamide. Es wurde lange Zeit auch bei der Behandlung von Dysenterie eingesetzt.\n\nIm Jahr 2000 wurde ein arsenikhaltiges Präparat unter dem Namen \"Trisenox\" in den USA zur Behandlung der akuten Promyelozytenleukämie (APL) zugelassen. Seit 2002 besteht für \"Trisenox\" in Europa eine Zulassung zur Behandlung der APL, (Vertrieb in EU und USA: Cephalon). Seine Wirksamkeit bei der Krebstherapie wird auch auf die antiangioneogenetische Wirkung zurückgeführt.\n\nDie verschiedenen Arsensulfide sind Bestandteil von Arzneimitteln der Chinesischen Medizin.\n\n\nArsenik als Insektizid bei der Taxidermie.\nAufgrund der toxischen Eigenschaften von Arsenverbindungen wurde früher überwiegend Arsenik zur Haltbarmachung von Wirbeltieren (\"Taxidermie\") als Insektizid verwendet. Viele andere Stoffe, wie auch Lindan, wurden zum selben Zweck verwendet, wie es die Fachliteratur der Präparatoren aus der Zeit von 1868 bis 1996 beschreibt. Solche Stoffe sind jedoch auch für Menschen giftig und stellen heute an Präparatoren besondere Anforderungen, da diese auch in Kontakt mit derart kontaminierten Präparaten kommen.\n\n\nBiologische Bedeutung.\nDie biologische Bedeutung des Arsens für den Menschen ist nicht vollständig geklärt. Es gilt als Spurenelement im Menschen, Mangelerscheinungen wurden bisher aber nur an Tieren nachgewiesen. Der notwendige Bedarf liegt, falls er bestehen sollte, zwischen 5 und 50&nbsp;µg pro Tag. Eine tägliche Arsenaufnahme von – je nach Wahl der Nahrungsmittel – bis zu einem Milligramm gilt als harmlos. In einer neuen Studie konnte eine erhöhte Arsenbelastung durch hohe Arsengehalte im Grundwasser von Reisanbaugebieten mit der Entstehung von Krebserkrankungen in Verbindung gebracht werden. Die Förderung der Krebsentwicklung ist jedoch dosisabhängig und nur bei Verzehr von belastetem Reis als täglichem Grundnahrungsmittel gegeben. Es gibt bei regelmäßigem Verzehr von Arsenverbindungen, speziell Arsentrioxid eine Gewöhnung, die beim Absetzen der Dosis sogar von Entzugserscheinungen begleitet werden.\n\nMenschen, die wegen der stimulierenden Allgemeinwirkung früher häufig Arsenik konsumierten (vor allem in der Steiermark), wobei häufig Gewöhnung und Sucht eintrat, werden Arsenikesser genannt.\n\nMeerestiere wie Muscheln oder Garnelen enthalten besonders viel Arsen, letztere bis zu 175&nbsp;ppm. Vermutlich agiert es durch die Bindung an freie Thiolgruppen in Enzymen als Inhibitor, verhindert also deren Wirkung.\n\nFür viele Tiere ist Arsen ein essentielles Spurenelement. So zeigen Hühner oder Ratten bei arsenfreier Ernährung deutliche Wachstumsstörungen; dies hängt wahrscheinlich mit dem Einfluss des Elements auf die Verstoffwechslung der Aminosäure Arginin zusammen. Zahlreiche Algen und Krebstiere enthalten organische Arsen-Verbindungen wie das schon erwähnte Arsenobetain. Arsen führt zur verstärkten Bildung der sauerstofftransportierenden roten Blutkörperchen. Aus diesem Grund wurde es früher dem Futter von Geflügel und Schweinen zugesetzt, um eine schnellere Mästung zu ermöglichen. Trainer von Rennpferden benutzten es zum illegalen Doping ihrer Tiere – heute kann der Zusatz von Arsen zur Nahrung allerdings leicht im Urin nachgewiesen werden.\n\nLösliche Arsenverbindungen werden leicht über den Magen-Darm-Trakt aufgenommen und rasch innerhalb von 24 Stunden im Körper verteilt. Man findet den größten Teil des aufgenommenen Arsens in den Muskeln, Knochen, Nieren und Lungen. Im Menschen wurde es zusammen mit Thallium in fast jedem Organ nachgewiesen. Blut enthält bis zu 8&nbsp;ppb Arsen, in den anderen Organen des Körpers wie etwa den Knochen hat es einen Anteil von zwischen 0,1 und 1,5 ppm, in Haaren liegt der Anteil bei etwa 1 ppm. Der Gesamtgehalt von Arsen im Körper eines Erwachsenen liegt im Durchschnitt bei etwa 7 Milligramm.\n\nOrganische Arsenverbindungen wie die aus Fischen und Meeresfrüchten stammende Dimethylarsinsäure, Trimethylarsenoxid, Trimethylarsin sowie Arsenobetain verlassen den menschlichen Körper fast unverändert innerhalb von zwei bis drei Tagen über die Nieren. Anorganische Arsenverbindungen werden in der Leber zu Monomethylarsonsäure (MMAA) und Dimethylarsinsäure (DMAA) umgewandelt und anschließend ebenso über die Nieren ausgeschieden.\n\nBei Pflanzen erhöht das Element den Kohlenhydrat-Umsatz. Der Gebänderte Saumfarn \"(Pteris vittata)\" nimmt das Halbmetall bevorzugt aus dem Boden auf und kann bis zu fünf Prozent seines Trockengewichts an Arsen aufnehmen. Aus diesem Grund wird die schnellwachsende Pflanze zur biologischen Säuberung arsenkontaminierter Böden eingesetzt.\n\nDie stimulierende Wirkung des Arsens ist vermutlich auch Ursache des früher in einigen Alpengegenden verbreiteten Arsenikessens. Im 17. Jahrhundert verzehrten manche der dortigen Bewohner lebenslang zweimal wöchentlich bis zu 250 Milligramm Arsen – bei Männern, weil es bei der Arbeit in den Höhenlagen half, bei Frauen, da es angeblich zu einer kräftigen Gesichtsfarbe beitrug. In der Wissenschaft lange als Märchen abgetan, nahm ein Bauer aus den Steirischen Alpen 1875 vor der in Graz versammelten deutschen Fachwelt eine Dosis von 400 Milligramm Arsentrioxid zu sich, die sich später auch in seinem Urin nachweisen ließ. Die Dosis lag weit über dem Doppelten der für normale Menschen tödlichen Arsenmenge, zeigte aber keinerlei negative Auswirkungen auf den Bauern. Ähnliches wird von Bewohnern einer Siedlung in der hochgelegenen chilenischen Atacamawüste berichtet, deren Trinkwasser hochgradig mit Arsen belastet ist, die jedoch keinerlei Vergiftungssymptome zeigen. Heute geht man davon aus, dass eine langsame Gewöhnung an das Gift mit sukzessive steigenden Dosen physiologisch möglich ist.\n\nÜber den Bakterienstamm GFAJ-1 wurde 2010 berichtet, dass er unter bestimmten Bedingungen in arsenathaltigen Nährmedien in der Lage sei, Arsenat anstatt Phosphat in Biomoleküle wie die DNA einzubauen, ohne dabei abzusterben, was bisher eher als unmöglich galt. Der Befund scheint jedoch auf unsauberen Arbeitsmethoden zu basieren, die Befunde konnten nicht repliziert werden.\n\n\nSicherheitshinweise.\nArsen-Stäube sind leicht entzündlich.\n\nToxizität.\nDreiwertige lösliche Verbindungen des Arsens sind hoch toxisch, weil sie biochemische Prozesse wie die DNA-Reparatur, den zellulären Energiestoffwechsel, rezeptorvermittelte Transportvorgänge und die Signaltransduktion stören. Dabei kommt es mutmaßlich nicht zu einer direkten Einwirkung auf die DNA, sondern zu einer Verdrängung des Zink-Ions aus seiner Bindung zu Metallothioneinen und damit zur Inaktivierung von Tumorsupressorproteinen (siehe auch Zinkfingerprotein). Arsen(III)- und Zink(II)-Ionen haben vergleichbare Ionenradien und damit ähnliche Affinität zu diesen Zinkfingerproteinen, allerdings führt Arsen dann nicht zur Aktivierung der Tumorsupressorproteine.\n\nEine akute Arsenvergiftung führt zu Krämpfen, Übelkeit, Erbrechen, inneren Blutungen, Durchfall und Koliken, bis hin zu Nieren- und Kreislaufversagen. Bei schweren Vergiftungen fühlt sich die Haut feucht und kalt an und der Betroffene kann in ein Koma fallen. Die Einnahme von 60 bis 170 Milligramm Arsenik gilt für Menschen als tödliche Dosis (LD&nbsp;=&nbsp;1,4&nbsp;mg/kg&nbsp;Körpergewicht); meist tritt der Tod innerhalb von mehreren Stunden bis wenigen Tagen durch Nieren- und Herz-Kreislauf-Versagen ein. Eine chronische Arsenbelastung kann Krankheiten der Haut und Schäden an den Blutgefäßen hervorrufen, was zum Absterben der betroffenen Regionen (Black Foot Disease) sowie zu bösartigen Tumoren der Haut, Lunge, Leber und Harnblase führt. Diese Symptome wurden auch als Reichensteiner Krankheit bezeichnet, nach einem Ort in Schlesien, dessen Trinkwasser durch den Arsenik-Abbau bis zu 0,6&nbsp;mg Arsen pro Liter enthielt.\n\nDie chronische Arsen-Vergiftung führt über die Bindung an Sulfhydryl-Gruppen von Enzymen der Blutbildung (zum Beispiel Delta-Amino-Laevulin-Säure-Synthetase) zu einem initialen Abfall des Hämoglobins im Blut, was zu einer reaktiven Polyglobulie führt. Des Weiteren kommt es bei chronischer Einnahme von Arsen zur Substitution der Phosphor-Atome im Adenosin-Triphosphat (ATP) und damit zu einer Entkopplung der Atmungskette, was zu einer weiteren reaktiven Polyglobulie führt. Klinisch finden sich hier nach Jahren der As-Exposition Trommelschlägelfinger, Uhrglasnägel, Mees-Nagelbänder und Akrozyanose (Raynaud-Syndrom), mit Folge der \"Black Foot Disease\".\n\nMetallisches Arsen dagegen zeigt wegen seiner Unlöslichkeit nur eine geringe Giftigkeit, da es vom Körper kaum aufgenommen wird (LD = 763&nbsp;mg/kg Ratte, oral). Es sollte aber, da es sich an der Luft leicht mit seinen sehr giftigen Oxiden wie dem Arsenik überzieht, stets mit größter Vorsicht behandelt werden. Anders verhält es sich mit Arsenik, das in früheren Zeiten als Stimulans von Arsenikessern benutzt wurde, um einer Arsenvergiftung vorzubeugen. Der Mechanismus dieser Immunisierung gegen Arsen ist nicht bekannt.\n\n\nGrenzwerte.\nAnionisches Arsen tritt als Arsenit ([AsO]) und Arsenat ([AsO]) in vielen Ländern im Grundwasser in hohen Konzentrationen auf. Durch Auswaschungen aus arsenhaltigen Erzen in Form von drei- und fünfwertigen Ionen trinken weltweit über 100 Millionen Menschen belastetes Wasser. Besonders in Indien, Bangladesh und Thailand, wo im 20.&nbsp;Jahrhundert mit internationaler Unterstützung zahlreiche Brunnen gegraben wurden, um von mit Krankheitserregern kontaminiertem Oberflächenwasser auf Grundwasser ausweichen zu können, führte diese unerkannte Belastung des Trinkwassers zu chronischer Arsenvergiftung bei weiten Teilen der betroffenen Bevölkerung. Das Problem kann, wo es bekannt wird, chemisch durch Oxidation der Arsenverbindungen und nachfolgende Ausfällung mit Eisenionen behoben werden. Von der Rice University wurde eine kostengünstige Filtermöglichkeit mit Nano-Magnetit entwickelt.\n\nDie Weltgesundheitsorganisation (WHO) empfiehlt seit 1992 einen Grenzwert für Arsen im Trinkwasser von 10 Mikrogramm pro Liter. Der Wert wird in vielen Staaten Europas und in den USA immer noch überschritten. In Deutschland wird er dagegen seit 1996 eingehalten. Eine Richtlinie der Europäischen Union (EU) von 1999 schreibt einen Höchstwert von 10 Mikrogramm pro Liter Trinkwasser EU-weit vor. Die USA verpflichteten sich im Jahre 2001, diesen Grenzwert ab 2006 einzuhalten.\n\nDas im Grundwasser vorkommende Arsen reichert sich in Reis zehnmal so stark an wie in anderen Getreidearten. Auf dem Weltmarkt angebotene Sorten enthalten zwischen 20 und 900 Mikrogramm Arsen pro Kilogramm. Im Jahr 2005 senkte die chinesische Regierung den zulässigen Gehalt anorganischer Arsenverbindungen von 700 auf 150 Mikrogramm pro Kilogramm Lebensmittel, im Juli 2014 beschloss die Codex Alimentarius-Kommission erstmals einen Höchstwert von 200 Mikrogramm für polierten Reis. Die für Lebensmittelsicherheit zuständige EU-Kommission diskutiert für Erzeugnisse aus Puffreis einen um 15 Prozent höheren Grenzwert und für spezielle Produkte für Kleinkinder einen nur halb so hohen (d.&nbsp;h. 100 Mikrogramm pro kg).\n\nFür andere belastete Lebensmittel wie Bier oder Fruchtsäfte gibt es noch keine Grenzwerte, obwohl sie mehr Arsen enthalten können, als für Trinkwasser zulässig ist. Verbraucherorganisationen fordern für Apfelsaft einen Grenzwert von 3, höchstens aber 4,4 ppb (entspricht Mikrogramm pro kg).\n\nFische und Meeresfrüchte weisen zwar hohe Gehalte an Arsen auf, jedoch nahezu ausschließlich in der als unbedenklich geltenden organisch gebundenen Form. Grenzwerte wie für Quecksilber oder Cadmium gibt es nicht.\n\nDas neue Chemikaliengesetz der EU umgesetzt in der Gefahrstoffverordnung Deutschlands von 2005 verbietet im Anhang&nbsp;4 die „gewerbliche“ (nicht private) Verarbeitung von arsenhaltigen Mitteln und Zubereitungen, die mehr als 0,3 Gewichtsprozent an Arsen aufweisen. Derartige Grenzwertregelungen sind gegeben, da Arsen in den Verzinkereien der Galvanikindustrie weltweit der Zinkschmelze zugesetzt wird, um die Haftungseigenschaften des Zinks an der Eisenoberfläche des zu verzinkenden Metallstückes zu verbessern. Auf Grund der Temperatur im Zink-Schmelzbad von 460&nbsp;°C bis 480&nbsp;°C kommt es zum Verdampfen von Arsen, Cadmium und anderen leicht flüchtigen Metallen und deren Anreicherung in der Luft des Arbeitsplatzes. So können zulässige Grenzwerte kurzfristig um das Tausendfache überschritten werden, mit der Folge der aerogen-alveolaren Aufnahme in den Körper. Messungen ergaben, dass Arsen (und Cadmium) im hochreinen Zink (99,995 Reinheitsgrad, DIN-1179-Reinheitsgrad) mit weniger als 0,0004 Gewichts-% ausgewiesen waren und nach Zugabe von 450 Gramm dieses hochreinen Zinks in die Zinkschmelze zu einem Anstieg der Cd-/As-Konzentration von 3 bis 7&nbsp;µg/m Luft auf über 3000&nbsp;µg/m Luft führten. Für Arsen wurde diese Tatsache überraschend in einer Verzinkerei durch Messung der Arsen-Konzentration in Zinkschmelze, Blut und Urin festgestellt (unveröffentlicht). Bei Galvanik-Arbeitern wird die Urin-Arsen-Konzentration mit 25 bis 68&nbsp;µg/l Urin gemessen, im Vergleich zu unbelasteter Bevölkerung mit 0,1&nbsp;µg Arsen/l Urin.\n\n\nAbreicherung.\nFür die Entfernung von ionischem Arsen aus dem Trinkwasser gibt es Verfahren, die auf Adsorption an Aktivkohle, aktiviertem Aluminiumoxid oder Eisenhydroxid-Granulat beruhen. Letzteres wird standardmäßig in Festbettreaktoren in der Trinkwasseraufbereitung in Deutschland und international eingesetzt. Daneben werden Ionenaustauscher verwendet. Es ist möglich, Arsen mittels gentechnisch veränderten Pflanzen aus dem Boden zu entfernen, die es in Blättern speichern. Zur Phytosanierung von Trinkwasser bietet sich die Dickstielige Wasserhyazinthe an, die Arsen insbesondere in ihr Wurzelgewebe einlagert und so eine Abreicherung des kontaminierten Wassers bewirkt. Organische Arsenverbindungen in belasteten Böden können enzymatisch mit Hilfe von Pilzen abgebaut werden.\n\nIn Bangladesh wird nach einem Verfahren der schweizerischen Forschungseinrichtung EAWAG versucht, Arsen mit Hilfe von transparenten PET-Flaschen und Zitronensaft abzureichern. Bei dieser SORAS \"(Solar Oxidation and Removal of Arsenic)\" genannten Methode oxidiert Sonnenlicht das Arsen; die Inhaltsstoffe des Zitronensafts helfen bei der Ausfällung. Mit dieser kostengünstigen Methode lässt sich der Arsengehalt um 75 bis 90 Prozent senken.\n\nIn Gewässern des Yellowstone-Nationalparks, die sich aus Geysiren und anderen Thermalquellen vulkanischen Ursprungs speisen, wurden eukaryontische Algen der Gattung \"Cyanidioschyzon\" gefunden, die die hohen Arsenkonzentrationen der Gewässer tolerieren und sie zu biologisch weniger verfügbaren organischen Verbindungen oxidieren können. An einer Nutzung zur Abreicherung in Trinkwasser wurde 2009 gearbeitet.\n\n\nAntidote.\nAls Antidote bei akuten Arsenvergiftungen stehen die schwefelhaltigen Komplexbildner Dimercaptopropansulfonsäure (DMPS), Dimercaptobernsteinsäure und das ältere, schlechter verträgliche Dimercaprol zur Verfügung. Sie sind noch bei starken Arsendosen effektiv, wenn die Vergiftung rechtzeitig diagnostiziert wird. Ihr Stellenwert bei der Behandlung chronischer Arsenvergiftungen ist hingegen umstritten. Aktivkohle ein bis mehrere Stunden nach der Einnahme kann das Metall ebenfalls binden und zur Ausscheidung bringen.\n\n\nProphylaxe.\nIndische Forscher haben im Tierversuch herausgefunden, dass die Einnahme von Knoblauch zur Senkung der Arsengehalte im Blut und der Erhöhung der Arsengehalte im Urin führen kann. Erklärt wird dies über eine Ausfällung des Arsens bei Reaktion mit schwefelhaltigen Substanzen wie etwa Allicin, das Bestandteil des Knoblauchs ist. Zur Prophylaxe werden zwei bis drei Knoblauchzehen täglich empfohlen.\n\n\nNachweis.\n\nAnorganische Nachweisreaktionen.\nArsenverbindungen zeigen beim Verbrennen eine wenig charakteristische fahlblaue Flammenfärbung. Bei der Glühröhrchenprobe erhitzt man Arsenverbindungen, welche teilweise sublimieren und sich an kalten Oberflächen in Form von schwarzem Arsen, weißem Arsen(III)-oxid oder gelbem Arsentrisulfid wieder niederschlagen.\n\n\nInstrumentelle Bestimmungsverfahren für Arsen.\n\nAtomabsorptionsspektrometrie (AAS).\nBei der Flammen-AAS werden die Arsenverbindungen in einer reduzierenden Luft-Acetylen-Flamme ionisiert. Anschließend wird eine Atomabsorptionsmessung bei 189,0&nbsp;nm beziehungsweise 193,8&nbsp;nm durchgeführt. Nachweisgrenzen bis zu 1&nbsp;µg/ml wurden beschrieben. Häufig wird das Arsen auch mit Hilfe von NaBH in das gasförmige Arsin (AsH) überführt (Hydridtechnik). In der Quarzrohrtechnik wird AsH zuerst bei rund 1000&nbsp;°C in einem elektrisch beheizten Quarzröhrchen thermisch in seine atomaren Bestandteile zersetzt, um anschließend die Absorption bei o.&nbsp;g. Wellenlängen zu bestimmen. Die Nachweisgrenze bei dieser Technik liegt bei 0,01&nbsp;µg/l. Eine weitere Methode ist die sog. Graphitrohrtechnik, bei der das Arsen einer festen Probe bei 1700&nbsp;°C und höher verflüchtigt und anschließend die Extinktion bei 193,8&nbsp;nm gemessen wird.\n\n\nAtomemissionsspektrometrie.\nDie Kopplung von Hydridtechnik mit dem induktiv gekoppelten Plasma/ laserinduzierter Fluoreszenzmessung ist eine sehr nachweisstarke Methode zur Bestimmung von Arsen. Mittels Hydriderzeugung freigesetztes AsH wird dabei im Plasma atomisiert und mit einem Laser zur Emission angeregt. Mit dieser Methode wurden Nachweisgrenzen von 0,04&nbsp;ng/mL erreicht.\n\n\nMassenspektrometrie (MS).\nBei der Massenspektrometrie wird die Arsenspezies zunächst durch ein induktiv gekoppeltes Argonplasma (ICP-MS) thermisch ionisiert. Anschließend wird das Plasma in das Massenspektrometer geleitet. Eine Nachweisgrenze von 0,2&nbsp;µg/l wurde für Arsenit beschrieben.\n\n\nPhotometrie.\nWeitverbreitet ist die photometrische Erfassung von As als Arsenomolybdänblau. As(V) reagiert zunächst mit (NH)MoO. Danach folgt eine Reduktion mit SnCl oder Hydrazin zu einem blauen Komplex. Die Photometrie erfolgt bei 730&nbsp;nm und ist somit nahezu störungsfrei. Die Nachweisgrenzen können durch Verwendung von basischen Farbstoffen als Komplexbildner verbessert werden.\n\n\nNeutronenaktivierungsanalyse.\nEine sehr empfindliche Arsenbestimmung im ppt-Bereich ist mittels Neutronenaktivierungsanalyse möglich. Sie kommt insbesondere dann zur Anwendung, wenn die Probe eine komplexe Zusammensetzung aufweist oder schwierig aufzuschließen ist. Allerdings gibt diese Methode keinen Hinweis auf die chemische Verbindung, in der das Arsen vorliegt. Bei der Wechselwirkung von Neutronen mit der Probe, die das natürliche Isotop Arsen-75 enthält, wird das schwerere Isotop Arsen-76 gebildet, das jedoch instabil ist und sich unter einem β-Zerfall in Selen-76 umwandelt. Gemessen werden dabei die β-Strahlen, über die ein Rückschluss auf die Menge des Arsens möglich ist.\n\n\nBiosensoren.\nBei Biosensoren wird die Biolumineszenz bei Kontakt von in Wasser gelöstem Arsen mit genetisch modifizierten Bakterien (z.&nbsp;B. \"Escherichia coli\" K12) und eines Lichtmessgeräts (Luminometer) detektiert. Die vorhandene Arsenkonzentration korreliert dabei direkt mit der emittierten Lichtmenge.\n\n\nVerbindungen.\n\nArsenwasserstoffe.\nChemische Verbindungen von Arsen und Wasserstoff (→ Arsane) sind im Vergleich zu den entsprechenden Verbindungen der Hauptgruppennachbarn Stickstoff und Phosphor nicht sehr zahlreich und sehr instabil. Es sind zurzeit drei Arsane bekannt.\n\n\nHalogenverbindungen.\nArsen bildet mit Halogenen binäre Verbindungen vom Typ AsX, AsX und AsX (X bezeichnet das entsprechende Halogen).\n\n\nSauerstoffverbindungen.\nWichtige Sauerstoffsäuren sind:\nDas wichtigste Arsenoxid ist Arsen(III)-oxid (Arsentrioxid auch Arsenik oder Weißarsenik, AsO, das Anhydrid der Arsenigen Säure), das in der Gasphase in Form von Doppelmolekülen mit der Formel AsO vorliegt. Es ist amphoter und weist damit auf den Halbmetallcharakter des Arsens hin. Neben AsO kennt man AsO (Arsenpentaoxid, das Anhydrid der Arsensäure) und das gemischte Anhydrid der Arsenigen Säure und Arsensäure AsO (Arsentetraoxid)\n\nEin historisch wichtiges Färbe- und Pflanzenschutzmittel ist ein Kupfer-Arsen-Oxid mit dem Trivialnamen Schweinfurter Grün (Cu(AsO)·Cu(CHCOO)).\n\n\nSchwefelverbindungen.\nEs bestehen zwei wichtige Arsensulfide, die beide als Minerale in der Natur vorkommen.\n\n\nArsen-Metall-Verbindungen.\nWichtige Verbindungen von Arsen mit Metallen sind\n\n\nOrganische Verbindungen.\nIn Analogie zu den Aminen und Phosphinen findet man entsprechende Verbindungen mit Arsen anstelle von Stickstoff oder Phosphor. Sie werden als Arsine bezeichnet.\n\nZu den Arsoranen, Verbindungen vom Typ RAs, wobei R für fünf – möglicherweise unterschiedliche – organische Gruppen steht, zählt man etwa Pentaphenylarsen oder Pentamethylarsen. Fehlt eine der fünf Gruppen, bleibt ein einfach positiv geladenes Ion zurück (R steht wiederum für – möglicherweise verschiedene – organische Gruppen), das man als Arsoniumion (AsR) bezeichnet.\nAnalog zu den Carbonsäuren lassen sich zwei Klassen arseno-organischer Säuren bilden:\n\nZudem sind Heteroaromaten mit Arsen als Heteroatom bekannt, wie Arsabenzol, das aus einem Benzolring besteht, in dem ein Kohlenstoffatom durch Arsen ersetzt ist und das somit analog zu Pyridin aufgebaut ist.\nAuch homocyclische Arsenverbindungen existieren. Beispiele sind\nderen Moleküle einen Fünf- beziehungsweise Sechsring aus Arsenatomen als Rückgrat aufweisen, an den nach außen hin je eine Methylgruppe pro Arsenatom gebunden ist. Eine polycyclische Variante bildet das nebenstehende Molekül, dessen Rückgrat sich aus einem Sechs- und zwei angehefteten Fünfringen zusammensetzt (R steht für jeweils eine \"tert\"-Butylgruppe).\n\nSchließlich lassen sich Arsenpolymere darstellen, lange Kettenmoleküle, die als Polyarsine bezeichnet werden. Sie bestehen aus einer zentralen „Strickleiter“ der Arsenatome, an die außen auf jeder Seite je „Sprosse“ eine Methylgruppe angeheftet ist, so dass sich die chemische Formel (AsCH) ergibt, wobei die natürliche Zahl \"n\" weit über 100 liegen kann. Polyarsine zeigen deutliche Halbleitereigenschaften.\n\n\nBioorganische Verbindungen.\nIn der Bioorganik spielen Arsenolipide, Arsenosaccharide und arsenhaltige Glycolipide eine bedeutende Rolle. Wichtige Vertreter dieser Stoffklassen sind zum Beispiel Arsenobetain, Arsenocholin und unterschiedlich substituierte Arsenoribosen. Sie treten vor allem kumuliert in maritimen Lebewesen auf und können auf diesem Weg in die menschliche Nahrungskette gelangen. Arsenhaltige Biomoleküle konnten in Algen, Meeresschwämmen und in Fischgewebe nach erfolgter Extraktion mittels HPLC-ICP-MS nachgewiesen werden. Die Analytik von Organo-Arsenverbindungen (einschließlich ihrer Speziation) ist sehr aufwändig.\n\n\nArsen in Kriminalgeschichte, Literatur und Film.\nDas Element Arsen erreichte zweifelhafte Berühmtheit als Mordgift, belegt durch geschichtliche Aufzeichnungen sowie die Instrumentalisierung in Literatur und Film. Es handelte sich bei dem Mordgift allerdings nie um elementares Arsen, sondern um dessen Verbindungen.\n\nIn Italien und Frankreich starben Herzöge, Könige und Päpste an vorsätzlich herbeigeführten Arsenvergiftungen. Im Frankreich des 17. Jahrhunderts steht die Marquise de Brinvilliers, die ihren Vater und zwei Brüder mit einer Arsenikmischung vergiftete, im Mittelpunkt eines Giftskandals. In Deutschland brachte die Serienmörderin Gesche Gottfried aus Bremen 15 Menschen zu Tode. Aufsehen erregte auch der Fall der Serienmörderin Anna Margaretha Zwanziger zu Beginn des 19. Jahrhunderts. Die Urheber der Morde blieben jedoch meist unerkannt, da Arsen bis 1836 in kleinen Mengen nicht nachgewiesen werden konnte. Erst die durch James Marsh entwickelte und nach ihm benannte Marshsche Probe machte es möglich, Spuren des Elementes zu identifizieren und somit eine unnatürliche Todesursache nachzuweisen. Im 19. und 20. Jahrhundert fanden weiter vorsätzliche Vergiftungen mit arsenhaltigen Mitteln statt – zum einen, weil sie leicht als Herbizide verfügbar waren, zum anderen ließ sich bei chronischer Gabe kleiner Dosen ein krankheitsbedingter Tod vortäuschen. Im September 1840 fiel im Prozess gegen Marie Lafarge das erste Urteil, das alleine auf den Ergebnissen der Marshschen Probe beruhte. Im Fall der Marie Besnard, die angeblich zwischen 1927 und 1949 für mehrere Todesfälle in ihrem Umfeld in Loudun verantwortlich sein sollte, konnte ein eindeutiger Beweis nicht erbracht werden, weil Untersuchungsergebnisse widersprüchlich waren, und sie musste 1954 letztendlich freigesprochen werden.\nJahrelang glaubte die Fachwelt, dass der Tod des ehemaligen französischen Kaisers Napoleon Bonaparte mit 51 Jahren auf der Insel St. Helena einem Giftanschlag mit Arsen zugeschrieben werden muss. Zumindest hatte man in seinen Haaren hochkonzentrierte Spuren des Giftes entdeckt. Heute existieren verschiedene andere Thesen zur Erklärung des Faktenbefunds. Eine Möglichkeit besteht darin, dass das Arsen \"nach\" seinem Tod den Haaren beigegeben wurde, um diese zu konservieren, eine damals durchaus übliche Methode. Möglich ist ein Übermaß der Benutzung der arsenhaltigen Fowlersche Lösung, die zu seiner Zeit bei vielen seiner Zeitgenossen als medizinisches Wundermittel galt. Die dritte und heute als wahrscheinlichste angesehene Möglichkeit ist, dass sich Napoleon durch organische Arsenverbindungen vergiftete, die Schimmelpilze beständig aus seinen mit grünen Arsenpigmenten gefertigten Tapeten freisetzten. Deren hoher Arsengehalt ist durch eine 1980 in einem Notizbuch aufgefundene Materialprobe schlüssig belegt.\n\nDer berühmte Philosoph René Descartes starb 1650 wenige Monate nach seiner Ankunft am Hofe der schwedischen Königin Christine. Der Verdacht, er sei von einem der Jesuiten, die sich am Hofe der protestantischen Königin aufhielten, aus religionspolitischen Gründen mit Arsen vergiftet worden, verstärkte sich, als Christine später tatsächlich zum Katholizismus konvertierte, konnte aber nicht erhärtet werden, so dass die offizielle Todesursache, Lungenentzündung, sich in den Biographien etablierte. Erst kürzlich wurde anhand von neu aufgefundenen und neu interpretierten Dokumenten der alte Verdacht erhärtet und behauptet, dass der „Giftmord an Descartes in sehr hohem Maße wahrscheinlich, um nicht zu sagen, fast sicher“ erscheint.\n\nIm Jahre 1900 kam es im britischen Manchester zu einer Massenvergiftung, von der mehrere Tausend Menschen betroffen waren. Wie sich herausstellte, hatten alle Bier derselben Brauerei getrunken. In Vorstufen der Bierproduktion wurde anscheinend Schwefelsäure eingesetzt, die ihrerseits aus Schwefel hergestellt wurde, der aus mit Arsenopyrit kontaminierten Sulfidmineralen stammte. Etwa 70 Menschen erlagen ihren Vergiftungen.\n\nIn den Jahren 2010 und 2011 starben in Österreich zwei Männer an einer Arsenvergiftung. Am 11. April 2013 wurde am Landesgericht Krems eine 52-jährige Polin des Mordes an den beiden für schuldig befunden und von dem Geschworenengericht nicht rechtskräftig zu lebenslanger Haft verurteilt.\nNoch in den 1950er Jahren auf dem Höhepunkt des Kalten Krieges erkrankte die US-amerikanische Botschafterin, Clare Booth Luce, in Rom durch eine Vergiftung mit dem aus Tapeten freigesetzten Arsen. Die Tatsache, dass die Krankheit auf die schimmelpilzbefallenen Tapeten und nicht auf gegnerische Geheimagenten zurückgeführt werden konnte, trug in diesem Fall nicht nur zur Genesung der Botschafterin, sondern auch zum Erhalt des Friedens bei.\n\nIn Friedrich Schillers bürgerlichem Trauerspiel \"Kabale und Liebe\" vergiftet der junge Major Ferdinand von Walter erst seine Geliebte Luise Millerin und dann sich selbst. Allerdings tritt in \"Kabale und Liebe\" der Tod unrealistischerweise binnen Minuten ein.\n\nDie Protagonistin des berühmten Romans \"Madame Bovary\" von Gustave Flaubert, die unglücklich verheiratete Landarztgattin Emma Bovary, stirbt am Ende des Romans durch Suizid mit Arsen in Form eines weißen Pulvers. Der Spross einer Arztfamilie Flaubert beschreibt die Vergiftungssymptome und den äußerst qualvollen Tod der Bovary sehr detailliert.\n\nIm Roman \"Starkes Gift\" \"(Strong Poison)\" von Dorothy L. Sayers ist das Opfer mit Arsen vergiftet worden. Die Verdächtige, Krimi-Schriftstellerin Harriet Vane, hat sich zur fraglichen Zeit intensiv mit Arsenmorden beschäftigt und sich dazu sogar vom Apotheker beraten lassen.\n\nDer berühmte Detektiv „Kalle Blomquist“ aus dem gleichnamigen Kinderbuch von Astrid Lindgren wendete die Marshsche Probe an, um ein mit Arsen vergiftetes Stück Schokolade zu überprüfen.\n\nIn dem Theaterstück von Joseph Kesselring \"Arsen und Spitzenhäubchen\" (englisch: \"Arsenic and Old Lace\") vergiften zwei alte Damen in gutmeinender Absicht ältere einsame Herren mit einer Arsen-, Strychnin- und Zyankali-Mischung. Bekannt wurde das Stück durch die gleichnamige Verfilmung von Frank Capra mit Cary Grant, Peter Lorre und Priscilla Lane in den Hauptrollen.\n\n\n"}
{"id": "98", "url": "https://de.wikipedia.org/wiki?curid=98", "title": "Astat", "text": "Astat\n\nAstat [] (von : „unbeständig, unstet“) ist ein radioaktives chemisches Element mit dem Elementsymbol At und der Ordnungszahl 85. Im Periodensystem steht es in der 7. Hauptgruppe bzw. der 17.&nbsp;IUPAC-Gruppe und zählt damit zu den Halogenen. Astat entsteht beim natürlichen Zerfall von Uran. Astat ist eines der seltensten natürlich vorkommenden Elemente der Erde, das bei Bedarf künstlich erzeugt werden muss.\n\n\nGeschichte.\nAls Dmitri Mendelejew 1869 sein Periodensystem festlegte, sagte er die Existenz einiger zu dieser Zeit noch nicht entdeckter Elemente voraus, darunter eines, das den Platz unter Iod einnehmen würde. In der Folge versuchten einige Wissenschaftler dieses Element, das als „Eka-Iod“ bezeichnet wurde, zu finden.\n\nIm Jahre 1931 behauptete Fred Allison, er und seine Mitarbeiter am Alabama Polytechnic Institute (heute Auburn University) hätten das fehlende Element entdeckt und gaben ihm die Bezeichnung \"Alabamine\" (Ab). Ihre Entdeckung konnte jedoch nicht bestätigt werden und wurde später als falsch erkannt.\n\nEbenfalls auf der Suche nach einem Mitglied der Familie des radioaktiven Thoriums fand der Chemiker De Rajendralal Mitra im Jahre 1937 in Dhaka, Bangladesch (damals Britisch-Indien), zwei neue Elemente. Das erste nannte er \"Dakin\" (Eka-Iod), wohl nach der englischen Bezeichnung für Dhaka (Dacca), das andere \"Gourium\". Beide Entdeckungen konnten jedoch nicht bestätigt werden.\n\nDer Name \"Helvetium\" wurde wiederum von dem Schweizer Chemiker Walter Minder vorgeschlagen, als er die Entdeckung des Elements 85 im Jahr 1940 ankündigte. Er änderte im Jahr 1942 jedoch seinen Vorschlag in \"Anglohelvetium\".\n\nBestätigt werden konnte die Entdeckung des Astat (altgriechisch ἀστατέω = „unbeständig sein“, aufgrund des radioaktiven Zerfalls) erstmals im Jahre 1940 durch die Wissenschaftler Dale Corson, Kenneth MacKenzie und Emilio Gino Segrè, die es in der University of California künstlich durch Beschuss von Bismut mit Alphateilchen herstellten.\n\nDrei Jahre später konnte das kurzlebige Element von Berta Karlik und Traude Bernert auch als Produkt des natürlichen Zerfallsprozesses von Uran gefunden werden.\n\n\nGewinnung und Darstellung.\nAstat wird durch Beschuss von Bismut mit Alphateilchen im Energiebereich von 26 bis 29&nbsp;MeV hergestellt. Man erhält dabei die relativ langlebigen Isotope At bis At, die dann im Stickstoffstrom bei 450 bis 600&nbsp;°C sublimiert und an einer gekühlten Platinscheibe abgetrennt werden.\n\n\nEigenschaften.\nBei diesem radioaktiven Element wurde mit Hilfe von Massenspektrometrie nachgewiesen, dass es sich chemisch wie die anderen Halogene, besonders wie Iod verhält (es sammelt sich wie dieses in der Schilddrüse an). Astat ist stärker metallisch als Iod.\nForscher am Brookhaven National Laboratory haben Experimente zur Identifikation und Messung von elementaren chemischen Reaktionen durchgeführt, die Astat beinhalten.\n\nMit dem On-Line-Isotopen-Massenseparator (ISOLDE) am CERN wurde 2013 das Ionisationspotenzial von Astat mit 9,31751(8) Elektronenvolt bestimmt.\n\n\nIsotope.\nAstat hat etwa 20 bekannte Isotope, die alle radioaktiv sind; das langlebigste ist At mit einer Halbwertszeit von 8,3 Stunden.\n\n\nVerwendung.\nOrganische Astatverbindungen dienen in der Nuklearmedizin zur Bestrahlung bösartiger Tumore. Astat-Isotope eignen sich aufgrund der kurzen Halbwertszeiten innerlich eingenommen als radioaktive Präparate zum Markieren der Schilddrüse. Das Element wird in der Schilddrüse angereichert und in der Leber gespeichert.\n\n\nVerbindungen.\nDie chemischen Eigenschaften von Astat konnten aufgrund der geringen Mengen bisher nur mit Tracerexperimenten festgestellt werden. Sie ähneln stark denjenigen des Iods, wobei es aber ein schwächeres Oxidationsmittel ist. Bisher konnten diverse Astatide, Interhalogenverbindungen und organische Verbindungen nachgewiesen werden. Auch die Anionen der entsprechenden Sauerstoffsäuren sind bekannt. Wegen des im Vergleich zu anderen Halogenen elektropositiveren Charakters wird es von Silber nur unvollständig ausgefällt. Dafür existiert das komplexstabilisierte Kation At(Py) (Py=Pyridin), wodurch Astat auch kathodisch abgeschieden werden kann. Nachgewiesen wurde auch das Hydrid, Astatwasserstoff HAt.\n\n\nSicherheitshinweise.\nEinstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.\n\n"}
{"id": "99", "url": "https://de.wikipedia.org/wiki?curid=99", "title": "Alkalimetalle", "text": "Alkalimetalle\n\nAls Alkalimetalle werden die chemischen Elemente Lithium, Natrium, Kalium, Rubidium, Caesium und Francium aus der 1. Hauptgruppe des Periodensystems bezeichnet. Sie sind silbrig glänzende, reaktive Metalle, die in ihrer Valenzschale ein einzelnes Elektron besitzen. Obwohl Wasserstoff in den meisten Darstellungen des Periodensystems in der ersten Hauptgruppe steht und zum Teil ähnliche chemische Eigenschaften wie die Alkalimetalle aufweist, kann er nicht zu diesen gezählt werden, da er unter Standardbedingungen weder fest ist noch metallische Eigenschaften aufweist.\n\n\nEtymologie.\nDer Name der Alkalimetalle leitet sich von dem arabischen Wort für „Pottasche“ ab, die alte Bezeichnung für aus Pflanzenaschen gewonnenes Kaliumcarbonat. Humphry Davy stellte im Jahre 1807 erstmals das Element Kalium durch eine Schmelzflusselektrolyse aus Kaliumhydroxid dar. Letzteres gewann er aus Kaliumcarbonat. In einigen Sprachen spiegelt sich dies im Namen wider. So heißt Kalium beispielsweise im Englischen und Französischen \"potassium\" und im Italienischen \"potassio\".\n\n\nEigenschaften.\nAlkalimetalle sind metallisch glänzende, silbrig-weiße weiche Leichtmetalle. Caesium hat bei geringster Verunreinigung einen Goldton. Sie sind mit dem Messer schneidbar. Alkalimetalle haben eine geringe Dichte. Sie reagieren mit vielen Stoffen, so beispielsweise mit Wasser, Luft oder Halogenen teilweise äußerst heftig unter starker Wärmeentwicklung. Insbesondere die schwereren Alkalimetalle können sich an der Luft selbst entzünden. Daher werden sie unter Schutzflüssigkeiten, wie Paraffin oder Petroleum (Lithium, Natrium und Kalium), bzw. unter Luftabschluss in Ampullen (Rubidium und Caesium) aufbewahrt.\n\nAls Elemente der ersten Gruppe des Periodensystems besitzen sie nur ein schwach gebundenes s-Elektron, das sie leicht abgeben. Ihre ersten Ionisierungsenergien und ihre Elektronegativitäten sind entsprechend klein. In Verbindungen kommen sie alle fast ausschließlich als einwertige Kationen vor, wenngleich sogar Verbindungen bekannt sind, in denen diese Metalle anionisch vorliegen (z.&nbsp;B. Natride, komplexiert mit sogenannten Kryptanden).Alkalimetalle und ihre Salze besitzen eine spezifische Flammenfärbung:\n\n\nAufgrund dieser Flammenfärbung werden Alkalimetallverbindungen für Feuerwerke benutzt.\n\nIn der Atomphysik werden Alkalimetalle eingesetzt, da sie sich aufgrund ihrer besonders einfachen elektronischen Struktur besonders leicht mit Lasern kühlen lassen.\n\n\nPhysikalische Eigenschaften.\nAlle Alkalimetalle kristallisieren in der kubisch-raumzentrierten Struktur. Lediglich Lithium und Natrium kristallisieren in der hexagonal-dichtesten Packung, wenn tiefe Temperaturen vorherrschen.\n\nDer Radius der Elementatome sowie der Kationen nimmt mit steigender Massenzahl zu. Viele andere Eigenschaften der Alkalimetalle zeigen einen Trend innerhalb der Gruppe von oben nach unten:\n\n\nReaktionen und Verbindungen.\nDie Alkalimetalle reagieren mit Wasserstoff unter Bildung salzartiger \"Alkalimetallhydride\":\n\nDie thermische Beständigkeit der Hydride nimmt vom Lithiumhydrid zum Caesiumhydrid ab. Alkalihydride werden als Reduktions- oder Trocknungsmittel eingesetzt.\n\nMit Sauerstoff reagieren die Metalle unter Bildung fester, weißer \"Alkalimetalloxide\" (Lithiumoxid, Natriumoxid), \"Alkalimetallperoxide\" (Natriumperoxid, Kaliumperoxid) und \"Alkalimetallhyperoxide\" (Kaliumhyperoxid, Rubidiumhyperoxid, Caesiumhyperoxid):\n\nDie Reaktion mit Wasser zu \"Alkalimetallhydroxiden\" erfolgt unter Freisetzung von Wasserstoff:\n\nVom Lithium zum Caesium steigt die Reaktivität stark an. Während eine annähernd kubische Probe von Lithium relativ träge reagiert, entzündet sich schon bei Natrium aufgrund der Hitzeentwicklung der entstehende Wasserstoff unter Anwesenheit von Luftsauerstoff. Ab dem Kalium erfolgt bei fortschreitender Reaktion auch Verdampfung und Entflammung des Metalls, nicht zuletzt auch wegen des mit der Ordnungszahl abnehmenden Siedepunkts (siehe oben). Aber auch unter Luftabschluss können schon weniger als 0,5&nbsp;g Natrium explosiv mit Wasser reagieren, was eigentlich durch die an der Kontaktfläche der Edukte entstehenden Reaktionsprodukte, Wasserstoff und Alkalimetalhydroxid, gehemmt werden sollte. Hoch&shy;geschwindigkeits&shy;aufnahmen eines Experiments, bei dem Tropfen einer unter Standardbedingungen flüssigen Legierung aus Kalium und Natrium unter einer Inertgas-Atmosphäre mit Wasser in Kontakt gebracht wurden, legen eine initial erfolgende Coulomb-Explosion („negative Oberflächenspannung“) bzw. die damit einhergehende starke Oberflächenvergrößerung der mit Ausnahme von Lithium nach kurzer Zeit schmelzflüssigen unlegierten Alkalimetallproben als Ursache für den ungehemmten Ablauf, die hohe Geschwindigkeit und damit die Heftigkeit dieser Reaktionen nahe. Die Alkalimetallhydroxide sind farblose Feststoffe, die sich in Wasser unter starker Erwärmung leicht lösen und dabei stark basisch reagieren. Die Hydroxide und ihre Lösungen wirken stark ätzend.\n\nDie Reaktivität der Alkalimetalle nimmt mit steigender Ordnungszahl zu, weil mit steigender Zahl von Elektronenschalen das Außenelektron immer mehr von der Anziehungskraft des positiv geladenen Atomkerns abgeschirmt wird und daher leichter abgespalten werden kann. Die Zunahme der Reaktivität lässt sich an der Reaktion der verschiedenen Metalle mit Wasser gut erkennen: Lithium und Natrium reagieren mit Wasser zwar heftig unter Wasserstoffentwicklung, aber ohne dass es zur Entzündung des Wasserstoffs kommt. Kalium und Rubidium reagieren unter spontaner Entzündung des Wasserstoffs, Caesium reagiert explosionsartig. Besonders gut reagieren die Alkalimetalle mit Nichtmetallen, denen nur wenige Elektronen fehlen, um Edelgaskonfiguration zu erreichen.\n\nMit Halogenen reagieren die Alkalimetalle zu den salzartigen \"Alkalimetallhalogeniden\":\n\nDie Reaktivität steigt vom Lithium zum Caesium und sinkt vom Fluor zum Iod. So reagiert Natrium mit Iod kaum und mit Brom sehr langsam, während die Reaktion von Kalium mit Brom und Iod explosionsartig erfolgt.\n\nAlkalimetalle können Halogenkohlenwasserstoffen unter Explosionserscheinungen das Halogen unter Bildung von Kohlenstoff und dem entsprechenden Alkalimetallhalogenid entziehen:\n\nAlkalimetalle ergeben mit flüssigem Ammoniak intensiv blau gefärbte Lösungen. Diese Lösungen, die aus positiven Alkalimetall-Ionen und solvatisierten Elektronen besteht, sind ein sehr starkes Reduktionsmittel und werden beispielsweise für die Birch-Reduktion eingesetzt. Wird diesen Lösungen ein geeigneter Komplexbildner zugesetzt, können sich entsprechende Salze mit Alkalimetall-Anionen, die sogenannten Alkalide, bilden.\n\n\nWasserstoff.\nWasserstoff, das erste Element der 1. Hauptgruppe, ist unter Normalbedingungen ein Nichtmetall. Er wird deshalb nicht zu den Alkalimetallen gezählt, teilt jedoch mit ihnen einige Eigenschaften. Wasserstoff tritt wie die Alkalimetalle stets einwertig auf und wandelt sich unter extrem hohem Druck in eine metallische Hochdruckmodifikation um, den metallischen Wasserstoff. Umgekehrt haben auch einige Alkalimetalle unter bestimmten Bedingungen Eigenschaften wie Wasserstoff, z.&nbsp;B. besteht Lithium als Gas zu 1 % aus zweiatomigen Molekülen.\n\n\n"}
{"id": "100", "url": "https://de.wikipedia.org/wiki?curid=100", "title": "Actinoide", "text": "Actinoide\n\nActinoide [] („Actiniumähnliche“; griech.: Endung \"-οειδής\" \"(-oeides)\" „ähnlich“) ist eine Gruppenbezeichnung bestimmter ähnlicher Elemente. Zugerechnet werden ihr das Actinium und die 14 im Periodensystem folgenden Elemente: Thorium, Protactinium, Uran und die Transurane Neptunium, Plutonium, Americium, Curium, Berkelium, Californium, Einsteinium, Fermium, Mendelevium, Nobelium und Lawrencium. Im Sinne des Begriffs gehört Actinium nicht zu den Actiniumähnlichen, jedoch folgt die Nomenklatur der IUPAC hier dem praktischen Gebrauch. Die frühere Bezeichnung Actinide entspricht nicht dem Vorschlag der Nomenklaturkommission, da nach diesem die Endung „-id“ für binäre Verbindungen wie z.&nbsp;B. Chloride reserviert ist; die Bezeichnung ist aber weiterhin erlaubt. Alle Actinoide sind Metalle und werden auch als \"Elemente der Actiniumreihe\" bezeichnet.\n\n\n\nEntdeckung und Gewinnung.\nIm Jahr 1934 publizierte die deutsche Chemikerin Ida Noddack eine Arbeit über drei Lücken im Periodischen System der Elemente, die später mit den Elementen Francium, Astat und Promethium gefüllt wurden. Gleichsam nebenbei merkte sie an, dass es denkbar sei, dass \"bei der Beschießung schwerer Kerne mit Neutronen diese Kerne in mehrere größere Bruchstücke zerfallen\". Aber nicht nur das. Denkbar sei auch, dass Elemente mit Ordnungszahlen Z&nbsp;> 92, also Transurane, gebildet werden könnten.\n\nTatsächlich synthetisierten Edwin M. McMillan und Philip H. Abelson erstmals im Jahr 1940 die ersten nicht in der Natur vorkommenden Actinoid-Nuklide U, Np und Pu durch Beschuss von Uran mit Neutronen.\n\nDa im Jahr 1940 noch kein Kernreaktor in Betrieb war, konnten die Neutronen nur aus einer Neutronenquelle stammen.\n\nObwohl bei dieser Kernreaktion letztlich auch ein Plutonium-Isotop entsteht, konnte Plutonium wahrscheinlich wegen der geringen Ausbeute noch nicht nachgewiesen werden. Als Entdeckungsjahr von Plutonium gilt das Jahr 1941, wie die Tabelle \"Entdeckung der Transurane\" zeigt.\n\nPlutonium wurde von den US-Amerikanern Glenn T. Seaborg, J.&nbsp;W.&nbsp;Kennedy, E.&nbsp;M.&nbsp;McMillan, Michael Cefola und Arthur Wahl entdeckt. Ende 1940 stellten sie das Plutonium-Isotop Pu durch Beschuss des Uran-Isotops U mit Deuteronen her, die in einem Zyklotron beschleunigt worden waren. Nachdem der eindeutige Nachweis für das Element 94 erbracht worden war, erhielt es 1941 den Namen \"Plutonium\". Der Name lag nahe, da die beiden Vorgängerelemente nach den Planeten Uranus und Neptun benannt worden waren. Details über die Kernreaktionen sind im Artikel \"Plutonium\" nachzulesen.\n\nDamit eine Transmutation mit elektrisch geladenen Teilchen wie Deuteronen stattfinden kann, müssen diese Teilchen auf eine Energie beschleunigt werden, die ausreicht, um die Coulombbarriere von Urankernen zu überwinden oder diese zumindest zu durchtunneln. Das war erstmals mit einem Zyklotron möglich. Die erste wägbare Menge Plutonium von etwa 4&nbsp;µg wurde 1942 isoliert.\nEnde 1942 wurde der erste Kernreaktor Chicago Pile in Betrieb genommen. Mit Kernreaktoren konnten vergleichsweise größere Mengen der Elemente Plutonium und Americium gewonnen werden, die als Targetmaterial dienten. In Verbindung mit geladenen Teilchen, mit beschleunigten α-Teilchen, wurden dann die Nuklide der Elemente Curium, Berkelium und Californium entdeckt. Der Vorteil des Verfahrens, der Beschuss beschleunigter geladener Teilchen auf leichtere Actinoide, ist, dass auch massereiche, neutronenarme Nuklide erhalten werden können, die durch eine Neutronenbestrahlung nicht gebildet werden.\n\nDas effektivste Verfahren, um synthetische Actinoide zu erzeugen, ist die Transmutation von Uran- oder Thorium-Nukliden im Kernbrennstoff eines Kernreaktors durch Neutroneneinfang oder (n,2n)-Reaktionen. Dieses Verfahren ist auf nicht allzu massereiche Actinoid-Nuklide beschränkt, etwa bis zu Massenzahlen von A&nbsp;=&nbsp;252. Rechnerisch (s.&nbsp;u) werden meist nur Nuklide bis A&nbsp;<&nbsp;248 einbezogen. \n\nAmericium zum Beispiel wurde in einem Zweistufenprozess entdeckt. In der ersten Stufe werden synthetische Actinoide in einem Kernreaktor gebildet, zum Beispiel Pu. Plutonium wird nach Entnahme des Kernbrennstoffs durch Wiederaufarbeitung chemisch extrahiert. Dann wird das Plutonium erneut im Reaktor oder mittels einer Quelle mit Neutronen bestrahlt. Der gegenwärtig betriebene 85&nbsp;MW High-Flux-Isotope Reactor am Oak Ridge National Laboratory in Tennessee, USA ist auf die Herstellung von Transcuriumelementen (Z&nbsp;> 96) speziell ausgelegt.\n\nDie massereichen Elemente Einsteinium und Fermium wurden durch den Test der ersten amerikanischen Wasserstoffbombe, Ivy Mike, am 1.&nbsp;November 1952 auf dem Eniwetok-Atoll auf unsere Erde gebracht. Nur wenige Wochen später wurden sie im Lawrence Berkeley National Laboratory, das an der geheimen Waffentwicklung nicht beteiligt war, auf Filterpapieren und in Ablagerungen an Korallen völlig unerwartet entdeckt. Bei der Wasserstoffbombenexplosion waren bis zu 16 Neutronen von einem Kern von U eingefangen worden. Dieser Zwischenkern zerfiel sehr schnell über eine Kette von Betazerfällen in Nuklide der bis dahin unbekannten Elemente. Das Uran stammte aus der Ummantelung der Wasserstoffbombe, die aus fünf Tonnen Natururan bestand.\n\nAus Gründen der militärischen Geheimhaltung durften die Ergebnisse zunächst nicht publiziert werden. Um sich dennoch die Priorität der Entdeckungen der beiden neuen Elemente zu sichern, wurden parallel erfolgreich Schwerionenreaktionen durchgeführt. U wurde mit N-Ionen, die an einem Zyklotron erzeugt wurden, bombardiert. Auf diesem Wege wurde zunächst Einsteinium synthetisiert und in der Publikation auf die Entdeckung von 1952 verwiesen. Ähnlich verfuhr man mit Fermium, das durch Beschuss mit Pu mit N-Ionen erzeugt wurde.\n\nWie die Tabelle zeigt, führten Schwerionenreaktionen auch zu den Elementen Mendelevium, Nobelium und Lawrencium mit den Ordnungszahlen 101 bis 103. Mit Schwerionenreaktionen wurden auch die Elemente mit höheren Ordnungszahlen synthetisiert.\n\n\nBildung und Gewinnung im Kernreaktor.\nDer Kernreaktor nimmt, was die Actinoide betrifft, nicht nur deshalb eine herausragende Stellung ein, weil er ohne Actinoide nicht betrieben werden könnte, sondern weil es nur innerhalb eines Kernreaktors möglich ist, größere wägbare Mengen von „höheren“ Actinoid-Nukliden zu bilden. Dieser Abschnitt beschreibt, welche Actinoid-Nuklide das sind und in welchem Massenanteil (relativ zur ursprünglichen Schwermetallmasse) sie gebildet werden.\n\nIn einem Kernreaktor werden zu Beginn des Betriebs große Mengen von Actinoiden in Form von Kernbrennstoff eingebracht, zum Beispiel mit U angereichertes Uran. In einem Leistungsreaktor sind Aktinoidmassen in der Größenordnung von 100&nbsp;t erthalten. Aus diesem Kernbrennstoff werden neben der gewünschten Energiefreisetzung durch Kernspaltung synthetische „höhere“ Actinoide durch Transmutation erzeugt.\n\n\nActinoid-Nuklide in einem Kernreaktor.\nDie Abbildung \"Nuklidkarte: Bildung und Zerfall von Actinoiden in einem Kernreaktor\" ist eine Nuklidkarte in der Anordnung nach Segrè. Das heißt, die Anzahl der Neutronen wird nach rechts zunehmend, die Anzahl der Protonen nach oben zunehmend aufgetragen. Um die Zeichnungsfläche sinnvoll zu nutzen, ist der Nuklidpfad am roten Punkt unterteilt.\n\nJedes Quadrat der Abbildung stellt ein Actinoid-Nuklid dar, das in einem Kernreaktor auftreten kann. Die Abbildung zeigt auch das Netzwerk von Bildung und Zerfall von Actinoid-Nukliden, ausgelöst durch den Neutroneneinfang freier Neutronen und durch andere Kernreaktionen. Ein ähnliches Schema findet man in einer Arbeit aus dem Jahr 2014. Dort sind die Isotope von Thorium und Protactinium, die zum Beispiel in Salzschmelzenreaktoren eine große Rolle spielen, und die Isotope der Elemente Berkelium und Californium nicht enthalten.\n\nDie Bildung von Actinoid-Nukliden wird in erster Linie geprägt durch:\n\nAußer diesen neutronen- oder gammainduzierten Kernreaktionen wirkt sich auch die radioaktive Umwandlung der Actinoid-Nuklide auf den Nuklid-Bestand in einem Reaktor zu einem gegebenen Zeitpunkt (Abbrandzustand) aus. Diese Zerfallsarten sind in der Abbildung durch Diagonalpfeile markiert.\n\nEine große Rolle für die Bilanz der Teilchendichten der Nuklide spielt der Beta-Minus-Zerfall, markiert mit aufwärts zeigenden Diagonalpfeilen. Die Quadrate der Nuklide dieses Zerfallstyps haben in der Nuklidkarte eine hellblaue Füllfarbe. Zwei Nuklide sind nur zum Teil betaaktiv. Das rosafarben hinterlegte Nuklid Np wandelt sich zu 86 % durch Elektroneneinfang und zu 14 % durch Beta-Minus-Zerfall (hellblaue Ecke) um. Ebenfalls zwei Zerfallsarten, aber in nahezu umgekehrtem Verhältnis (83 % Beta-Minus-Zerfall, 17 % Elektroneneinfang) zeigt auch der Grundzustand von Am (rosa Ecke). Dieses Nuklid besitzt außerdem einen langlebigen metastabilen Zustand (weiß hinterlegt), der in der Regel durch Am symbolisiert wird. Nuklide mit Positronen-Emissionen (Beta-Plus-Zerfälle) kommen in einem Kernreaktor nicht vor.\n\nDie Actinoid-Nuklide, die sich unter Aussendung von α-Teilchen spontan umwandeln, sind in der Nuklidkarte der Abbildung mit einer gelben Füllfarbe versehen worden. Der α-Zerfall spielt wegen der langen Halbwertszeiten (T) bei Bildung und Zerfall der Actinoide während der Aufenthaltsdauer des Brennstoffs in einem Leistungsreaktor (max. ca. 3 Jahre) so gut wie keine Rolle. Ausnahmen sind die beiden relativ kurzlebigen Nuklide Cm (T&nbsp;=&nbsp;163&nbsp;d) und Pu (T&nbsp;=&nbsp;2.9&nbsp;a). Nur für diese beiden Fälle ist der α-Zerfall durch lange, abwärts zeigende Pfeile auf der Nuklidkarte markiert.\n\nAlle in der Abbildung angegebenen Halbwertszeiten sind die aktuellen evaluierten Nukleardaten NUBASE2012, abgerufen über den Nukleardaten-Viewer JANIS&nbsp;4. Manche Halbwertszeiten sind gerundet.\n\n\nZur Bedeutung von Abbrandprogrammen.\nNur bei der ersten Inbetriebnahme des Reaktors kennt man die Zusammensetzung an Nukliden des Kernbrennstoffs genau. Es ist aber für jeden Zeitpunkt (Abbrandzustand) erforderlich, grundlegende physikalische Größen wie den Neutronenfluss zu kennen. Zu den grundlegenden physikalischen Größen gehören auch die Teilchendichten und \"Massendichten\" aller im Reaktor gebildeten Actinoid-Nuklide. Das betrifft sowohl die der anfangs eingebrachten (oder was davon noch vorhanden ist) als auch die der im Reaktorbetrieb gebildeten Nuklide. Die tatsächlichen Teilchendichten (und Massendichten) der Actinoid-Nuklide in Abhängigkeit vom Abbrand im laufenden Betrieb zu messen, ist nicht möglich. Erst nach der Entladung von Kernbrennstoff können diese Größen im Prinzip chemisch oder massenspektrometrisch untersucht werden. Das ist sehr aufwendig. Deshalb kommt einer Berechnung, die den Betriebsablauf eines Kernreaktors begleitet, ein hoher Stellenwert zu.\n\nDie Entwicklung der Teilchendichten (und Massendichten) von Nukliden in einem Kernreaktor wird in Abhängigkeit vom mittleren Abbrand (engl. Burnup, Depletion) vereinfacht in sog. Abbrandprogrammen berechnet, zum Beispiel:\n\n\n\nFür eine detaillierte Analyse werden hochkomplexe Reaktorprogrammsysteme \"()\" eingesetzt, deren Leistungsumfang weit über den der zuvor genannten Programme hinausgeht und deren Anwendung eine lange Einarbeitungszeit erfordert, zum Beispiel:\n\n\n\nIn letzteren Programmsystemen sind neben der anfänglichen Materialzusammensetzung auch geometrische Details von Reaktorbauteilen (Zellen) vorzugeben. Von Zeit zu Zeit werden solche Rechnungen mit den Ergebnissen von chemischer und massenspektrometrischer Analyse von entladenem Kernbrennstoff verglichen\nund gegebenenfalls genauere Messungen noch unsicherer Nukleardaten oder genauere Berechnungsmethoden angestoßen.\n\n\nEntwicklung der Massen der Actinoide im Kernreaktor.\nDie Abbildung \"Entwicklung der Massen der Actinoide ...\" zeigt die Zu- oder Abnahme der Massen der 14 häufigsten Actinoid-Nuklide in einem Druckwasserreaktor, der mit angereichertem Uran betrieben wird. Die Zahlenwerte wurden für eine größere Brennstoff-Zelle in Abhängigkeit vom Abbrand (der spezifischen Energiefreisetzung) berechnet. Die Berechnungen wurden im Jahr 2005 mit dem Programmsystem HELIOS&nbsp;1.8 ausgeführt. Als Anfangsanreicherung des Nuklids U wurde für dieses Beispiel 4 % gewählt. Die Punkte auf den Kurven der Abbildung markieren die Schrittweiten in der Abbrandrechnung. Die Schrittweite ist anfangs kleiner, um auch diejenigen Spaltprodukte genauer zu erfassen, die ihre Sättigung sehr schnell erreichen. Das trifft vor allem auf die starken Neutronenabsorber Xe und Sm zu. Eine ähnliche Abbildung, eingeschränkt auf Uran- und Plutonium-Isotope, findet man im Lehrbuch \"Neutron Physics\" von Paul Reuss.\n\nDie Masse jedes Nuklids wird durch die anfangs eingesetzte Masse an Schwermetall, der Masse des Urans, geteilt. Dargestellt sind die Massenanteile aller Nuklide, die anfangs vorhanden waren (drei Uran-Nuklide) oder die mit einer Masse von mindestens 10&nbsp;g pro eingesetzter Tonne Schwermetall nach einem Abbrand von maximal 80&nbsp;MWd/kg gebildet werden.\n\nWie die Abbildung zeigt, nehmen die Massen der drei anfangs vorhandenen Uranisotope U, U und U mit steigendem Abbrand monoton ab. Gleichzeitig nehmen die Massen der höheren Actinoide fast linear zu (man beachte die logarithmische Skalierung der Ordinate). Von allen synthetischen Actinoiden nimmt die Masse des Nuklids Pu am stärksten zu. Damit wächst auch die Anzahl der Spaltungen der Kerne des Nuklids Pu. Ab einem Abbrand von ca. 45&nbsp;MWd/kg nimmt dessen Masse wieder geringfügig ab.\n\nWürde man die Ordinate nach unten auf einen Massenanteil von mindestens 1&nbsp;g pro eingesetzter Tonne Schwermetall herabsetzen, kämen auf der Abbildung ab einem Abrand von ca. 45&nbsp;MWd/kg das Nuklid Am und ab einem Abrand von ca. 60&nbsp;MWd/kg das Nuklid Cm hinzu.\n\nZusammengefasst: In einem Druckwasserreaktor mit Uran-Brennelementen (ohne MOX-Brennelemente) werden aus den ursprünglich vorhandenen Actinoid-Nukliden U und U (und einem geringen Anteil U) maximal 13 synthetische Actinoid-Nuklide innerhalb der üblichen Betriebszeiten gebildet, deren Anteil größer als 1&nbsp;g je Tonne Startmasse Schwermetall&nbsp;(SM) ist, also 1&nbsp;ppm. Das sind die Nuklide U, Np, Pu, Pu, Pu, Pu, Pu, Am, Am, Am, Cm, Cm und Cm. Nuklide der Elemente Berkelium und Californium werden in einem Kernreaktor ebenfalls, aber nur in sehr geringen Mengen gebildet.\n\nDie Actinoide können, wie erwähnt, durch chemische Aufarbeitung von entladenem Brennstoff extrahiert werden. Der Massenanteil von Curium zum Beispiel beträgt ca. 0,00024 bei einem (gegenwärtig üblichen) Abbrand von 60&nbsp;MWd/kg:\nwobei formula_3 die Masse des Curiums und formula_4 die Startmasse des Schwermetalls bedeuten.\nIn einem Leistungsreaktor beträgt die anfängliche Schwermetallmasse ca. formula_5, verteilt auf 193 Brennelemente. Angenommen, alle Brennelemente seien entladen worden, die diesen Abbrandzustand erreicht haben. Folglich ist die Masse des Curiums\nIm gesamten Reaktor sind bei diesem mittleren Abbrand im Brennstoff ca. 24&nbsp;kg Curium entstanden.\n\nAnzumerken ist, dass Leistungsreaktoren nicht betrieben werden, um Actinoide zu gewinnen, sondern um möglichst viele Actinoide zu spalten und Energie freizusetzen. Die Gesamtmasse aller Actinoide verringert sich durch Kernspaltung, und zwar bei einem mittleren Abbrand von 60&nbsp;MWd/kg um insgesamt nur ca. 6 %. Diese findet sich in der Masse der Spaltprodukte wieder.\n\n\nEigenschaften.\n\nNukleare Eigenschaften.\nDie hervorgehobene Stellung der Actinoide, man denke an ihre Bedeutung zur nuklearen Energiefreisetzung und an Kernwaffen, werden durch die Eigenschaften ihrer Atomkerne determiniert.\n\n\nKlassische physikalische Eigenschaften.\nEinige physikalische Eigenschaften der Actinoid-Elemente findet man unter dem Stichwort der Namen der Elemente. Die Voraussetzung dafür, dass klassische physikalische Eigenschaften, zum Beispiel Kristallstruktur, Massendichte, Schmelzpunkt, Siedepunkt oder elektrische Leitfähigkeit gemessen werden können, ist es, dass das Element in wägbaren Mengen gewonnen werden kann. Die Anzahl der tatsächlich gemessenen physikalischen Eigenschaften nimmt mit wachsender Ordnungszahl des Elements schnell ab. Zum Beispiel ist Californium das letzte Actinoid-Element, von dem die Massendichte gemessen werden konnte.\n\nZur Atomphysik der Actinoide ist anzumerken:\n\n\n\n\n\nChemische Eigenschaften.\nAlle Actinoide bilden dreifach geladene Ionen, sie werden wie das Actinium als Untergruppe der 3. Nebengruppe aufgefasst. Die „leichteren“ Actinoide (Thorium bis Americium) kommen in einer größeren Anzahl von Oxidationszahlen vor als die entsprechenden Lanthanoide.\n\n\nVerbindungen.\nDie Eigenschaften beziehen sich auf das häufigste bzw. stabilste Isotop.\n\n\nOxide.\nDie vierwertigen Oxide der Actinoide kristallisieren im kubischen Kristallsystem; der Strukturtyp ist der CaF-Typ (Fluorit) mit der und den Koordinationszahlen \"An\"[8], O[4].\n\n\nHalogenide.\nDie dreiwertigen Chloride der Actinoide kristallisieren im hexagonalen Kristallsystem. Die Struktur des Uran(III)-chlorids ist die Leitstruktur für eine Reihe weiterer Verbindungen. In dieser werden die Metallatome von je neun Chloratomen umgeben. Als Koordinationspolyeder ergibt sich dabei ein dreifach überkapptes, trigonales Prisma, wie es auch bei den späteren Actinoiden und den Lanthanoiden häufig anzutreffen ist. Es kristallisiert im hexagonalen Kristallsystem in der und zwei Formeleinheiten pro Elementarzelle.\n\n\n"}
{"id": "101", "url": "https://de.wikipedia.org/wiki?curid=101", "title": "Americium", "text": "Americium\n\nAmericium () ist ein chemisches Element mit dem Elementsymbol Am und der Ordnungszahl 95. Im Periodensystem steht es in der Gruppe der Actinoide (7.&nbsp;Periode, f-Block) und zählt auch zu den Transuranen. Americium ist neben Europium das einzige nach einem Erdteil benannte Element. Es ist ein leicht verformbares radioaktives Metall silbrig-weißen Aussehens.\n\nVon Americium gibt es kein stabiles Isotop. Auf der Erde kommt es ausschließlich in künstlich erzeugter Form vor. Das Element wurde erstmals im Spätherbst 1944 erzeugt, die Entdeckung jedoch zunächst nicht veröffentlicht. Kurioserweise wurde seine Existenz in einer amerikanischen Radiosendung für Kinder durch den Entdecker Glenn T. Seaborg, den Gast der Sendung, der Öffentlichkeit preisgegeben.\n\nAmericium wird in Kernreaktoren gebildet, eine Tonne abgebrannten Kernbrennstoffs enthält durchschnittlich etwa 100&nbsp;g des Elements. Es wird als Quelle ionisierender Strahlung eingesetzt, z.&nbsp;B. in der Fluoreszenzspektroskopie und in Ionisationsrauchmeldern. Das Americiumisotop Am wurde wegen seiner gegenüber Plutonium (Pu) wesentlich längeren Halbwertszeit von 432,2&nbsp;Jahren zur Befüllung von Radionuklidbatterien (RTG) für Raumsonden vorgeschlagen, welche dann hunderte Jahre lang elektrische Energie zum Betrieb bereitstellen würden.\n\n\nGeschichte.\nAmericium wurde im Spätherbst&nbsp;1944 von Glenn T. Seaborg, Ralph A. James, Leon O. Morgan und Albert Ghiorso im 60-Zoll-Cyclotron an der Universität von Kalifornien in Berkeley sowie am metallurgischen Laboratorium der Universität von Chicago (heute: Argonne National Laboratory) erzeugt. Nach Neptunium und Plutonium war Americium das vierte Transuran, das seit dem Jahr 1940 entdeckt wurde; das um eine Ordnungszahl höhere Curium wurde als drittes schon im Sommer&nbsp;1944 erzeugt. Der Name für das Element wurde in Anlehnung zum Erdteil Amerika gewählt – in Analogie zu Europium, dem Seltene-Erden-Metall, das im Periodensystem genau über Americium steht: \"The name americium (after the Americas) and the symbol Am are suggested for the element on the basis of its position as the sixth member of the actinide rare-earth series, analogous to europium, Eu, of the lanthanide series.\"\n\nZur Erzeugung des neuen Elements wurden in der Regel die Oxide der Ausgangselemente verwendet. Dazu wurde zunächst Plutoniumnitratlösung (mit dem Isotop Pu) auf eine Platinfolie von etwa 0,5&nbsp;cm aufgetragen; die Lösung danach eingedampft und der Rückstand dann zum Oxid (PuO) geglüht. Nach dem Beschuss im Cyclotron wurde die Beschichtung mittels Salpetersäure gelöst, anschließend wieder mit einer konzentrierten wässrigen Ammoniaklösung als Hydroxid ausgefällt; der Rückstand wurde in Perchlorsäure gelöst. Die weitere Trennung erfolgte mit Ionenaustauschern. In ihren Versuchsreihen wurden der Reihe nach vier verschiedene Isotope erzeugt: Am, Am, Am und Am.\n\nAls erstes Isotop isolierten sie Am aus einer Plutonium-Probe, die mit Neutronen bestrahlt wurde. Es zerfällt durch Aussendung eines α-Teilchens in Np. Die Halbwertszeit dieses α-Zerfalls wurde zunächst auf 510&nbsp;±&nbsp;20&nbsp;Jahre bestimmt; der heute allgemein akzeptierte Wert ist 432,2&nbsp;a.\n\nAls zweites Isotop wurde Am durch erneuten Neutronenbeschuss des zuvor erzeugten Am gefunden. Durch nachfolgenden raschen β-Zerfall entsteht dabei Cm, das zuvor schon entdeckte Curium. Die Halbwertszeit dieses β-Zerfalls wurde zunächst auf 17&nbsp;Stunden bestimmt, der heute als gültig ermittelte Wert beträgt 16,02&nbsp;h.\n\nErstmals öffentlich bekannt gemacht wurde die Entdeckung des Elements in der amerikanischen Radiosendung \"Quiz Kids\" am 11.&nbsp;November 1945 durch Glenn T. Seaborg, noch vor der eigentlichen Bekanntmachung bei einem Symposium der American Chemical Society: Einer der jungen Zuhörer fragte den Gast der Sendung, Seaborg, ob während des Zweiten Weltkrieges im Zuge der Erforschung von Nuklearwaffen neue Elemente entdeckt wurden. Seaborg bejahte die Frage und enthüllte dabei auch gleichzeitig die Entdeckung des nächsthöheren Elements, Curium.\n\nAmericium (Am und Am) und seine Produktion wurde später unter dem Namen \"Element 95 and method of producing said element\" patentiert, wobei als Erfinder nur Glenn T. Seaborg angegeben wurde.\n\nIn elementarer Form wurde es erstmals im Jahr 1951 durch Reduktion von Americium(III)-fluorid mit Barium dargestellt.\n\n\nVorkommen.\nAmericiumisotope entstehen im r-Prozess in Supernovae und kommen auf der Erde wegen ihrer im Vergleich zum Alter der Erde zu geringen Halbwertszeit nicht natürlich vor.\n\nHeutzutage wird jedoch Americium als Nebenprodukt in Kernkraftwerken erbrütet; das Americiumisotop Am entsteht als Zerfallsprodukt (u.&nbsp;a. in abgebrannten Brennstäben) aus dem Plutoniumisotop Pu. Eine Tonne abgebrannten Kernbrennstoffs enthält durchschnittlich etwa 100&nbsp;g verschiedener Americiumisotope. Es handelt sich dabei hauptsächlich um die α-Strahler Am und Am, die aufgrund ihrer relativ langen Halbwertszeiten in der Endlagerung unerwünscht sind und deshalb zum Transuranabfall zählen. Eine Verminderung der Langzeitradiotoxizität in nuklearen Endlagern wäre durch Abtrennung langlebiger Isotope aus abgebrannten Kernbrennstoffen möglich. Zur Beseitigung des Americiums wird derzeit die Partitioning & Transmutation-Strategie untersucht.\n\n\nGewinnung und Darstellung.\n\nGewinnung von Americiumisotopen.\nAmericium fällt in geringen Mengen in Kernreaktoren an. Es steht heute in Mengen von wenigen Kilogramm zur Verfügung. Durch die aufwändige Gewinnung aus abgebrannten Brennstäben hat es einen sehr hohen Preis. Seit der Markteinführung 1962 soll der Preis für Americium(IV)-oxid mit dem Isotop Am bei etwa 1500&nbsp;US-Dollar pro Gramm liegen. Das Americiumisotop Am entsteht in geringeren Mengen im Reaktor aus Am und ist deshalb mit 160&nbsp;US-Dollar pro Milligramm Am noch wesentlich teurer.\n\nAmericium wird über das Plutoniumisotop Pu in Kernreaktoren mit hohem U-Anteil zwangsläufig erbrütet, da es aus diesem durch Neutroneneinfang und zwei anschließende β-Zerfälle (über U und Np) entsteht.\n\nDanach wird, wenn es nicht zur Kernspaltung kommt, aus dem Pu, neben anderen Nukliden, durch stufenweisen Neutroneneinfang (n,γ) und anschließenden β-Zerfall Am oder Am erbrütet.\n\nDas Plutonium, welches aus abgebrannten Brennstäben von Leistungsreaktoren gewonnen werden kann, besteht zu etwa 12 % aus dem Isotop Pu. Deshalb erreichen erst 70 Jahre, nachdem der Brutprozess beendet wurde, die abgebrannten Brennstäbe ihren Maximalgehalt von Am; danach nimmt der Gehalt wieder (langsamer als der Anstieg) ab.\n\nAus dem so entstandenen Am kann durch weiteren Neutroneneinfang im Reaktor Am entstehen. Bei Leichtwasserreaktoren soll aus dem Am zu 79 % Am und zu 10 % Am entstehen:\n\nzu 79 %: formula_5\n\nzu 10 %: formula_6\n\nFür die Erbrütung von Am ist ein vierfacher Neutroneneinfang des Pu erforderlich:\n\n\nDarstellung elementaren Americiums.\nMetallisches Americium kann durch Reduktion aus seinen Verbindungen erhalten werden. Zuerst wurde Americium(III)-fluorid zur Reduktion verwendet. Dieses wird hierzu in wasser- und sauerstofffreier Umgebung in Reaktionsapparaturen aus Tantal und Wolfram mit elementarem Barium zur Reaktion gebracht.\n\nAuch die Reduktion von Americium(IV)-oxid mittels Lanthan oder Thorium ergibt metallisches Americium.\n\n\nEigenschaften.\nIm Periodensystem steht das Americium mit der Ordnungszahl 95 in der Reihe der Actinoide, sein Vorgänger ist das Plutonium, das nachfolgende Element ist das Curium. Sein Analogon in der Reihe der Lanthanoiden ist das Europium.\n\n\nPhysikalische Eigenschaften.\nAmericium ist ein radioaktives Metall. Frisch hergestelltes Americium ist ein silberweißes Metall, welches jedoch bei Raumtemperatur langsam matt wird. Es ist leicht verformbar. Sein Schmelzpunkt beträgt 1176&nbsp;°C, der Siedepunkt liegt bei 2607&nbsp;°C. Die Dichte beträgt 13,67&nbsp;g·cm. Es tritt in zwei Modifikationen auf.\n\nDie bei Standardbedingungen stabile Modifikation α-Am kristallisiert im hexagonalen Kristallsystem in der mit den Gitterparametern \"a\"&nbsp;=&nbsp;346,8&nbsp;pm und \"c\"&nbsp;=&nbsp;1124&nbsp;pm sowie vier Formeleinheiten pro Elementarzelle. Die Kristallstruktur besteht aus einer doppelt-hexagonal dichtesten Kugelpackung (d.&nbsp;h.c.p.) mit der Schichtfolge ABAC und ist damit isotyp zur Struktur von α-La.\n\nBei hohem Druck geht α-Am in β-Am über. Die β-Modifikation kristallisiert im kubischen Kristallsystem in der Raumgruppe&nbsp; mit dem Gitterparameter \"a\"&nbsp;=&nbsp;489&nbsp;pm, was einem kubisch flächenzentrierten Gitter (f.c.c.) beziehungsweise einer kubisch dichtesten Kugelpackung mit der Stapelfolge ABC entspricht.\n\nDie Lösungsenthalpie von Americium-Metall in Salzsäure bei Standardbedingungen beträgt −620,6&nbsp;±&nbsp;1,3&nbsp;kJ·mol. Ausgehend von diesem Wert erfolgte die erstmalige Berechnung der Standardbildungsenthalpie (Δ\"H\") von Am auf −621,2&nbsp;±&nbsp;2,0&nbsp;kJ·mol und des Standardpotentials Am&nbsp;/&nbsp;Am auf −2,08&nbsp;±&nbsp;0,01&nbsp;V.\n\n\nChemische Eigenschaften.\nAmericium ist ein sehr reaktionsfähiges Element, das schon mit Luftsauerstoff reagiert und sich gut in Säuren löst. Gegenüber Alkalien ist es stabil.\n\nDie stabilste Oxidationsstufe für Americium ist +3, die Am(III)-Verbindungen sind gegen Oxidation und Reduktion sehr stabil. Mit dem Americium liegt der erste Vertreter der Actinoiden vor, der in seinem Verhalten eher den Lanthanoiden ähnelt als den d-Block-Elementen.\n\nEs ist auch in den Oxidationsstufen +2 sowie +4, +5, +6 und +7 zu finden. Je nach Oxidationszahl variiert die Farbe von Americium in wässriger Lösung ebenso wie in festen Verbindungen:Am&nbsp;(gelbrosa), Am&nbsp;(gelbrot), AmO&nbsp;(gelb), AmO&nbsp;(zitronengelb), AmO&nbsp;(dunkelgrün).\n\nIm Gegensatz zum homologen Europium – Americium hat eine zu Europium analoge Elektronenkonfiguration – kann Am in wässriger Lösung nicht zu Am reduziert werden.\n\nVerbindungen mit Americium ab Oxidationszahl +4 aufwärts sind starke Oxidationsmittel, vergleichbar dem Permanganat-Ion (MnO) in saurer Lösung.\n\nDie in wässriger Lösung nicht beständigen Am-Ionen lassen sich nur noch mit starken Oxidationsmitteln aus Am(III) darstellen. In fester Form sind zwei Verbindungen des Americiums in der Oxidationsstufe +4 bekannt: Americium(IV)-oxid (AmO) und Americium(IV)-fluorid (AmF).\n\nDer fünfwertige Oxidationszustand wurde beim Americium erstmals 1951 beobachtet. In wässriger Lösung liegen primär AmO-Ionen (sauer) oder AmO-Ionen (alkalisch) vor, die jedoch instabil sind und einer raschen Disproportionierung unterliegen:\n\nZunächst ist von einer Disproportionierung zur Oxidationsstufe +6 und +4 auszugehen:\n\nEtwas beständiger als Am(IV) und Am(V) sind die Americium(VI)-Verbindungen. Sie lassen sich aus Am(III) durch Oxidation mit Ammoniumperoxodisulfat in verdünnter Salpetersäure herstellen. Der typische rosafarbene Ton verschwindet in Richtung zu einer starken Gelbfärbung. Zudem kann die Oxidation mit Silber(I)-oxid in Perchlorsäure quantitativ erreicht werden. In Natriumcarbonat- oder Natriumhydrogencarbonat-Lösungen ist eine Oxidation mit Ozon oder Natriumperoxodisulfat gleichfalls möglich.\n\n\nBiologische Aspekte.\nEine biologische Funktion des Americiums ist nicht bekannt. Vorgeschlagen wurde der Einsatz immobilisierter Bakterienzellen zur Entfernung von Americium und anderen Schwermetallen aus Fließgewässern. So können Enterobakterien der Gattung \"Citrobacter\" durch die Phosphataseaktivität in ihrer Zellwand bestimmte Americiumnuklide aus wässriger Lösung ausfällen und als Metall-Phosphat-Komplex binden. Ferner wurden die Faktoren untersucht, die die Biosorption und Bioakkumulation des Americiums durch Bakterien und Pilze beeinflussen.\n\n\nSpaltbarkeit.\nDas Isotop Am hat mit rund 5700&nbsp;barn den höchsten bisher (10/2008) gemessenen thermischen Spaltquerschnitt. Damit geht eine kleine kritische Masse einher, weswegen Am als Spaltmaterial vorgeschlagen wurde, um beispielsweise Raumschiffe mit Kernenergieantrieb anzutreiben.\n\nDieses Isotop eignet sich prinzipiell auch zum Bau von Kernwaffen. Die kritische Masse einer reinen Am-Kugel beträgt etwa 9–14&nbsp;kg. Die Unsicherheiten der verfügbaren Wirkungsquerschnitte lassen derzeit keine genauere Aussage zu. Mit Reflektor beträgt die kritische Masse noch etwa 3–5&nbsp;kg. In wässriger Lösung wird sie nochmals stark herabgesetzt. Auf diese Weise ließen sich sehr kompakte Sprengköpfe bauen. Nach öffentlichem Kenntnisstand wurden bisher keine Kernwaffen aus Am gebaut, was mit der geringen Verfügbarkeit und dem hohen Preis begründet werden kann.\n\nAus denselben Gründen wird Am auch nicht als Kernbrennstoff in Kernreaktoren eingesetzt, obwohl es dazu prinzipiell sowohl in thermischen als auch in schnellen Reaktoren geeignet wäre., Auch die beiden anderen häufiger verfügbaren Isotope, Am und Am können in einem schnellen Reaktor eine Kettenreaktion aufrechterhalten. Die kritischen Massen sind hier jedoch sehr hoch. Sie betragen unreflektiert 57,6–75,6&nbsp;kg bei Am und 209&nbsp;kg bei Am so dass sich durch die Verwendung keine Vorteile gegenüber herkömmlichen Spaltstoffen ergeben.\n\nEntsprechend ist Americium rechtlich nach Abs. 1 des Atomgesetzes nicht den Kernbrennstoffen zugeordnet. Es existieren jedoch Vorschläge, sehr kompakte Reaktoren mit einem Americium-Inventar von lediglich knapp 20&nbsp;g zu konstruieren, die in Krankenhäusern als Neutronenquelle für die Neutroneneinfangtherapie verwendet werden können.\n\n\nIsotope.\nVon Americium sind 16 Isotope und 11 Kernisomere mit Halbwertszeiten zwischen Bruchteilen von Mikrosekunden und 7370&nbsp;Jahren bekannt. Es gibt zwei langlebige α-strahlende Isotope Am mit 432,2 und Am mit 7370&nbsp;Jahren Halbwertszeit. Außerdem hat das Kernisomer Am mit 141&nbsp;Jahren eine lange Halbwertszeit. Die restlichen Kernisomere und Isotope haben mit 0,64&nbsp;µs bei Am bis 50,8&nbsp;Stunden bei Am kurze Halbwertszeiten.\n\nAm ist das am häufigsten erbrütete Americiumisotop und liegt auf der Neptunium-Reihe. Es zerfällt mit einer Halbwertszeit von 432,2&nbsp;Jahren mit einem α-Zerfall zu Np. Am gibt nur mit einer Wahrscheinlichkeit von 0,35 % die gesamte Zerfallsenergie mit dem α-Teilchen ab, sondern emittiert meistens noch ein oder mehrere Gammaquanten.\n\nAm ist kurzlebig und zerfällt mit einer Halbwertszeit von 16,02&nbsp;h zu 82,7 % durch β-Zerfall zu Cm und zu 17,3 % durch Elektroneneinfang zu Pu. Das Cm zerfällt zu Pu und dieses weiter zu U, das auf der Uran-Radium-Reihe liegt. Das Pu zerfällt über die gleiche Zerfallskette wie Pu. Während jedoch Pu als Seitenarm beim U auf die Zerfallskette kommt, steht Pu noch vor dem U. Pu zerfällt durch α-Zerfall in U, den Beginn der natürlichen Uran-Radium-Reihe.\n\nAm zerfällt mit einer Halbwertszeit von 141&nbsp;Jahren zu 99,541 % durch Innere Konversion zu Am und zu 0,459 % durch α-Zerfall zu Np. Dieses zerfällt zu Pu und dann weiter zu U, das auf der Uran-Radium-Reihe liegt.\n\nAm ist mit einer Halbwertszeit von 7370&nbsp;Jahren das langlebigste Americiumisotop. Es geht zunächst durch α-Strahlung in Np über, das durch β-Zerfall weiter zu Pu zerfällt. Das Pu zerfällt durch α-Strahlung zu Uran U, dem offiziellen Anfang der Uran-Actinium-Reihe.\n\nDie Americiumisotope mit ungerader Neutronenzahl, also gerader Massenzahl, sind gut durch thermische Neutronen spaltbar.\n\n\"→ Liste der Americiumisotope\"\n\n\nVerwendung.\nFür die Verwendung von Americium sind vor allem die beiden langlebigsten Isotope Am und Am von Interesse. In der Regel wird es in Form des Oxids (AmO) verwendet.\n\nIonisationsrauchmelder.\nDie α-Strahlung des Am wird in Ionisationsrauchmeldern genutzt. Es wird gegenüber Ra bevorzugt, da es vergleichsweise wenig γ-Strahlung emittiert. Dafür muss aber die Aktivität gegenüber Radium ca. das Fünffache betragen. Die Zerfallsreihe von Am „endet“ für den Verwendungszeitraum quasi direkt nach dessen α-Zerfall bei Np, das eine Halbwertszeit von rund 2,144 Millionen Jahren besitzt.\n\n\nRadionuklidbatterien.\nAm wurde wegen seiner gegenüber Pu wesentlich längeren Halbwertszeit zur Befüllung von Radionuklidbatterien (RTG) von Raumsonden vorgeschlagen. Dank seiner Halbwertszeit von 432,2&nbsp;Jahren könnte ein RTG mit Am-Füllung hunderte Jahre lang – anstatt nur einige Jahrzehnte (wie mit einer Pu-Füllung) – elektrische Energie zum Betrieb einer Raumsonde bereitstellen. Es soll voraussichtlich in den Radionuklidbatterien zum Einsatz kommen, deren Entwicklung die ESA erwägt und deren Entwicklung in den 2020er-Jahren abgeschlossen werden könnte.\n\n\nNeutronenquellen.\nAm als Oxid mit Beryllium verpresst stellt eine Neutronenquelle dar, die beispielsweise für radiochemische Untersuchungen eingesetzt wird. Hierzu wird der hohe Wirkungsquerschnitt des Berylliums für (α,n)-Kernreaktionen ausgenutzt, wobei das Americium als Produzent der α-Teilchen dient. Die entsprechenden Reaktionsgleichungen lauten:\n\nDerartige Neutronenquellen kommen beispielsweise in der Neutronenradiographie und -tomographie zum Einsatz.\n\n\nIonisator.\nNeben dem häufig verwendeten Po als Ionisator zur Beseitigung von unerwünschter elektrostatischer Aufladung kam auch Am zum Einsatz. Dazu wurde z.&nbsp;B. die Quelle am Kopf einer Bürste montiert mit der man langsam über die zu behandelnden Oberflächen strich und dadurch eine Wiederverschmutzung durch elektrostatisch angezogene Staubpartikel vermeiden konnte.\n\n\nHerstellung anderer Elemente.\nAmericium ist Ausgangsmaterial zur Erzeugung höherer Transurane und auch der Transactinoide. Aus Am entsteht zu 82,7 % Curium (Cm) und zu 17,3 % Plutonium (Pu). Im Kernreaktor wird zwangsläufig in geringen Mengen durch Neutroneneinfang aus Am das Am erbrütet, das durch β-Zerfall zum Curiumisotop Cm zerfällt.\n\nIn Teilchenbeschleunigern führt zum Beispiel der Beschuss von Am mit Kohlenstoffkernen (C) beziehungsweise Neonkernen (Ne) zu den Elementen Einsteinium Es beziehungsweise Dubnium Db.\n\n\nSpektrometer.\nMit seiner intensiven Gammastrahlungs-Spektrallinie bei 60&nbsp;keV eignet sich Am gut als Strahlenquelle für die Röntgen-Fluoreszenzspektroskopie. Dies wird auch zur Kalibrierung von Gammaspektrometern im niederenergetischen Bereich verwendet, da die benachbarten Linien vergleichsweise schwach sind und so ein einzeln stehender Peak entsteht. Zudem wird der Peak nur vernachlässigbar durch das Compton-Kontinuum höherenergetischer Linien gestört, da diese ebenfalls höchstens mit einer um mindestens drei Größenordnungen geringeren Intensität auftreten.\n\n\nSicherheitshinweise und Gefahren.\nEinstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen, welche eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielt. Eine chemische Gefahr liegt überhaupt nur dann vor, wenn es sich um eine dafür relevante Stoffmenge handelt.\n\nDa von Americium nur radioaktive Isotope existieren, darf es selbst sowie seine Verbindungen nur in geeigneten Laboratorien unter speziellen Vorkehrungen gehandhabt werden. Die meisten gängigen Americiumisotope sind α-Strahler, weshalb eine Inkorporation unbedingt vermieden werden muss. Das breite Spektrum der hieraus resultierenden meist ebenfalls radioaktiven Tochternuklide stellt ein weiteres Risiko dar, das bei der Wahl der Sicherheitsvorkehrungen berücksichtigt werden muss. Am gibt beim radioaktiven Zerfall große Mengen relativ weicher Gammastrahlung ab, die sich gut abschirmen lässt.\n\nNach Untersuchungen des Forschers Arnulf Seidel vom Institut für Strahlenbiologie des Kernforschungszentrums Karlsruhe erzeugt Americium (wie Plutonium), bei Aufnahme in den Körper, mehr Knochentumore als dieselbe Dosis Radium.\n\nDie biologische Halbwertszeit von Am beträgt in den Knochen 50 Jahre und in der Leber 20 Jahre. In den Gonaden verbleibt es dagegen offensichtlich dauerhaft.\n\n\nVerbindungen.\n\"→ Kategorie: \"\n\n\nOxide.\nVon Americium existieren Oxide der Oxidationsstufen +3 (AmO) und +4 (AmO).\n\nAmericium(III)-oxid (AmO) ist ein rotbrauner Feststoff und hat einen Schmelzpunkt von 2205&nbsp;°C.\n\nAmericium(IV)-oxid (AmO) ist die wichtigste Verbindung dieses Elements. Nahezu alle Anwendungen dieses Elements basieren auf dieser Verbindung. Sie entsteht unter anderem implizit in Kernreaktoren beim Bestrahlen von Urandioxid (UO) bzw. Plutoniumdioxid (PuO) mit Neutronen. Es ist ein schwarzer Feststoff und kristallisiert – wie die anderen Actinoiden(IV)-oxide – im kubischen Kristallsystem in der Fluorit-Struktur.\n\n\nHalogenide.\nHalogenide sind für die Oxidationsstufen +2, +3 und +4 bekannt. Die stabilste Stufe +3 ist für sämtliche Verbindungen von Fluor bis Iod bekannt und in wässriger Lösung stabil.\n\nAmericium(III)-fluorid (AmF) ist schwerlöslich und kann durch die Umsetzung einer wässrigen Americiumlösung mit Fluoridsalzen im schwach Sauren durch Fällung hergestellt werden:\n\nDas tetravalente Americium(IV)-fluorid (AmF) ist durch die Umsetzung von Americium(III)-fluorid mit molekularem Fluor zugänglich:\n\nIn der wässrigen Phase wurde das vierwertige Americium auch beobachtet.\n\nAmericium(III)-chlorid (AmCl) bildet rosafarbene hexagonale Kristalle. Seine Kristallstruktur ist isotyp mit Uran(III)-chlorid. Der Schmelzpunkt der Verbindung liegt bei 715&nbsp;°C. Das Hexahydrat (AmCl·6&nbsp;HO) weist eine monokline Kristallstruktur auf.\n\nDurch Reduktion mit Na-Amalgam aus Am(III)-Verbindungen sind Am(II)-Salze zugänglich: die schwarzen Halogenide AmCl, AmBr und AmI. Sie sind sehr sauerstoffempfindlich, und oxidieren in Wasser unter Freisetzung von Wasserstoff zu Am(III)-Verbindungen.\n\nChalkogenide und Pentelide.\nVon den Chalkogeniden sind bekannt: das Sulfid (AmS), zwei Selenide (AmSe und AmSe) und zwei Telluride (AmTe und AmTe).\n\nDie Pentelide des Americiums (Am) des Typs AmX sind für die Elemente Phosphor, Arsen, Antimon und Bismut dargestellt worden. Sie kristallisieren im NaCl-Gitter.\n\n\nSilicide und Boride.\nAmericiummonosilicid (AmSi) und Americium„disilicid“ (AmSi mit: 1,87 < x < 2,0) wurden durch Reduktion von Americium(III)-fluorid mit elementaren Silicium im Vakuum bei 1050&nbsp;°C (AmSi) und 1150–1200&nbsp;°C (AmSi) dargestellt. AmSi ist eine schwarze Masse, isomorph mit LaSi. AmSi ist eine hellsilbrige Verbindung mit einem tetragonalen Kristallgitter.\n\nBoride der Zusammensetzungen AmB und AmB sind gleichfalls bekannt.\n\n\nMetallorganische Verbindungen.\nAnalog zu Uranocen, einer Organometallverbindung in der Uran von zwei Cyclooctatetraen-Liganden komplexiert ist, wurden die entsprechenden Komplexe von Thorium, Protactinium, Neptunium, Plutonium und auch des Americiums, (η-CH)Am, dargestellt.\n\n\n"}
{"id": "102", "url": "https://de.wikipedia.org/wiki?curid=102", "title": "Atom", "text": "Atom\n\nAtome (von „unteilbar“) sind die Bausteine, aus denen alle festen, flüssigen oder gasförmigen Stoffe bestehen. Alle Materialeigenschaften dieser Stoffe sowie ihr Verhalten in chemischen Reaktionen werden durch die Eigenschaften und die räumliche Anordnung ihrer Atome festgelegt. Jedes Atom gehört zu einem bestimmten chemischen Element und bildet dessen kleinste Einheit. Zurzeit sind 118 Elemente bekannt, von denen etwa 90 auf der Erde natürlich vorkommen. Atome verschiedener Elemente unterscheiden sich in ihrer Größe und Masse und vor allem in ihrer Fähigkeit, mit anderen Atomen chemisch zu reagieren und sich zu Molekülen oder festen Körpern zu verbinden. Die Durchmesser von Atomen liegen im Bereich von 6&#8239;·&#8239;10&nbsp;m (Helium) bis 5&#8239;·&#8239;10&nbsp;m (Cäsium), ihre Massen in einem Bereich von 1,7&#8239;·&#8239;10&nbsp;kg (Wasserstoff) bis knapp 5&#8239;·10&nbsp;kg (die derzeit schwersten synthetisch hergestellten Kerne).\n\nAtome sind nicht unteilbar, wie zum Zeitpunkt der Namensgebung angenommen, sondern zeigen einen wohlbestimmten Aufbau aus noch kleineren Teilchen. Sie bestehen aus einem Atomkern und einer Atomhülle. Der Atomkern hat einen Durchmesser von etwa einem Zehn- bis Hunderttausendstel des gesamten Atomdurchmessers, enthält jedoch über 99,9&nbsp;Prozent der Atommasse. Er besteht aus positiv geladenen Protonen und einer Anzahl von etwa gleich schweren, elektrisch neutralen Neutronen. Diese Nukleonen sind durch die starke Wechselwirkung aneinander gebunden. Die Hülle besteht aus negativ geladenen Elektronen. Sie trägt mit weniger als 0,06&nbsp;Prozent zur Masse bei, bestimmt jedoch die Größe des Atoms. Der positive Kern und die negative Hülle sind durch elektrostatische Anziehung aneinander gebunden. In der elektrisch neutralen Grundform des Atoms ist die Anzahl der Elektronen in der Hülle gleich der Anzahl der Protonen im Kern. Diese Zahl legt den genauen Aufbau der Hülle und damit auch das chemische Verhalten des Atoms fest und wird deshalb als \"chemische Ordnungszahl\" bezeichnet. Alle Atome desselben Elements haben die gleiche chemische Ordnungszahl. Sind zusätzliche Elektronen vorhanden oder fehlen welche, ist das Atom negativ bzw. positiv geladen und wird als Ion bezeichnet.\n\nDie Vorstellung vom atomaren Aufbau der Materie existierte bereits in der Antike, war jedoch bis in die Neuzeit umstritten. Der endgültige Nachweis konnte erst Anfang des 20.&nbsp;Jahrhunderts erbracht werden und gilt als eine der bedeutendsten Entdeckungen in Physik und Chemie. Einzelne Atome sind selbst mit den stärksten Lichtmikroskopen nicht zu erkennen. Eine direkte Beobachtung einzelner Atome ist erst seit Mitte des 20.&nbsp;Jahrhunderts mit Feldionenmikroskopen möglich, seit einigen Jahren auch mit Rastertunnelmikroskopen und hochauflösenden Elektronenmikroskopen. Die Atomphysik, die neben dem Aufbau der Atome auch die Vorgänge in ihrem Inneren und ihre Wechselwirkungen mit anderen Atomen erforscht, hat entscheidend zur Entwicklung der modernen Physik und insbesondere der Quantenmechanik beigetragen.\n\n\nErforschungsgeschichte.\nDie Vorstellung vom atomaren Aufbau der Materie existierte bereits in der Antike. Aufgrund ihrer extrem geringen Größe sind einzelne Atome selbst mit den stärksten Lichtmikroskopen nicht zu erkennen, noch Anfang des 20.&nbsp;Jahrhunderts war ihre Existenz umstritten. Der endgültige Nachweis gilt als eine der bedeutendsten Entdeckungen in Physik und Chemie. Einen entscheidenden Beitrag lieferte Albert Einstein 1905, indem er die bereits seit langem bekannte, im Mikroskop direkt sichtbare Brownsche Bewegung kleiner Körnchen durch zufällige Stöße von Atomen oder Molekülen in deren Umgebung erklärte. Erst seit wenigen Jahrzehnten erlauben Feldionenmikroskope und Rastertunnelmikroskope, seit einigen Jahren zudem auch Elektronenmikroskope, einzelne Atome direkt zu beobachten.\n\n\nPhilosophische Überlegungen.\nDas Konzept des Atomismus, nämlich dass Materie aus Grundeinheiten aufgebaut ist – „kleinsten Teilchen“, die nicht immer weiter in kleinere Stücke zerteilt werden können – existiert seit Jahrtausenden, genauso wie das Gegenkonzept, Materie sei ein beliebig teilbares Kontinuum. Doch diese Ideen beruhten zunächst ausschließlich auf philosophischen Überlegungen und nicht auf empirischer experimenteller Untersuchung. Dabei wurden den Atomen verschiedene Eigenschaften zugeschrieben, und zwar je nach Zeitalter, Kultur und philosophischer Schule sehr unterschiedliche.\n\nEine frühe Erwähnung des Atomkonzepts in der Philosophie ist aus Indien bekannt. Die Nyaya- und Vaisheshika-Schulen entwickelten ausgearbeitete Theorien, wie sich Atome zu komplexeren Gebilden zusammenschlössen (erst in Paaren, dann je drei Paare).\n\nExperimentell arbeitende Naturwissenschaftler machten sich Ende des 18.&nbsp;Jahrhunderts die Hypothese vom Atom zu eigen, weil diese Hypothese im Rahmen eines Teilchenmodells der Materie eine elegante Erklärung für neue Entdeckungen in der Chemie bot. Doch wurde gleichzeitig die gegenteilige Vorstellung, Materie sei ein Kontinuum, von Philosophen und auch unter Naturwissenschaftlern noch bis ins 20.&nbsp;Jahrhundert hinein aufrechterhalten.\n\nIn der griechischen Philosophie ist die Atomvorstellung erstmals im 5.&nbsp;Jahrhundert v.&nbsp;Chr. bei Leukipp überliefert. Sein Schüler Demokrit systematisierte sie und führte den Begriff () ein, was etwa „das Unzerschneidbare“ bedeutet, also ein nicht weiter zerteilbares Objekt. Diese Bezeichnung wurde Ende des 18.&nbsp;Jahrhunderts für die damals hypothetischen kleinsten Einheiten der chemischen Elemente der beginnenden modernen Chemie übernommen, denn mit chemischen Methoden lassen sich Atome in der Tat nicht „zerschneiden“.\n\n\nNaturwissenschaftliche Erforschung.\nIm Rahmen der wissenschaftlichen Erforschung konnte die Existenz von Atomen bestätigt werden. Es wurden viele verschiedene Atommodelle entwickelt, um ihren Aufbau zu beschreiben. Insbesondere das Wasserstoffatom als das einfachste aller Atome war dabei wichtig. Einige der Modelle werden heute nicht mehr verwendet und sind nur von wissenschaftsgeschichtlichem Interesse. Andere gelten je nach Anwendungsbereich als Näherung noch heute. In der Regel wird das einfachste Modell genommen, welches im gegebenen Zusammenhang noch ausreicht, um die auftretenden Fragen zu klären.\n\n\nBestätigung der Atomhypothese.\nRobert Boyle vertrat 1661 in seinem Werk \"The Sceptical Chymist\" die Meinung, die Materie sei aus diversen Kombinationen verschiedener \"corpuscules\" aufgebaut und nicht aus den vier Elementen der Alchemie: Wasser, Erde, Feuer, Luft. Damit bereitete er die Überwindung der Alchemie durch den Element- und Atombegriff der modernen Chemie vor.\n\nDaniel Bernoulli zeigte 1740, dass der gleichmäßige Druck von Gasen auf die Behälterwände und insbesondere das Gesetz von Boyle und Mariotte sich durch zahllose Stöße kleinster Teilchen erklären lässt. Damit wurde seine Forschung zum Vorläufer der kinetischen Gastheorie und statistischen Mechanik.\n\nAb Ende des 18.&nbsp;Jahrhunderts wurde die Vorstellung von Atomen genutzt, um die wohlbestimmten Winkel an den Kanten und Ecken der Edelsteine auf die verschiedenen möglichen Schichtungen von harten Kugeln zurückzuführen.\n\nNachdem Antoine Lavoisier 1789 den heutigen Begriff des chemischen Elements geprägt und die ersten Elemente richtig identifiziert hatte, benutzte 1803 John Dalton das Atomkonzept, um zu erklären, wieso Elemente immer in Mengenverhältnissen kleiner ganzer Zahlen miteinander reagieren (Gesetz der multiplen Proportionen). Er nahm an, dass jedes Element aus gleichartigen Atomen besteht, die sich nach festen Regeln miteinander verbinden können und so Stoffe mit anderen Materialeigenschaften bilden. Außerdem ging er davon aus, dass alle Atome eines Elements die gleiche Masse hätten, und begründete den Begriff Atomgewicht.\n\nDie Beobachtungen zum chemischen und physikalischen Verhalten von Gasen konnte Amedeo Avogadro 1811 dahingehend zusammenfassen, dass zwei näherungsweise ideale Gase bei gleichen Werten von Volumen, Druck und Temperatur des Gases immer aus gleich vielen identischen Teilchen („Molekülen“) bestehen. Die Moleküle bestehen bei elementaren Gasen wie Wasserstoff, Sauerstoff oder Stickstoff immer aus zwei Atomen des Elements (Avogadrosches Gesetz).\n\n1866 konnte Johann Loschmidt die Größe der Luftmoleküle bestimmen, indem er mit der von James C. Maxwell aus der kinetischen Gastheorie gewonnenen Formel die von George Stokes gemessenen Werte für die innere Reibung in Luft auswertete. Damit konnte er das Gewicht eines Luftmoleküls bestimmen. Außerdem erhielt er die nach ihm benannte Loschmidtsche Zahl als Anzahl der Luftmoleküle pro Kubikzentimeter (unter Normalbedingungen (T&nbsp;=&nbsp;273,15&nbsp;K = 0&nbsp;°C) und (p&nbsp;=&nbsp;101,325&nbsp;kPa)).\n\nInfolge der Arbeiten von Avogadro und Stanislao Cannizzaro wurde angenommen, dass Atome nicht als einzelne Teilchen auftreten, sondern nur als Bestandteile von Molekülen aus mindestens zwei Atomen. Doch 1876 gelang August Kundt und Emil Warburg der erste Nachweis eines einatomigen Gases. Sie bestimmten den Adiabatenexponenten von Quecksilber-Dampf bei hoher Temperatur und erhielten einen Wert, wie er nach der kinetischen Gastheorie nur für Teilchen in Gestalt echter Massepunkte auftreten kann. Ab 1895 kamen entsprechende Beobachtungen an den neu entdeckten Edelgasen hinzu.\n\nNach Erscheinen seiner Dissertation über die Bestimmung von Moleküldimensionen schlug Albert Einstein im selben Jahr 1905 ein Experiment vor, um die Hypothese von der Existenz der Atome anhand der Zitterbewegung kleiner Partikel in Wasser quantitativ zu prüfen. Nach seiner Theorie müssten die Partikel aufgrund der Unregelmäßigkeit der Stöße durch die Wassermoleküle kleine, aber immerhin unter dem Mikroskop sichtbare Bewegungen ausführen. Es war Einstein dabei zunächst nicht bekannt, dass er damit die seit 1827 bekannte Brownsche Bewegung von Pollen quantitativ erklärt hatte, für deren Ursache schon 1863 Christian Wiener erstmals Molekularstöße angenommen hatte. Der französische Physiker Jean Perrin bestimmte auf der Grundlage von Einsteins Theorie die Masse und Größe von Molekülen experimentell und fand ähnliche Ergebnisse wie Loschmidt. Diese Arbeiten trugen entscheidend zur allgemeinen Anerkennung der bis dahin so genannten „Atomhypothese“ bei.\n\n\nTeilbarkeit und Aufbau der Atome.\nJoseph John Thomson entdeckte 1897, dass die Kathodenstrahlen aus Teilchen bestimmter Ladung und Masse bestehen, deren Masse kleiner als ein Tausendstel der Atommasse ist. Diese Teilchen wurden als \"Elektronen\" bezeichnet und erwiesen sich als ein Bestandteil aller Materie, was dem Konzept des Atoms als unzerteilbarer Einheit widersprach. Thomson glaubte, dass die Elektronen dem Atom seine Masse verliehen und dass sie im Atom in einem masselosen, positiv geladenen Medium verteilt seien wie „Rosinen in einem Kuchen“ (Thomsonsches Atommodell).\n\nDie kurz zuvor entdeckte Radioaktivität wurde 1903 von Ernest Rutherford und Frederick Soddy mit Umwandlungen verschiedener Atomsorten ineinander in Verbindung gebracht. Sie konnten 1908 nachweisen, dass α-Teilchen, die bei Alphastrahlung ausgesandt werden, Helium-Atome bilden.\n\nZusammen mit seiner Forschergruppe beschoss Ernest Rutherford 1909 eine Goldfolie mit α-Teilchen. Er stellte fest, dass die meisten der Teilchen die Folie fast ungehindert durchdrangen, einige wenige aber um sehr viel größere Winkel abgelenkt wurden, als nach Thomsons Modell möglich. Rutherford schloss daraus, dass fast die ganze Masse des Atoms in einem sehr viel kleineren, elektrisch geladenen Volumen in der Mitte des Atoms konzentriert sei und schuf damit das seitdem gültige Rutherfordsche Atommodell mit dem grundlegenden Aufbau des Atoms aus Atomkern und Atomhülle. Die stark abgelenkten α-Teilchen waren diejenigen, die einem Kern zufällig näher als etwa ein Hundertstel des Atomradius gekommen waren. Die Ladungszahl des Atomkerns entpuppte sich als die chemische Ordnungszahl des betreffenden Elements, und α-Teilchen erwiesen sich als die Atomkerne des Heliums.\n\nDer Chemiker Frederick Soddy stellte 1911 fest, dass manche der natürlichen radioaktiven Elemente aus Atomen mit unterschiedlichen Massen und unterschiedlicher Radioaktivität bestehen mussten. Der Begriff Isotop für physikalisch verschiedene Atome desselben chemischen Elements wurde 1913 von Margaret Todd vorgeschlagen. Da die Isotope desselben Elements an ihrem chemischen Verhalten nicht zu unterscheiden waren, entwickelte der Physiker J.J.&nbsp;Thomson ein erstes Massenspektrometer zu ihrer physikalischen Trennung. Damit konnte er 1913 am Beispiel von Neon nachweisen, dass es auch stabile Elemente mit mehreren Isotopen gibt.\n\n1918 fand Francis William Aston mit einem Massenspektrometer von erheblich größerer Genauigkeit heraus, dass fast alle Elemente Gemische aus mehreren Isotopen sind, wobei die Massen der einzelnen Isotope immer (nahezu) ganzzahlige Vielfache der Masse des Wasserstoffatoms sind. Rutherford wies 1919 in der ersten beobachteten Kernreaktion nach, dass durch Beschuss mit α-Teilchen aus den Kernen von Stickstoffatomen die Kerne von Wasserstoffatomen herausgeschossen werden können. Diesen gab er den Namen Proton und entwickelte ein Atommodell, in dem die Atome nur aus Protonen und Elektronen bestehen, wobei die Protonen und ein Teil der Elektronen den kleinen, schweren Atomkern bilden, die übrigen Elektronen die große, leichte Atomhülle. Die Vorstellung von Elektronen im Atomkern stellte sich jedoch als falsch heraus und wurde fallengelassen, nachdem 1932 von James Chadwick das Neutron als ein neutraler Kernbaustein mit etwa gleicher Masse wie das Proton nachgewiesen wurde. Damit entstand das heutige Atommodell: Der Atomkern ist zusammengesetzt aus so vielen Protonen wie die Ordnungszahl angibt, und zusätzlich so vielen Neutronen, dass die betreffende Isotopenmasse erreicht wird.\n\n\nQuantenmechanische Atommodelle.\nDie beobachteten Eigenschaften (wie Größe, Stabilität, Reaktionsweisen) der Atomkerne und Atomhüllen konnten im Rahmen der klassischen Physik keine Erklärung finden. Erklärungsversuche setzten an der Annahme an, dass die Hülle aus Elektronen besteht, die durch elektrostatische Anziehung im Atom gebunden sind. So konnte Niels Bohr, aufbauend auf Rutherfords Atommodell aus Kern und Hülle, 1913 erstmals erklären, wie es in den optischen Spektren reiner Elemente zu den Spektrallinien kommt, die für das jeweilige Element absolut charakteristisch sind (Spektralanalyse nach Robert Wilhelm Bunsen und Gustav Robert Kirchhoff 1859). Bohr nahm an, dass die Elektronen sich nur auf bestimmten quantisierten Umlaufbahnen (Schalen) aufhalten und von einer zur anderen „springen“, sich jedoch nicht dazwischen aufhalten können. Beim Quantensprung von einer äußeren zu einer weiter innen liegenden Bahn muss das Elektron eine bestimmte Menge an Energie abgeben, die als Lichtquant bestimmter Wellenlänge erscheint. Im Franck-Hertz-Versuch konnte die quantisierte Energieaufnahme und -abgabe an Quecksilberatomen experimentell bestätigt werden. Das Bohrsche Atommodell ergab zwar nur für Systeme mit lediglich einem Elektron (damals nur Wasserstoff und ionisiertes Helium) quantitativ richtige Resultate. Jedoch bildete es im Laufe des folgenden Jahrzehnts das Fundament für eine Reihe von Verfeinerungen, die zu einem qualitativen Verständnis des Aufbaus der Elektronenhüllen aller Elemente führten. Damit wurde das Bohrsche Atommodell zur Grundlage des populären Bildes vom Atom als einem kleinen Planetensystem.\n\n1916 versuchte Gilbert Newton Lewis, im Rahmen des Bohrschen Atommodells die chemische Bindung durch Wechselwirkung der Elektronen eines Atoms mit einem anderen Atom zu erklären. Walther Kossel ging 1916 erstmals von abgeschlossenen „Elektronenschalen“ bei den Edelgasen aus, um zu erklären, dass die chemischen Eigenschaften der Elemente grob periodisch mit der Ordnungszahl variieren, wobei sich Elemente, die im Periodensystem benachbart sind, in der Elektronenzahl um eins oder zwei unterscheiden. Dies wurde bis 1921 von Niels Bohr zum „Aufbauprinzip“ weiterentwickelt, wonach mit zunehmender Kernladungszahl jedes weitere Elektron in die jeweils energetisch niedrigste Elektronenschale der Atomhülle, die noch Plätze frei hat, aufgenommen wird, ohne dass die schon vorhandenen Elektronen sich wesentlich umordnen.\n\nAufbauend auf dem von Louis de Broglie 1924 postulierten Welle-Teilchen-Dualismus entwickelte Erwin Schrödinger 1926 die Wellenmechanik. Sie beschreibt die Elektronen nicht als Massenpunkte auf bestimmten Bahnen, sondern als dreidimensionale \"Materiewellen\". Als Folge dieser Beschreibung ist es unter anderem unzulässig, einem Elektron gleichzeitig genaue Werte für Ort und Impuls zuzuschreiben. Dieser Sachverhalt wurde 1927 von Werner Heisenberg in der Unschärferelation formuliert. Demnach können statt der Bewegung auf bestimmten Bahnen nur \"Wahrscheinlichkeitsverteilungen\" für Wertebereiche von Ort und Impuls angegeben werden, eine Vorstellung, die nur schwer zu veranschaulichen ist. Den quantisierten Umlaufbahnen des Bohrschen Modells entsprechen hier stehende Materiewellen oder „Atomorbitale“. Sie geben unter anderem an, wie sich in der Nähe des Atomkerns die Aufenthaltswahrscheinlichkeit der Elektronen konzentriert, und bestimmen damit die wirkliche Größe des Atoms.\n\nDie Beschreibung der Eigenschaften der Atome gelang mit diesem ersten vollständig quantenmechanischen Atommodell sehr viel besser als mit den Vorläufermodellen. Insbesondere ließen sich auch bei Atomen mit mehreren Elektronen die Spektrallinien und die Struktur der Atomhülle in räumlicher und energetischer Hinsicht darstellen, einschließlich der genauen Möglichkeiten, mit den Atomhüllen anderer Atome gebundene Zustände zu bilden, also stabile Moleküle. Daher wurde das Bohrsche Atommodell zugunsten des quantenmechanischen Orbitalmodells des Atoms verworfen.\n\nDas Orbitalmodell ist bis heute Grundlage und Ausgangspunkt genauer quantenmechanischer Berechnungen fast aller Eigenschaften der Atome. Das gilt insbesondere für ihre Fähigkeit, sich mit anderen Atomen zu einzelnen Molekülen oder zu ausgedehnten Festkörpern zu verbinden. Bei Atomen mit mehreren Elektronen muss dafür außer dem Pauli-Prinzip auch die elektrostatische Wechselwirkung jedes Elektrons mit allen anderen berücksichtigt werden. Diese hängt u.&nbsp;a. von der Form der besetzten Orbitale ab. Andererseits wirkt sich umgekehrt die Wechselwirkung auf die Form und Energie der Orbitale aus. Es ergibt sich das Problem, die Orbitale in selbstkonsistenter Weise so zu bestimmen, dass sich ein stabiles System ergibt. Die Hartree-Fock-Methode geht von Orbitalen einer bestimmten Form aus und variiert diese systematisch, bis die Rechnung eine minimale Gesamtenergie ergibt. Wenn man die Orbitale nach der Dichtefunktionaltheorie bestimmen will, geht man von einer ortsabhängigen Gesamtdichte der Elektronen aus und bildet daraus eine Schrödingergleichung zur Bestimmung der Orbitale der einzelnen Elektronen. Hier wird die anfänglich angenommene Gesamtdichte variiert, bis sie mit der Gesamtdichte, die aus den besetzten Orbitalen zu berechnen ist, gut übereinstimmt.\n\nDas Orbitalmodell bei einem Atom mit mehr als einem Elektron ist physikalisch als eine Näherung zu bezeichnen, nämlich als eine Ein-Teilchen-Näherung. Sie besteht darin, dass jedem einzelnen Elektron ein bestimmtes Orbital zugeschrieben wird. Ein so gebildeter Zustand gehört zu der einfachsten Art von Mehrteilchenzuständen und wird hier als Konfiguration des Atoms bezeichnet. Genauere Modelle berücksichtigen, dass nach den Regeln der Quantenmechanik die Hülle auch in einem Zustand sein kann, der durch Superposition verschiedener Konfigurationen entsteht, wo also mit verschiedenen Wahrscheinlichkeitsamplituden gleichzeitig verschiedene Elektronenkonfigurationen vorliegen (eine sogenannte Konfigurationsmischung). Hiermit werden die genauesten Berechnungen von Energieniveaus und Wechselwirkungen der Atome möglich. Wegen des dazu nötigen mathematischen Aufwands werden jedoch, wo es möglich ist, auch weiterhin einfachere Atommodelle genutzt. Zu nennen ist hier das Thomas-Fermi-Modell, in dem die Elektronenhülle pauschal wie ein im Potentialtopf gebundenes ideales Elektronengas, das Fermigas, behandelt wird, dessen Dichte wiederum die Form des Potentialtopfs bestimmt.\n\n\nErklärung grundlegender Atomeigenschaften.\nDie Elektronen der Atomhülle sind aufgrund ihrer negativen Ladung durch elektrostatische Anziehung an den positiven Atomkern gebunden. Anschaulich bilden sie eine Elektronenwolke ohne scharfen Rand. Ein neutrales Atom enthält genauso viele Elektronen in der Hülle wie Protonen im Kern. Die Hülle hat einen etwa zehn- bis hunderttausend Mal größeren Durchmesser als der Kern, trägt jedoch weniger als 0,06&nbsp;Prozent zur Atommasse bei. Sie ist für energiereiche freie Teilchen (z.&nbsp;B. Photonen der Röntgenstrahlung oder Elektronen und Alphateilchen der ionisierenden Strahlung) mit Energien ab einigen hundert Elektronenvolt (eV) sehr durchlässig. Daher wird das Atom zuweilen als „weitgehend leer“ beschrieben.\n\nFür geladene Teilchen geringer Energie im Bereich bis zu einigen zehn&nbsp;eV ist die Hülle aber praktisch undurchdringlich. In diesem Bereich liegen auch die kinetische Energie und die Bindungsenergie der Elektronen im äußeren Teil der Hülle. Daher erfahren zwei Atome immer eine starke Abstoßungskraft, wenn sie sich so weit annähern, dass sich ihre Hüllen merklich überschneiden würden. Der Bereich der kinetischen Energien ganzer Atome und Moleküle, wie sie unter normalen Bedingungen auf der Erde vorkommen, liegt noch deutlich darunter. Z.&nbsp;B. beträgt die thermische Energie formula_1 (formula_2 Boltzmannkonstante, formula_3 absolute Temperatur), die für die Größenordnung dieses Energiebereichs typisch ist, bei Raumtemperatur nur ungefähr 0,025&nbsp;eV. Unter diesen Bedingungen ist die Atomhülle daher erstens stabil, weil ihr keine Elektronen entrissen werden, und zweitens undurchdringlich, weil sie sich nicht merklich mit den Hüllen anderer Atome überschneidet. Damit wird das Atom zum universellen Baustein der alltäglichen makroskopischen Materie. Seine, wenn auch nicht ganz scharf definierte, Größe verdankt es der gegenseitigen Undurchdringlichkeit der Hüllen.\n\nWenn sich die Hüllen zweier Atome aber nur geringfügig mit ihren äußeren Randbereichen überschneiden, kann zwischen ihnen eine anziehende Kraft entstehen. Sie ist die Ursache für die Entstehung von stabilen Molekülen, also den kleinsten Teilchen einer chemischen Verbindung. Bedingung ist, dass insgesamt ein Gewinn an Bindungsenergie damit einhergeht, dass ein oder zwei Elektronen von einer Hülle ganz oder mit gewisser Wahrscheinlichkeit zu der anderen Hülle überwechseln oder an beiden Hüllen beteiligt sind. Das ist nur bei genau passendem Aufbau beider Hüllen gegeben. Daher treten chemische Bindungen nur bei entsprechend geeigneten Kombinationen von Atomen auf.\n\nBei größeren Abständen, etwa bei einigen Atomdurchmessern, ziehen sich hingegen Atome aller Arten gegenseitig schwach an, unabhängig von der Möglichkeit, eine chemische Bindung einzugehen. Diese Van-der-Waals-Kräfte bewirken, dass jedes Gas bei genügend niedriger Temperatur zu einer Flüssigkeit oder einem Feststoff kondensiert. Sie sind also für den Wechsel der Aggregatzustände verantwortlich und wirken zwischen den neutralen Atomen bzw. Molekülen, sind aber auch elektrischen Ursprungs. Sie werden dadurch erklärt, dass sich zwei Atome durch leichte räumliche Verschiebung ihrer Elektronenwolken gegenseitig elektrische Dipolmomente induzieren, die einander elektrostatisch anziehen.\n\n\nWeitere Entdeckungen.\nDer Chemiker Otto Hahn, ein Schüler Rutherfords, versuchte im Jahr 1938, durch Einfang von Neutronen an Urankernen Atome mit größerer Masse (Transurane) herzustellen, wie das bei leichteren Elementen seit Jahren gelungen war. Fritz Straßmann wies jedoch überraschenderweise nach, dass dabei das viel leichtere Barium entstanden war. Die Physiker Lise Meitner und Otto Frisch konnten den Vorgang als Kernspaltung identifizieren, indem sie mittels einer Ionisationskammer mehrere radioaktive Spaltprodukte nachwiesen.\n\nAb den 1950er Jahren konnten Atome durch die Entwicklung verbesserter Teilchenbeschleuniger und Teilchendetektoren beim Beschuss mit Teilchen sehr hoher Energie untersucht werden. Ende der 1960er Jahre zeigte sich in der „tiefinelastischen Streuung“ von Elektronen an Atomkernen, dass auch Neutronen und Protonen keine unteilbaren Einheiten sind, sondern aus Quarks zusammengesetzt sind.\n\n1951 entwickelte Erwin Müller das Feldionenmikroskop und konnte damit von einer Nadelspitze erstmals ein Abbild erzeugen, das auf direkte Weise so stark vergrößert war, dass einzelne Atome darin sichtbar wurden (wenn auch nur als verschwommene Flecken). 1953 entwickelte Wolfgang Paul die magnetische Ionenfalle (Paulfalle), in der einzelne Ionen gespeichert und mit immer höherer Genauigkeit untersucht werden konnten.\n\n1985 entwickelte eine Arbeitsgruppe um Steven Chu die Laserkühlung, ein Verfahren, die Temperatur einer Ansammlung von Atomen mittels Laser&shy;strahlung stark zu verringern. Im selben Jahr gelang es einer Gruppe um William D. Phillips, neutrale Natriumatome in einer magneto-optischen Falle einzuschließen. Durch Kombination dieser Verfahren mit einer Methode, die den Dopplereffekt nutzt, gelang es einer Arbeitsgruppe um Claude Cohen-Tannoudji, geringe Mengen von Atomen auf Temperaturen von einigen Mikrokelvin zu kühlen. Mit diesem Verfahren können Atome mit höchster Genauigkeit untersucht werden; außerdem ermöglichte es auch die experimentelle Realisierung der Bose-Einstein-Kondensation.\n\nAnfang der 1980er Jahre wurde von Gerd Binnig und Heinrich Rohrer das Rastertunnelmikroskop entwickelt, in dem eine Nadelspitze eine Oberfläche mittels des Tunneleffekts so fein abtastet, dass einzelne Atome sichtbar werden. Damit wurde es auch möglich, Atome einzeln an bestimmte Plätze zu setzen. In den 1990er Jahren konnten Serge Haroche und David Wineland in Experimenten die Wechselwirkung eines einzelnen Atoms mit einem einzelnen Photon erfolgreich untersuchen. In den 2000er Jahren wurde die Handhabbarkeit einzelner Atome unter anderem genutzt, um einen Transistor aus nur einem Metallatom mit organischen Liganden herzustellen.\n\nViele dieser Entdeckungen wurden mit dem Nobelpreis (Physik oder Chemie) ausgezeichnet.\n\n\nKlassifizierung.\n\nElemente, Isotope, Nuklide.\nDie Unterscheidung und Bezeichnung verschiedener Atomsorten geht zunächst vom Aufbau des Atomkerns aus, während der Zustand der Hülle gegebenenfalls durch zusätzliche Symbole angegeben wird. Kennzahlen sind die Protonenzahl (Ordnungszahl, Kernladungszahl) \"Z\", die Neutronenzahl \"N\" des Kerns, und die daraus gebildete Massenzahl \"A=Z+N\". Je nach ihrer Protonenzahl gehören die Atome zu einem der 118 bekannten chemischen Elemente, von Wasserstoff mit \"Z\"=1 bis Oganesson mit \"Z\"=118. Davon sind 91 in natürlichen Vorkommen entdeckt worden, 27 nur nach künstlicher Herstellung durch Kernreaktionen. Die Ordnung der Elemente wird im Periodensystem – wichtig für die Chemie – graphisch veranschaulicht. Darin werden die Elemente mit aufsteigender Ordnungszahl in Form einer Tabelle angeordnet. Jede Zeile wird als Periode des Periodensystems bezeichnet und endet, wenn das jeweilige Orbital mit Elektronen voll besetzt ist (Edelgas). In den nächsten Zeilen wiederholt sich aufgrund der schrittweisen Elektronenbesetzung der nächsten Orbitale der chemische Charakter der Elemente. So stehen Elemente mit ähnlichen chemischen Eigenschaften in einer Spalte untereinander; sie bilden eine Gruppe des Periodensystems.\n\nAtome eines Elements, die sich in der Neutronenzahl unterscheiden, gehören zu verschiedenen Isotopen des Elements. Insgesamt bestehen die 118 Elemente aus etwa 2800 Isotopen, wovon 2500 künstlich erzeugt wurden. Isotope werden – bis auf die Ausnahmen der Wasserstoffisotope Deuterium und Tritium – nach dem chemischen Element und der Massenzahl bezeichnet. Das Symbol für ein bestimmtes Isotop des Elements X hat die Form formula_4, formula_5 oder X-\"A\" (Beispiele: formula_6, formula_7, Pb-208). Die Angabe der Protonenzahl \"Z\" ist redundant, da sich \"Z\" schon aus der Ordnungszahl des Elements ergibt.\n\nNuklid ist die ganz allgemeine Bezeichnung für Atomarten, unabhängig davon, ob sie zum gleichen Element gehören oder nicht. Die Nuklidkarte oder Isotopenkarte – wichtig für die Kernphysik und ihre Anwendungen – ist eine Tabelle, in der jede Atomart einen eigenen Platz erhält. Dazu wird auf einer Achse die Anzahl der Protonen, auf der anderen die der Neutronen aufgetragen. Häufig wird die Stabilität und bei instabilen Nukliden auch die Art der Umwandlung oder die Größenordnung der Halbwertszeit durch bestimmte Farben dargestellt.\n\n\nStabile und instabile (radioaktive) Atome.\nDer Atomkern eines Nuklids formula_8 kann im energetischen Grundzustand und in verschiedenen Anregungszuständen vorliegen. Wenn darunter relativ langlebige, sogenannte metastabile Zustände sind, werden diese als Isomere bezeichnet und als eigene Nuklide gezählt (Symbol formula_9, formula_10 o.&nbsp;ä.). Nach dieser Definition sind mit dem Stand von 2003 insgesamt etwa 3200 Nuklide bekannt.\n\nIn der Kernphysik werden Nuklide mit unterschiedlichen Protonenzahlen, aber gleicher Massenzahl formula_11 als Isobare bezeichnet. Seltener werden unter dem Namen Isotone Nuklide mit verschiedenen Protonenzahlen, aber gleicher Neutronenzahl zusammengefasst.\n\nNur etwa 250 Isotope von 80 Elementen haben einen stabilen Kern. Alle anderen Atome sind instabil und wandeln sich über kurz oder lang in Atome eines stabilen Isotops um. Da sie dabei im Allgemeinen ionisierende Strahlung aussenden, heißen sie auch Radioisotope oder Radionuklide. Auf der Erde wurden in den natürlichen Vorkommen neben allen 250 stabilen Isotopen 30 Radioisotope gefunden, die sich auf 10 radioaktive Elemente verteilen und die natürliche Radioaktivität verursachen. Viele weitere kurzlebige Isotope existieren im Inneren von Sternen, insbesondere während der Supernova-Phase.\n\n\nSeltene und theoretische Formen.\nAls Rydberg-Atom wird ein Atom bezeichnet, in dem ein Elektron in einem so hohen Energiezustand angeregt ist, dass es den Atomkern, teilweise auch den gesamten Atomrumpf, bestehend aus dem Atomkern und den restlichen Elektronen, in weitem Abstand umkreist und sein Verhalten damit dem eines klassischen Teilchens ähnelt. Rydberg-Atome können über 100.000-mal größer sein als nicht angeregte Atome. Da sie extrem empfindlich auf äußere Felder reagieren, kann man mit ihnen z.&nbsp;B. die Wechselwirkung mit einem einzelnen Photon im Detail untersuchen. Sind zwei oder mehr Elektronen in solchen Zuständen angeregt, spricht man von planetarischen Atomen.\n\nIm teils übertragenen Sinn werden als exotische Atome auch solche Systeme bezeichnet, die in physikalischer Hinsicht gewisse Ähnlichkeiten zu den gewöhnlichen Atomen aufweisen. In ihnen kann z.&nbsp;B. eines der Protonen, Neutronen oder Elektronen durch ein anderes Teilchen derselben Ladung ersetzt worden sein. Wird etwa ein Elektron durch ein schwereres Myon ersetzt, bildet sich ein myonisches Atom. Als Positronium wird ein exotisches Atom bezeichnet, in dem ein Elektron statt an ein Proton an ein Positron, das ist das positiv geladene Antiteilchen des Elektrons, gebunden ist. Auch Atome, die gänzlich aus Antiteilchen zur normalen Materie aufgebaut sind, sind möglich. So wurden erstmals 1995 am Genfer CERN Antiwasserstoffatome künstlich hergestellt und nachgewiesen. An solchen exotischen Atomen lassen sich unter anderem fundamentale physikalische Theorien überprüfen.\n\nDes Weiteren wird der Name Atom manchmal auch für Zwei-Teilchen-Systeme verwendet, die nicht durch elektromagnetische Wechselwirkung zusammengehalten werden, sondern durch die starke Wechselwirkung. Bei einem solchen Quarkonium handelt es sich um ein kurzlebiges Elementarteilchen vom Typ Meson, das aus einem Quark und seinem Antiteilchen aufgebaut ist. Ein Quarkonium-Atom lässt sich in seinen verschiedenen metastabilen Zuständen so durch Quantenzahlen klassifizieren wie das Wasserstoffatom.\n\n\nEntstehung.\nEtwa eine Sekunde nach dem Urknall kamen die ständigen Umwandlungen zwischen den Elementarteilchen zur Ruhe, übrig blieben Elektronen, Protonen und Neutronen. In den darauf folgenden drei Minuten verbanden sich in der primordialen Nukleosynthese die vorhandenen Neutronen mit Protonen zu den einfachsten Kernen: Deuterium, Helium, in geringerem Umfang auch Lithium und möglicherweise in noch kleineren Mengen Beryllium und Bor. Die übrigen Protonen (86 Prozent) blieben erhalten. Die ersten neutralen Atome mit dauerhaft gebundenen Elektronen wurden erst 380.000&nbsp;Jahre nach dem Urknall in der Rekombinationsphase gebildet, als das Universum durch Expansion so weit abgekühlt war, dass die Atome nicht sogleich wieder ionisiert wurden.\n\nDie Kerne aller schwereren Atome wurden und werden durch verschiedene Prozesse der Kernfusion erzeugt. Am wichtigsten ist die stellare Nukleosynthese, durch die in Sternen zunächst Helium, anschließend auch die schwereren Elemente bis zum Eisen gebildet werden. Elemente mit höheren Kernladungszahlen als Eisen entstehen in explosionsartigen Vorgängen wie im r-Prozess in Supernovae und im s-Prozess in AGB-Sternen, die kurz vor dem Ende ihrer Lebensdauer sind.\n\nKleine Mengen verschiedener Elemente und Isotope werden auch dadurch gebildet, dass schwere Kerne wieder geteilt werden. Das geschieht durch radioaktive Zerfälle (siehe Zerfallsreihe), die u.&nbsp;a. für einen Teil des Vorkommens von Helium und Blei verantwortlich sind, und Spallationen, die für die Entstehung von Lithium, Beryllium und Bor wichtig sind.\n\n\nVorkommen und Verteilung.\nIm beobachtbaren Universum liegen die Atome mit einer mittleren Dichte von 0,25&nbsp;Atome/m³ vor. Nach dem Urknallmodell (Lambda-CDM-Modell) bilden sie etwa 4,9&nbsp;Prozent der gesamten Energiedichte. Der Rest, dessen Natur noch weitgehend unklar ist, setzt sich aus etwa 27&nbsp;Prozent dunkler Materie und 68&nbsp;Prozent dunkler Energie zusammen, sowie kleinen Beiträgen von Neutrinos und elektromagnetischer Strahlung. Im Inneren einer Galaxie wie etwa der Milchstraße ist im interstellaren Medium (ISM) die Dichte der Atome wesentlich höher und liegt zwischen 10 und 10 Atome/m. Die Sonne befindet sich in der weitgehend staubfreien lokalen Blase, daher ist die Dichte in der Umgebung des Sonnensystems nur etwa 10 Atome/m. In festen Himmelskörpern wie der Erde beträgt die Atomdichte etwa 10 Atome/m.\n\nIn der Verteilung der Elemente dominiert im Universum Wasserstoff mit rund drei Viertel der Masse, danach folgt Helium mit etwa einem Viertel. Alle schwereren Elemente sind viel seltener und machen nur einen kleinen Teil der im Universum vorhandenen Atome aus. Ihre Häufigkeiten werden von den verschiedenen Mechanismen der Nukleosynthese bestimmt.\n\nIm Sonnensystem sind Wasserstoff und Helium vorwiegend in der Sonne und den Gasplaneten enthalten. Dagegen überwiegen auf der Erde die schweren Elemente. Die häufigsten Elemente sind hier Sauerstoff, Eisen, Silicium und Magnesium. Der Erdkern besteht vorwiegend aus Eisen, während in der Erdkruste Sauerstoff und Silicium vorherrschen.\n\n\nBestandteile des Atoms.\nDie beiden Hauptbestandteile eines Atoms sind der Atomkern und die Atomhülle. Die Hülle besteht aus Elektronen. Sie trägt mit weniger als 0,06&nbsp;Prozent zur Masse des Atoms bei, bestimmt aber dessen Größe und dessen Verhalten gegenüber anderen Atomen, wenn sie einander nahe kommen. Der Kern besteht aus Protonen und Neutronen, ist im Durchmesser zehn- bis hunderttausendmal kleiner als die Hülle, enthält aber mehr als 99,9&nbsp;Prozent der Masse des Atoms.\n\n\nAtomkern.\n\nAufbau.\nDie in einem Atom vorhandenen Protonen und Neutronen, zusammen auch als Nukleonen bezeichnet, sind aneinander gebundenen und bilden den Atomkern. Die Nukleonen zählen zu den Hadronen. Das Proton ist positiv geladen, das Neutron ist elektrisch neutral. Proton und Neutron haben einen Durchmesser von etwa 1,6&nbsp;fm (Femtometer) und sind selber keine Elementarteilchen, sondern nach dem Standardmodell der Elementarteilchenphysik aus den punktförmigen Quarks aufgebaut. Jeweils drei Quarks binden sich durch die starke Wechselwirkung, die durch Gluonen vermittelt wird, zu einem Nukleon. Die starke Wechselwirkung ist darüber hinaus für den Zusammenhalt der Nukleonen im Atomkern verantwortlich, insbesondere ist die Anziehung bis zu etwa 2,5&nbsp;fm Abstand deutlich stärker als die gegenseitige elektrische Abstoßung der Protonen. Unterhalb von etwa 1,6&nbsp;fm wird die starke Wechselwirkung der Hadronen jedoch stark abstoßend. Anschaulich gesprochen verhalten sich die Nukleonen im Kern also etwa wie harte Kugeln, die aneinander haften. Daher steigt das Volumen des Kerns proportional zur Nukleonenzahl (Massenzahl) formula_11. Sein Radius beträgt etwa formula_13&nbsp;fm.\n\nDer leichteste Atomkern besteht aus nur einem Proton. Mehrere Protonen stoßen sich zwar gemäß der Elektrostatik ab, können zusammen mit einer geeigneten Anzahl von Neutronen aber ein stabiles System bilden. Doch schon bei kleinen Abweichungen von dem energetisch günstigsten Zahlenverhältnis ist der Kern instabil und wandelt sich spontan um, indem aus einem Neutron ein Proton wird oder umgekehrt und die frei werdende Energie und Ladung als Betastrahlung abgegeben wird. Kerne mit bis zu etwa 20 Protonen sind nur bei annähernd gleich großer Neutronenzahl stabil. Darüber steigt in den stabilen Atomkernen das Verhältnis von Neutronen zu Protonen von 1:1 bis auf etwa 1,5:1, weil bei größeren Protonenzahlen wegen ihrer elektrostatischen Abstoßung die Anzahl der Neutronen schneller anwachsen muss als die der Protonen (Details siehe Tröpfchenmodell). Die Bindungsenergie liegt in stabilen Kernen (abgesehen von den leichtesten) oberhalb von 7&nbsp;MeV pro Nukleon (siehe Abbildung) und übertrifft damit die Bindungsenergie der äußeren Elektronen der Atomhülle oder die chemische Bindungsenergie in stabilen Molekülen um das ca. 10-fache. Kerne mit bestimmten Nukleonenzahlen, die als Magische Zahl bezeichnet werden, beispielsweise Helium-4, Sauerstoff-16 oder Blei-208, sind besonders stabil, was mit dem Schalenmodell des Atomkerns erklärt werden kann.\n\nOberhalb einer Zahl von 82&nbsp;Protonen (also jenseits von Blei) sind alle Kerne instabil. Sie wandeln sich durch Ausstoßen eines Kerns He-4 in leichtere Kerne um (Alphastrahlung). Dies wiederholt sich, zusammen mit Betastrahlung, so lange, bis ein stabiler Kern erreicht ist; mehrere Zerfallsstufen bilden eine Zerfallsreihe. Auch zu den Protonenzahlen 43 (Technetium) und 61 (Promethium) existiert kein stabiler Kern. Daher kann es insgesamt nur 80 verschiedene stabile chemische Elemente geben, alle weiteren sind radioaktiv. Sie kommen auf der Erde nur dann natürlich vor, wenn sie selber oder eine ihrer Muttersubstanzen eine genügend lange Halbwertzeit haben.\n\n\nMasse.\nDa der Großteil der Atommasse von den Neutronen und Protonen stammt und diese etwa gleich schwer sind, wird die Gesamtzahl dieser Teilchen in einem Atom als Massenzahl bezeichnet. Die genaue Masse eines Atoms wird oft in der atomaren Masseneinheit u angegeben; ihr Zahlenwert ist dann etwa gleich der Massenzahl. Kleinere Abweichungen entstehen durch den Massendefekt der Atomkerne. Die atomare Masseneinheit ergibt sich aus der Definition der SI-Einheit des Mols in der Art und Weise, dass ein Atom des Kohlenstoffisotops C (im Grundzustand inklusive seiner Hüllenelektronen) eine Masse von exakt 12&nbsp;u besitzt. Damit beträgt 1&nbsp;u gleich 1,66053904&nbsp;·&nbsp;10&nbsp;kg. Ein Atom des leichtesten Wasserstoffisotops hat eine Masse von 1,007825&nbsp;u. Das schwerste stabile Nuklid ist das Bleiisotop Pb mit einer Masse von 207,9766521&nbsp;u.\n\nDa makroskopische Stoffmengen so viele Atome enthalten, dass die Angabe ihrer Anzahl als natürliche Zahl unhandlich wäre, erhielt die Stoffmenge eine eigene Einheit, das Mol. Ein Mol sind etwa 6,022&nbsp;·&nbsp;10 Atome (oder auch Moleküle oder andere Teilchen; die betrachtete Teilchenart muss immer mitgenannt werden). Die Masse von 1&nbsp;Mol Atomen der Atommasse \"X\"&nbsp;u ist daher exakt \"X\"&nbsp;g. Daher ist es in der Chemie üblich, Atommassen statt in u auch indirekt in g/mol anzugeben.\n\n\nBildung und Zerfall.\nIn welcher Art ein instabiler Atomkern zerfällt, ist für das jeweilige Radionuklid typisch. Bei manchen Nukliden können die (untereinander völlig gleichen) Kerne auch auf verschiedene Arten zerfallen, so dass mehrere Zerfallskanäle mit bestimmten Anteilen beteiligt sind. Die wichtigsten radioaktiven Zerfälle sind\n\nDie Energien der Strahlungen sind für das jeweilige Nuklid charakteristisch, ebenso wie die Halbwertszeit, die angibt, wie lange es dauert, bis die Hälfte einer Probe des Nuklids zerfallen ist.\n\nDurch Anlagerung eines Neutrons kann sich ein Kern in das nächstschwerere Isotop desselben Elements verwandeln. Durch den Beschuss mit Neutronen oder anderen Atomkernen kann ein großer Atomkern in mehrere kleinere Kerne gespalten werden. Einige schwere Nuklide können sich auch ohne äußere Einwirkung spontan spalten.\n\nGrößere Atomkerne können aus kleineren Kernen gebildet werden. Dieser Vorgang wird Kernfusion genannt. Für eine Fusion müssen sich Atomkerne sehr nahe kommen. Diesem Annähern steht die elektrostatische Abstoßung beider Kerne, der sogenannte Coulombwall, entgegen. Aus diesem Grund ist eine Kernfusion (außer in bestimmten Experimenten) nur unter sehr hohen Temperaturen von mehreren Millionen Grad und hohen Drücken, wie sie im Inneren von Sternen herrschen, möglich. Die Kernfusion ist bei Nukliden bis zum Nickel-62 eine exotherme Reaktion, so dass sie im Großen selbsterhaltend ablaufen kann. Sie ist die Energiequelle der Sterne. Bei Atomkernen jenseits des Nickels nimmt die Bindungsenergie pro Nukleon ab; die Fusion schwererer Atomkerne ist daher endotherm und damit kein selbsterhaltender Prozess. Die Kernfusion in Sternen kommt daher zum Erliegen, wenn die leichten Atomkerne aufgebraucht sind.\n\n\nAtomhülle.\n\nAufbau und Bindungsenergie.\nDie Atomhülle besteht aus Elektronen, die aufgrund ihrer negativen Ladung an den positiven Atomkern gebunden sind. Sie wird oft auch als Elektronenhülle bezeichnet. Bei einem neutralen Atom beträgt die durchschnittliche Bindungsenergie der formula_14 Elektronen der Hülle etwa formula_15. Sie nimmt daher mit steigender Teilchenzahl erheblich zu, im Gegensatz zur durchschnittlichen Bindungsenergie pro Nukleon im Kern. Zur Erklärung wird angeführt, dass zwischen Nukleonen nur Bindungskräfte kurzer Reichweite wirken, die kaum über die benachbarten Teilchen hinausreichen, während die Hülle durch die elektrostatische Anziehungskraft gebunden ist, die als langreichweitige Wechselwirkung mit größerem Abstand vom Kern vergleichsweise schwach abnimmt.\n\nAbgesehen von der Masse, die zu über 99,9&nbsp;Prozent im Atomkern konzentriert ist, ist die Atomhülle für praktisch alle äußeren Eigenschaften des Atoms verantwortlich. Der Begriff Atommodell bezieht sich daher im engeren Sinn meist nur auf die Hülle (siehe Liste der Atommodelle). Ein einfaches Atommodell ist das Schalenmodell, nach dem die Elektronen sich in bestimmten Schalen um den Kern anordnen, in denen jeweils für eine bestimmte Anzahl Elektronen Platz ist. Allerdings haben diese Schalen weder einen bestimmten Radius noch eine bestimmte Dicke, sondern überlappen und durchdringen einander teilweise.\n\nWesentliche Eigenschaften der Hülle sind oben unter Quantenmechanische Atommodelle und Erklärung grundlegender Atomeigenschaften dargestellt. In den nachfolgenden Abschnitten folgen weitere Details.\n\n\nInterpretation grundlegender Atomeigenschaften im Rahmen des Schalenmodells.\nDie Atomhülle bestimmt die Stärke und Abstandsabhängigkeit der Kräfte zwischen zwei Atomen. Im Abstandsbereich mehrerer Atomdurchmesser polarisieren sich die gesamten Atomhüllen wechselseitig, sodass durch elektrostatische Anziehung anziehende Kräfte, die Van-der-Waals-Kräfte, entstehen. Sie bewirken vor allem die Kondensation der Gase zu Flüssigkeiten, also einen Wechsel der Aggregatzustände.\n\nDie (näherungsweise) Inkompressibilität der Flüssigkeiten und Festkörper hingegen beruht darauf, dass alle Atome bei starker Annäherung einander stark abstoßen, sobald sich ihre Hüllen im Raum merklich überschneiden und daher verformen müssen. Außer im Fall zweier Wasserstoff&shy;atome, die jeweils nur ein Elektron in der Hülle haben, spielt die elektrostatische Abstoßung der beiden Atomkerne dabei nur eine geringe Rolle.\n\nIn einem mittleren Abstandsbereich zwischen dem Vorherrschen der schwach anziehenden Van-der-Waals-Kräfte und der starken Abstoßung kommt es zwischen zwei oder mehr zueinander passenden Atomhüllen zu einer besonders starken Anziehung, der chemischen Bindung. Bei Atomen bestimmter Elemente kann diese Anziehung zu einem stabilen Molekül führen, das aus Atomen in zahlenmäßig genau festgelegter Beteiligung und räumlicher Anordnung aufgebaut ist. Die Moleküle sind die kleinsten Stoffeinheiten der chemischen Verbindungen, also der homogenen Materialien in all ihrer Vielfalt. Vermittelt über die Hüllen ihrer Atome ziehen auch Moleküle einander an. Ein fester Körper entsteht, wenn viele Moleküle sich aneinander binden und dabei, weil es energetisch günstig ist, eine feste Anordnung einhalten. Ist diese Anordnung regelmäßig, bildet sich ein Kristallgitter. Infolge dieser Bindung ist der feste Körper nicht nur weitgehend inkompressibel wie eine Flüssigkeit, sondern im Unterschied zu dieser auch auf Zug belastbar und deutlich weniger leicht verformbar. Verbinden sich Atome metallischer Elemente miteinander, ist ihre Anzahl nicht festgelegt und es können sich nach Größe und Gestalt beliebige Körper bilden. Vor allem chemisch reine Metalle zeigen dann meist auch eine große Verformbarkeit. Verbindungen verschiedener Metalle werden Legierung genannt. Die Art der Bindung von Metallatomen erklärt, warum Elektronen sich fast frei durch das Kristallgitter bewegen können, was die große elektrische Leitfähigkeit und Wärmeleitfähigkeit der Metalle verursacht. Zusammengefasst ergeben sich aus der Wechselwirkung der Atomhüllen miteinander die mechanische Stabilität und viele weitere Eigenschaften der makroskopischen Materialien.\n\nAufgrund des unscharfen Randes der Atomhülle liegt die Größe der Atome nicht eindeutig fest. Die als Atomradien tabellierten Werte sind aus der Bindungslänge gewonnen, das ist der energetisch günstigste Abstand zwischen den Atomkernen in einer chemischen Bindung. Insgesamt zeigt sich mit steigender Ordnungszahl eine in etwa periodische Variation der Atomgröße, die mit der periodischen Variation des chemischen Verhaltens gut übereinstimmt. Im Periodensystem der Elemente gilt allgemein, dass innerhalb einer Periode, also einer Zeile des Systems, eine bestimmte Schale aufgefüllt wird. Von links nach rechts nimmt die Größe der Atome dabei ab, weil die Kernladung anwächst und daher alle Schalen stärker angezogen werden. Wenn eine bestimmte Schale mit den stark gebundenen Elektronen gefüllt ist, gehört das Atom zu den Edelgasen. Mit dem nächsten Elektron beginnt die Besetzung der Schale mit nächstgrößerer Energie, was mit einem größeren Radius verbunden ist. Innerhalb einer Gruppe, also einer Spalte des Periodensystems, nimmt die Größe daher von oben nach unten zu. Dementsprechend ist das kleinste Atom das Heliumatom am Ende der ersten Periode mit einem Radius von 32&nbsp;pm, während eines der größten Atome das Caesium&shy;atom ist, das erste Atom der 5.&nbsp;Periode. Es hat einen Radius von 225&nbsp;pm.\n\n\nErklärung der Atomeigenschaften im Rahmen des Orbitalmodells.\nDie dem Schalenmodell zugrundeliegenden Elektronenschalen ergeben sich durch die Quantisierung der Elektronenenergien im Kraftfeld des Atomkerns nach den Regeln der Quantenmechanik. Um den Kern herum bilden sich verschiedene Atomorbitale, das sind unscharf begrenzte Wahrscheinlichkeitsverteilungen für \"mögliche\" räumliche Zustände der Elektronen. Jedes Orbital kann aufgrund des Pauli-Prinzips mit maximal zwei Elektronen besetzt werden, dem Elektronenpaar. Die Orbitale, die unter Vernachlässigung der gegenseitigen Abstoßung der Elektronen und der Feinstruktur theoretisch die gleiche Energie hätten, bilden eine Schale. Die Schalen werden mit der Hauptquantenzahl durchnummeriert oder fortlaufend mit den Buchstaben \"K, L, M,\"… bezeichnet. Genauere Messungen zeigen, dass ab der zweiten Schale nicht alle Elektronen einer Schale die gleiche Energie besitzen. Falls erforderlich, wird durch die Nebenquantenzahl oder Drehimpulsquantenzahl eine bestimmte Unterschale identifiziert.\n\nSind die Orbitale, angefangen vom energetisch niedrigsten, so weit mit Elektronen besetzt, dass die gesamte Elektronenzahl gleich der Protonenzahl des Kerns ist, ist das Atom neutral und befindet sich im Grundzustand. Werden in einem Atom ein oder mehrere Elektronen in energetisch höherliegende Orbitale versetzt, ist das Atom in einem angeregten Zustand. Die Energien der angeregten Zustände haben für jedes Atom wohlbestimmte Werte, die sein Termschema bilden. Ein angeregtes Atom kann seine Überschussenergie abgeben durch Stöße mit anderen Atomen, durch Emission eines der Elektronen (Auger-Effekt) oder durch Emission eines Photons, also durch Erzeugung von Licht oder Röntgenstrahlung. Bei sehr hoher Temperatur oder in Gasentladungen können die Atome durch Stöße Elektronen verlieren (siehe Ionisationsenergie), es entsteht ein Plasma, so z.&nbsp;B. in einer heißen Flamme oder in einem Stern.\nDa die Energien der Quanten der emittierten Strahlung je nach Atom bzw. Molekül und den beteiligten Zuständen verschieden sind, lässt sich durch Spektroskopie dieser Strahlung die Quelle im Allgemeinen eindeutig identifizieren. Beispielsweise zeigen die einzelnen Atome ihr elementspezifisches optisches Linienspektrum. Bekannt ist etwa die Natrium-D-Linie, eine Doppellinie im gelben Spektralbereich bei 588,99&nbsp;nm und 589,59&nbsp;nm, die auch in nebenstehender Abbildung mit D-1 bezeichnet wird. Ihr Aufleuchten zeigt die Anwesenheit von angeregten Natrium-Atomen an, sei es auf der Sonne oder über der Herdflamme bei Anwesenheit von Natrium oder seinen Salzen. Da diese Strahlung einem Atom auch durch Absorption dieselbe Energie zuführen kann, lassen sich die Spektrallinien der Elemente sowohl in Absorptions- als auch in Emissionsspektren beobachten. Diese Spektrallinien lassen sich auch verwenden, um Frequenzen sehr präzise zu vermessen, beispielsweise für Atomuhren.\n\nObwohl Elektronen sich untereinander elektrostatisch abstoßen, können zusätzlich bis zu zwei weitere Elektronen gebunden werden, wenn es bei der höchsten vorkommenden Elektronenenergie noch Orbitale mit weiteren freien Plätzen gibt (siehe Elektronenaffinität).\nChemische Reaktionen, d.&nbsp;h. die Verbindung mehrerer Atome zu einem Molekül oder sehr vieler Atome zu einem Festkörper, werden dadurch erklärt, dass ein oder zwei Elektronen aus einem der äußeren Orbitale eines Atoms (Valenzelektronen) unter Energiegewinn auf einen freien Platz in einem Orbital eines benachbarten Atoms ganz hinüberwechseln (Ionenbindung) oder sich mit einer gewissen Wahrscheinlichkeit dort aufhalten (kovalente Bindung durch ein bindendes Elektronenpaar). Dabei bestimmt die Elektronegativität der Elemente, bei welchem Atom sich die Elektronen wahrscheinlicher aufhalten. In der Regel werden chemische Bindungen so gebildet, dass die Atome die Elektronenkonfiguration eines Edelgases erhalten (Edelgasregel). Für das chemische Verhalten des Atoms sind also Form und Besetzung seiner Orbitale entscheidend. Da diese allein von der Protonenzahl bestimmt werden, zeigen alle Atome mit gleicher Protonenzahl, also die Isotope eines Elements, nahezu das gleiche chemische Verhalten.\n\nNähern sich zwei Atome über die chemische Bindung hinaus noch stärker an, müssen die Elektronen eines Atoms wegen des Pauli-Prinzips auf freie, aber energetisch ungünstige Orbitale des anderen Atoms ausweichen, was einen erhöhten Energiebedarf und damit eine abstoßende Kraft nach sich zieht.\n\n\nWechselwirkung zwischen Kern und Hülle.\nMit großer Genauigkeit wird die Wechselwirkung zwischen Kern und Hülle schon durch den einfachen Ansatz beschrieben, in dem der Kern eine punktförmige Quelle eines elektrostatischen Felds nach dem Coulomb-Gesetz darstellt. Alle genannten Atommodelle beruhen hierauf. Aufgrund zusätzlicher Effekte, die in erweiterten Modellen behandelt werden, sind nur extrem kleine Korrekturen nötig, die unter dem Namen Hyperfeinstruktur zusammengefasst werden. Zu berücksichtigen sind hier drei Effekte: erstens die endliche Ausdehnung, die jeder Kern besitzt, zweitens eine magnetische Dipolwechselwirkung, wenn sowohl Kern als auch Hülle eine Drehimpulsquantenzahl von mindestens ½ haben, und drittens eine elektrische Quadrupolwechselwirkung, wenn beide Drehimpulsquantenzahlen mindestens 1 sind.\n\nDie endliche Ausdehnung des Kerns – verglichen mit einer theoretischen Punktladung – bewirkt eine schwächere Anziehung derjenigen Elektronen, deren Aufenthaltswahrscheinlichkeit bis in den Kern hineinreicht. Betroffen sind nur \"s\"-Orbitale (Bahndrehimpuls Null). Bei Atomen mittlerer Ordnungszahl liegt die Korrektur in der Größenordnung von 1&nbsp;Prozent. Die magnetischen Dipol- bzw. elektrischen Quadrupol-Momente von Hülle und Kern bewirken eine Kopplung mit der Folge, dass die Gesamtenergie eines freien Atoms je nach Quantenzahl seines Gesamtdrehimpulses äußerst geringfügig aufgespalten ist. Im H-Atom beträgt die Aufspaltung etwa ein Millionstel der Bindungsenergie des Elektrons (siehe 21-cm-Linie). Anschaulich gesprochen hängt die Energie davon ab, in welchem Winkel die Achsen des magnetischen Dipolmoments bzw. elektrischen Quadrupolmoments von Kern und Hülle zueinander stehen.\n\nAuch bei Atomen in Flüssigkeiten und Festkörpern treten diese Wechselwirkungen in entsprechend modifizierter Form auf. Trotz der Kleinheit der dadurch verursachten Effekte haben sie eine große Rolle in der Atom- und Kernforschung gespielt und sind in besonderen Fällen auch bei modernen Anwendungen wichtig.\n\n\nBeobachtung.\n\nIndirekte Beobachtung.\nIndirekte Möglichkeiten, Atome zu erkennen, beruhen auf der Beobachtung der von ihnen ausgehenden Strahlung. So kann aus Atomspektren beispielsweise die Elementzusammensetzung entfernter Sterne bestimmt werden. Die verschiedenen Elemente lassen sich durch charakteristische Spektrallinien identifizieren, die auf Emission oder Absorption durch Atome des entsprechenden Elements in der Sternatmosphäre zurückgehen. Gasentladungslampen, die dasselbe Element enthalten, zeigen diese Linien als Emissionslinien. Auf diese Weise wurde z.&nbsp;B. 1868 Helium im Spektrum der Sonne nachgewiesen – über 10&nbsp;Jahre bevor es auf der Erde entdeckt wurde.\n\nEin Atom kann ionisiert werden, indem eines seiner Elektronen entfernt wird. Die elektrische Ladung sorgt dafür, dass die Flugbahn eines Ions von einem Magnetfeld abgelenkt wird. Dabei werden leichte Ionen stärker abgelenkt als schwere. Das Massenspektrometer nutzt dieses Prinzip, um das Masse-zu-Ladung-Verhältnis von Ionen und damit die Atommassen zu bestimmen.\n\nDie Elektronenenergieverlustspektroskopie misst den Energieverlust eines Elektronenstrahls bei der Wechselwirkung mit einer Probe in einem Transmissionselektronenmikroskop.\n\n\nBeobachtung einzelner Atome.\nEine direkte Abbildung, die einzelne Atome erkennen lässt, wurde erstmals 1951 mit dem Feldionenmikroskop (oder Feldemissionsmikroskop) erzielt. Auf einem kugelförmigen Bildschirm, in dessen Mittelpunkt sich eine extrem feine Nadelspitze befindet, erscheint ein etwa millionenfach vergrößertes Bild. Darin sind die obersten Atome, die die Spitze bilden, nebeneinander als einzelne Lichtpunkte zu erkennen. Dies kann heute auch im Physikunterricht an der Schule vorgeführt werden. Das Bild entsteht in Echtzeit und erlaubt z.&nbsp;B. die Betrachtung der Wärmebewegung einzelner Fremdatome auf der Spitze.\n\nAuch das Rastertunnelmikroskop ist ein Gerät, das einzelne Atome an der Oberfläche eines Körpers sichtbar macht. Es verwendet den Tunneleffekt, der es Teilchen erlaubt, eine Energiebarriere zu passieren, die sie nach klassischer Physik nicht überwinden könnten. Bei diesem Gerät tunneln Elektronen zwischen einer elektrisch leitenden Spitze und der elektrisch leitenden Probe. Bei Seitwärtsbewegungen zur Abrasterung der Probe wird die Höhe der Spitze so nachgeregelt, dass immer derselbe Strom fließt. Die Bewegung der Spitze bildet die Topographie und Elektronenstruktur der Probenoberfläche ab. Da der Tunnelstrom sehr stark vom Abstand abhängt, ist die laterale Auflösung viel feiner als der Radius der Spitze, manchmal atomar.\n\nEine tomographische Atomsonde erstellt ein dreidimensionales Bild mit einer Auflösung unterhalb eines Nanometers und kann einzelne Atome ihrem chemischen Element zuordnen.\n\nAufbauend auf einer um 2010 entwickelten Atom-Licht-Schnittstelle ist es 2020 gelungen, Fotos einzelner Atome zu machen, die weniger als einen Tausendstel Millimeter über einer lichtleitenden Glasfaser schweben. Dadurch ist es unter Laborbedingungen nun möglich, Effekte wie die Absorption und Aussendung von Licht kontrollierter als bisher zu untersuchen. Dies kann bei der Entwicklung neuartiger optischer Glasfaser-Netzwerke helfen.\n\n\n"}
{"id": "104", "url": "https://de.wikipedia.org/wiki?curid=104", "title": "Arzt", "text": "Arzt\n\nEin Arzt oder eine Ärztin ist eine medizinisch ausgebildete und zur Ausübung der Heilkunde zugelassene Person. Der Arztberuf gilt der Vorbeugung (Prävention), Erkennung (Diagnose), Behandlung (Therapie) und Nachsorge von Krankheiten, Leiden oder gesundheitlichen Beeinträchtigungen und umfasst auch ausbildende Tätigkeiten.\n\nÄrzte stellen sich in den Dienst der Gesundheit und sind bei ihrem Handeln moralischen und ethischen Grundsätzen verpflichtet (siehe etwa die Genfer Deklaration des Weltärztebundes). Die Vielfalt an Krankheiten und Behandlungsmöglichkeiten hat in der Humanmedizin und der Tiermedizin zu einer großen Anzahl von Fachgebieten und weiteren Differenzierungen geführt (siehe die Liste medizinischer Fachgebiete).\n\n\nBezeichnungen.\nDie Bezeichnung \"Arzt\" (, ; verwandt mit „Arznei“) zog während des Mittelalters aus der lateinischen Gelehrtensprache ins Deutsche ein, und zwar über die latinisierte Variante (spätlateinisch auch ) des griechischen , klassische Aussprache [], ‚Oberarzt‘, ‚Leibarzt‘ (seit dem 2. Jahrhundert die Amtsbezeichnung von Leibärzten bei Hofe und von öffentlich bestallten Gemeindeärzten), einer Zusammensetzung aus , ‚Kommando‘ und . In vielen fachsprachlichen Komposita tritt das ursprüngliche griechische Wort bzw. die latinisierte Form als Wortbestandteil auf: \"iatrogen\" „durch ärztliches Handeln verursacht“; \"Psychiater\" „Seelenarzt“; \"Pädiater\" „Kinderarzt“ usw. Über mittelhochdeutsche Vermittlung gelangte das Wort in andere Sprachen, so , .\n\nDie germanische Bezeichnung für den Heilberuf () ist beispielsweise im dänischen , im schwedischen , im englischen (vgl. Bald’s Leechbook), oder im deutschen Familiennamen \"Lachmann\" erhalten und hat sich in andere Sprachen verbreitet, z.&nbsp;B. , . Im polnischen und tschechischen ist die germanische Wurzel mit einem slawischen Suffix (\"-arz\", \"-ař\") verbunden.\n\nDie lateinische Bezeichnung (ursprünglich als allgemeine, vom Ausbildungsstand unabhängige, Berufszeichnung; seit dem 10. Jahrhundert dann vom bzw. , dem Wundarzt, unterschieden), oder eine davon abgeleitete Form findet sich vor allem in den romanischen Sprachen, etwa , , , , , aber unter romanischem Einfluss auch in anderen Sprachen: , . Zur Unterscheidung vom (im 18. Jahrhundert noch nicht „vollpromovierten“) \"chirurgicus\" wurde auch der Begriff \"medicus purus\" („reiner Arzt“) gebraucht (Bestrebungen, die Chirurgie mit der „Medizin“ zu vereinen, setzten etwa in der Mitte des 18. Jahrhunderts ein). Die Bezeichnung meinte meist einen akademisch ausgebildeten Arzt (vgl. englisch \"physician\").\n\nIn vielen Sprachen wird der Arzt umgangssprachlich nach seinem zumeist geführten akademischen Grad \"Doktor\" genannt. Gelegentlich ebenfalls als Arzt wurden vor allem ab dem 13. Jahrhundert volksmedizinisch arbeitende \"Laienärzte\" bezeichnet.\n\n\nZum Arztberuf.\n\nGeschichte.\nDie Funktion des Arztes ist eine der ältesten der Menschheit. Medizingeschichtlich gesehen entstand der Arztberuf (veraltet auch das Arzttum) aus dem Stand der Heilkundigen, die schon unter den Priestern des Altertums zu finden waren. Erste schriftliche Belege des Arztberufs stammen aus Mesopotamien und wurden im 3. Jahrtausend v. Chr. verfasst.\n\nDie Ausbildung von Ärzten der Antike fand in sogenannten \"Ärzteschulen\" (z.&nbsp;B. Schule von Kos, Schule von Knidos, Alexandrinische Schule) statt, die sich hinsichtlich ihrer Wissenvermittlung an unterschiedlichen ärztlichen Theorien (z.&nbsp;B. Methodiker, Pneumatiker, Hippokratiker) und philosophischen Strömungen (z.&nbsp;B. Epikureer, Stoiker) ausrichteten.\n\nÜber den Arzt schreibt Hippokrates bzw. der Verfasser des wohl im 3. Jahrhundert v. Chr. entstandenen Textes \"Der Arzt\" ausführlich: „Er soll von gesundem Aussehen und im Verhältnis zu der ihm eigenen Konstitution wohlgenährt sein […]. Ferner soll sein Äußeres sauber sein, was in einer angemessenen Kleidung und in wohlriechenden Salben zum Ausdruck kommt, deren Geruch unverdächtig ist […]. Was die seelischen Eigenschaften betrifft, so sei er besonnen, was sich nicht nur darin äußert, daß er schweigen kann […]. Man soll saubere und weiche Läppchen benutzten, für die Augen Scharpie, für die Wunden Schwämme. […].“\n\nDie moderne Ausbildung von Ärzten begann im 18. Jahrhundert mit der Erweiterung des naturwissenschaftlichen Wissens und der Einführung von systematischem praktischem Unterricht am Krankenbett.\n\nEine einheitliche Prüfungsordnung (siehe auch Approbationsordnung) für Ärzte gab es in Deutschland erstmals 1883.\n\n2014 war der Anteil der Ärztinnen an der Gesamtzahl der berufstätigen Ärzte bereits auf 45,5 Prozent gestiegen, wenngleich der Anteil der Frauen 2015 zu Beginn des Studiums bei fast zwei Dritteln lag.\n\nBis ins 21. Jahrhundert galt für Ärzte \"Salus aegroti suprema lex\" („Das Wohl des Kranken sei oberstes Gebot“). Hinzugekommen ist in der Rechtsprechung das Selbstbestimmungsrecht des Patienten.\n\n\nGesundheit und Krankheitsverhalten.\nWährend die körperliche Gesundheit von männlichen Ärzten mit derjenigen der allgemeinen männlichen Bevölkerung vergleichbar zu sein scheint, scheint die körperliche Gesundheit von Ärztinnen besser zu sein als die der allgemeinen weiblichen Bevölkerung.\n\nHinsichtlich der psychischen Gesundheit fällt auf, dass Depressionen und Suchterkrankungen bei Ärzten häufiger vorkommen als in der restlichen Bevölkerung. Ein weiteres bei Medizinern häufig auftretendes Krankheitsbild ist das Burnout-Syndrom, das bereits bei Medizinstudenten in einer erhöhten Rate nachgewiesen werden kann.\n\nMehrere Studien zeigten eine gegenüber der allgemeinen Bevölkerung erhöhte Suizidrate unter Ärzten. Das gegenüber der Normalbevölkerung erhöhte relative Risiko, einen Suizid zu begehen, lag für Ärzte bei 1,1–3,4 und für Ärztinnen bei 2,5–3,7. Da in den Studien meist nur eine kleine Zahl von Suiziden untersucht wurde, waren die Vertrauensbereiche des wahren Wertes der Risikoerhöhung weit. Es wird vermutet, dass eine beträchtliche Anzahl von Selbstmorden nicht erfasst wird, da diese fälschlicherweise als Vergiftungen oder Unfälle deklariert werden. Von den verschiedenen beruflichen Spezialisierungen sind insbesondere Psychiater, Anästhesisten und Allgemeinmediziner von einer erhöhten Suizidrate betroffen. Als Ursachen des erhöhten Suizidrisikos werden verschiedene Faktoren diskutiert. Ein Persönlichkeitsprofil mit zwanghaften Zügen kann infolge der beruflichen Anforderungen zu einer depressiven Störung führen. Die Schwierigkeiten, Familie und Karrierewunsch miteinander zu vereinbaren, können insbesondere bei Ärztinnen zu Erschöpfung und Depression führen. Suchterkrankungen (wie beispielsweise Alkohol-, Drogen- und Medikamentenabhängigkeit), die bei Ärzten häufiger auftreten, gehen ihrerseits meistens mit Depressionen und einer erhöhten Suizidrate einher. Dieses für Ärzte und Ärztinnen festgestellte Risikoprofil ist berufsunabhängig und trifft für die meisten Suizidenten zu.\n\nPsychische Probleme korrelieren häufig mit Zeitdruck und mangelnder Autonomie am Arbeitsplatz sowie belastenden Patient-Arzt-Beziehungen. Ärzte werden seltener krankgeschrieben und zeigen eine mangelhafte Inanspruchnahme medizinischer Versorgungsleistungen. Häufig behandeln sich Ärzte selbst. Die eigenständige Behandlung eigener psychischer Störungen ist jedoch häufig ineffektiv.\n\n\nSchutzpatron.\nDie heiligen Zwillingsbrüder Cosmas und Damian gelten wegen ihres Arztberufs unter anderem auch als Schutzpatrone der Ärzte. Ein weiterer Schutzpatron ist der heilige Pantaleon, einer der Vierzehn Nothelfer.\n\n\nNationales.\n\nDeutschland.\n\nRechtliche Einordnung des Berufes.\nDer Arzt gehört in Deutschland (seit 1935) zu den Freien Berufen und ist (seit 1887) ein klassischer Kammerberuf.\n\nÄrzte unterliegen einer staatlichen Überwachung der Zulassung (Approbation in Deutschland, s. u. in anderen EU-Ländern) und unter anderem dem Arztwerberecht, welches weitgehende Einschränkungen in der Publikation und Veröffentlichungen bedeutet. Ärzte haften ihren Patienten zwar in der Regel nicht auf Erfolg ihres Handelns, können ihnen aber unter dem Gesichtspunkt der Arzthaftung zum Schadenersatz verpflichtet sein.\n\nDie freie Ausübung der Heilkunde ist in Deutschland nur approbierten Ärzten erlaubt. Mit festgelegten Einschränkungen dürfen auch Heilpraktiker Kranke behandeln, wobei die klar festgelegten Grenzen einzuhalten sind. Ausnahmsweise werden spezielle Bereiche der Diagnostik und Therapie auch (meist auf Veranlassung von Ärzten) von Angehörigen der Gesundheitsfachberufe durchgeführt.\n\nAb dem Zeitpunkt der ärztlichen Approbation darf der Arzt die gesetzlich geschützte Bezeichnung „Arzt“ führen und erhält mit ihr die staatliche Erlaubnis zur eigenverantwortlichen und selbstständigen ärztlichen Tätigkeit. Die bundesweit einheitliche Approbationsordnung regelt das zuvor erfolgreich abzuleistende mindestens sechsjährige Medizinstudium bezüglich der Dauer und der Inhalte der Ausbildung in den einzelnen Fächern, sowie der Prüfungen. Das Studium der Medizin umfasst u.&nbsp;a. drei Examina, sowie ein Jahr praktische Tätigkeit (sogenanntes „Praktisches Jahr“). Von Oktober 1988 bis Oktober 2004 war zur Erlangung der Vollapprobation zusätzlich eine 18-monatige, gering bezahlte Tätigkeit als Arzt im Praktikum unter Aufsicht eines approbierten Arztes gesetzlich vorgeschrieben. Meist arbeitet ein approbierter Arzt für mehrere Jahre als Assistenzarzt an von der Landesärztekammer anerkannten Weiterbildungsstätten (wie 1956 Krankenhäuser, 35,6 % waren 2015 in privater Trägerschaft; seltener einzelne Großpraxen), um sich auf einem oder mehreren Spezialgebieten der Medizin anrechenbar weiterzubilden und eventuell nach zusätzlich mindestens vierjähriger Weiterbildungszeit eine Facharztprüfung abzulegen. Die Anforderungen dazu sind in den Weiterbildungsordnungen der Landesärztekammern geregelt. Niedergelassene Ärzte arbeiten in freier Praxis, gegebenenfalls auch mit mehreren Ärzten in einer Berufsausübungsgemeinschaft (früher: Gemeinschaftspraxis) oder Praxisgemeinschaft (s.&nbsp;a. Vertragsarztrechtsänderungsgesetz). Honorarärzte arbeiten auf Honorarbasis für verschiedene Kliniken oder niedergelassene Ärzte.\n\nJeder Arzt ist meldepflichtiges Pflichtmitglied der Ärztekammer (des Bundeslandes), in deren Gebiet er wohnt bzw. seine ärztliche Tätigkeit ausübt. Im Jahr 2012 waren in Deutschland bei den Landesärztekammern 459.021 Ärzte gemeldet. Zur Behandlung von Versicherten der gesetzlichen Krankenversicherungen benötigt der Arzt eine Zulassung (Facharzt in eigener Praxis) oder Ermächtigung (als Arzt in einem Krankenhaus oder ähnlicher Institution) und ist dann auch Pflichtmitglied der Kassenärztlichen Vereinigung seines Niederlassungsbezirks. Die kassenärztliche Zulassung besitzen 135.388 Ärzte (Ende 2008): selbstständige 58.095 Hausärzte und 77.293 Fachärzte. In den Kliniken sind 146.300 Ärzte angestellt. Ende 2013 arbeiteten 35.893 ausländische Ärzte in Deutschland, öfter im Osten. 2013 betrug die Zahl der berufstätigen Ärzte in Deutschland 357.252.\n\nStrafrechtlich sind ärztliche Eingriffe der Körperverletzung gleichgesetzt. Diese ist nicht strafbar, wenn die Einwilligung der behandelten Person nach einer Aufklärung vorliegt und die Handlung auf dem Stand des aktuellen medizinischen Wissens vorgenommen wird (§§&nbsp;223&nbsp;ff. StGB). Ausnahmen bestehen, wenn der Patient aufgrund seines Zustandes (z.&nbsp;B. Bewusstlosigkeit) nicht in der Lage ist, seine Entscheidung mitzuteilen, und durch die Unterlassung des Eingriffs die Gefahr von negativen gesundheitlichen Folgen oder sogar dem Tod des Patienten besteht. Zudem können eingeschränkt- oder nichteinwilligungsfähige Personen, wie z.&nbsp;B. Kinder oder in bestimmten Fällen seelisch Erkrankte, auch gegen ihren Willen behandelt werden. Hierfür existieren strenge rechtliche Regelungen und Verfahrenswege, bei welchen neben dem Arzt auch andere Institutionen, z.&nbsp;B. Amtsgericht oder gesetzlicher Betreuer, an der Entscheidung mitwirken.\n\nVor Inkrafttreten des Gesetzes zur Bekämpfung von Korruption im Gesundheitswesen haben niedergelassene, für die vertragsärztliche Versorgung zugelassene Ärzte die Tatbestandsmerkmale des StGB nicht erfüllt, da diese laut Beschluss des Bundesgerichtshofs (BGH) vom 29. März 2012 weder als Amtsträger i.&nbsp;S.&nbsp;d. §&nbsp;11&nbsp;I Nr.&nbsp;2c StGB noch als Beauftragte der gesetzlichen Krankenkassen i.&nbsp;S.&nbsp;d. §&nbsp;299 StGB handelten. Die Gesetzeslücke wurde ab 4. Juni 2016 geschlossen, indem StGB (Bestechlichkeit im Gesundheitswesen) und StGB (Bestechung im Gesundheitswesen) hinzugefügt, sowie §&nbsp;300 und §&nbsp;302 StGB geändert wurden.\n\nDie Erteilung der Approbation hängt seit dem 1. April 2012 nicht mehr von der Staatsangehörigkeit ab (Änderung des §3 BAÖ durch § 29 des Gesetzes zur Verbesserung der Feststellung und Anerkennung im Ausland erworbener Berufsqualifikationen).\n\n\nKompetenzen und Pflichten.\nDie Verordnung von rezeptpflichtigen Arzneimitteln und die meisten invasiven Maßnahmen sind in Deutschland ausnahmslos dem approbierten Arzt vorbehalten. Hierbei ist er persönlich zur Einhaltung des anerkannten wissenschaftlichen Standes und medizinethischer Vorgaben verpflichtet. Die Genfer Deklaration orientierte sich 1948 am Eid des Hippokrates. Weiter unterliegen Ärzte speziellen Regelungen, wie dem Berufs- und Standesrecht, welches auch an die Genfer Konvention anknüpft. Insbesondere ist auch im Strafrecht die Einhaltung der ärztlichen Schweigepflicht nach §&nbsp;203 StGB festgehalten.\n\n\nAkademische Grade.\nIn Deutschland gibt es aus historischen Gründen unterschiedliche medizinische akademische Grade. Diese weisen im Gegensatz zum Facharzttitel nicht auf eine besondere Fachkompetenz hin, sondern dienen als Beleg einer wissenschaftlichen Leistung in einem medizinischen Bereich:\n\n\n\n\nBehandlungszeit.\nLaut einer Studie des Instituts für Qualität und Wirtschaftlichkeit im Gesundheitswesen haben deutsche Ärzte trotz längerer Arbeitszeiten je Patient die kürzeste Sprechzeit in Europa. Sie liegt 30 % niedriger als der europäische Durchschnitt.\n\nKlinikärzte verbringen rund 44 % ihrer Zeit für Schreibtätigkeiten und Protokolle (Stand: 2014/2015). Laut einem Projektbericht des Statistischen Bundesamts vom August 2015 wenden Arzt-, Psychotherapeuten- und Zahnarztpraxen jährlich durchschnittlich 96 Tage Zeit für die Erfüllung von Informationspflichten auf, wobei dieser Wert den gesamten Zeitaufwand aller Praxismitarbeiter darstellt und sämtliche Informationspflichten, auch die der gemeinsamen Selbstverwaltung, umfasst.\n\nLaut der deutschlandweiten MB-Online-Befragung des Marburger Bunds „MB-Monitor“ von 2017 sind 66 % der Krankenhausärzte der Auffassung, dass ihnen nicht ausreichend Zeit für die Behandlung ihrer Patienten zur Verfügung steht.\n\n\nEinkommen.\nDie Einkommen von Ärzten in Deutschland variieren, da das Spektrum medizinischer Tätigkeiten breit gefächert ist. Auch finden sich unter Ärzten Unterschiede bei der Arbeitszeit, insbesondere zwischen klinisch tätigen (beispielsweise 24 Stunden-Schichten sowie eine hohe Anzahl an Überstunden) und niedergelassenen (hoher Anteil „nicht-medizinischer“-Tätigkeit aufgrund der Selbständigkeit).\n\n\nNiedergelassene Ärzte.\nNach dem \"Zi-Praxis-Panel\" des Zentralinstituts für die kassenärztliche Versorgung in Deutschland (Jahresbericht 2017) über die wirtschaftliche Situation und die Rahmenbedingungen in der vertragsärztlichen Versorgung der Jahre 2013 bis 2016, lag der Mittelwert des Jahresüberschusses je Praxisinhaber im Jahr 2016 bei circa 170.000&nbsp;Euro.\n\nMittelwerte der Jahresüberschüsse je Praxisinhaber (nach ärztlichem Fachgebiet) in Deutschland im Jahr 2016:\n\nUm einem Mangel an Landärzten entgegenzuwirken, wollte die Bundesregierung 2011 in einem neuen „Versorgungsgesetz“ das Einkommen von Landärzten erhöhen. Unter einer Vielzahl von Gesetzen war das GKV-Versorgungsstrukturgesetz 2012 und Juni 2015 das Gesetz zur Stärkung der Versorgung in der gesetzlichen Krankenversicherung.\n\n\nKlinisch tätige Ärzte.\nDie durchschnittlichen Gehälter klinisch tätiger Ärzte unterscheiden sich stark nach den jeweiligen Positionen:\n\nAußendarstellung und Werbung.\nNeben den strengen rechtlichen Vorgaben zur Ausübung seines Berufs ist der Arzt auch bei der Außendarstellung bzw. Werbung zu seinen Leistungen und seiner Praxis umfangreichen Verordnungen und Gesetzen unterworfen. Im Unterschied zu anderen Branchen ist Ärzten anpreisende oder vergleichende Werbung absolut verboten. Seit dem 105. Deutschen Ärztetag 2002 sind sachliche, berufsbezogene Informationen über ihre Tätigkeit gestattet. Hauptkriterium ist dabei das schützenswerte Interesse des mündigen Patienten. Umstritten war ab 1998 die Individuelle Gesundheitsleistung eingeführt worden.\n\nStatistiken.\nEnde 2006 waren in Deutschland ca. 407.000 Ärzte gemeldet, davon 95.700 ohne ärztliche Tätigkeit (siehe Abb.). Die Kassenzulassung besaßen 59.000 Hausärzte und 60.600 Fachärzte. In den Krankenhäusern waren 148.300 Ärzte angestellt.\n\nIm Jahr 2011 wurden in Deutschland rund 342.100 berufstätige Ärzte und rund 107.300 Ärzte ohne ärztliche Tätigkeit gezählt. Auf durchschnittlich 239 Einwohner kam ein berufstätiger Arzt.\n\nDie chronologische Entwicklung kann aus der folgenden Tabelle und der Abbildung abgelesen werden.\n\nIn der Gesamtzahl approbierter Ärzte sind auch die nicht (mehr) berufstätigen und die nicht ärztlich tätigen Ärzte enthalten. Die Bundesärztekammer und die Kassenärztliche Bundesvereinigung haben für Deutschland 385.149 Ärztinnen und Ärzte gezählt, die 2017 ärztlich tätig waren, und damit 6.542 Ärzte mehr als im Vorjahr. Der Anteil von Frauen stieg weiter an und erreichte 2017 46,8 %, nach 46,5 % im Vorjahr. Auch der Anteil älterer Ärzte stieg weiterhin an. 2017 waren 18,4 % der Ärzte 60 Jahre oder älter (2016: 17,9 %). Insgesamt waren 2017 172.647 Ärztinnen und Ärzte in der vertragsärztlichen Versorgung, also als Niedergelassene tätig, selbständig oder bei einem Vertragsarzt angestellt.\n\nArztbesuche: Deutsche Erwachsene (zwischen 18 und 79 Jahren) gehen im Durchschnitt 9,2-mal pro Jahr zum Arzt.\n\n\nÖsterreich.\nIn Österreich ist man mit der Sponsion zunächst \"Doktor der gesamten Heilkunde\" (Doctor medicinae universae/Dr. med. univ.). Mittlerweile handelt es sich entgegen der Bezeichnung nicht um einen Doktorgrad, sondern um einen Diplomgrad ähnlich dem Magister oder dem Diplomingenieur. Vor dem Wintersemester 2002/03 war das Medizinstudium in Österreich ein Doktoratsstudium, welches auch Übergangsregelungen kannte. Der eigentliche Doktorgrad der Medizin (\"Doctor scientae medicinae\" bzw. \"Dr. scient. med.\") kann seitdem im Anschluss an das Diplomstudium in einem dreijährigen Doktoratsstudium erworben werden.\n\nSelbständig als Arzt tätig werden darf man nur, wenn für drei Jahre im Rahmen des „Turnus“ verschiedene (definierte) Disziplinen durchlaufen wurden und die Arbeit vom jeweiligen Abteilungsvorstand positiv bewertet wurde. Danach ist eine weiter abschließende Prüfung abzulegen. Damit hat man das „jus practicandi“ erworben, also die Berechtigung zur selbständigen Berufsausübung als Arzt für Allgemeinmedizin. Alternativ kann sofort nach der Sponsion die (meist sechsjährige) Ausbildung zu einem Facharzt erfolgen, nach der wiederum eine Prüfung abzulegen ist. Viele Fachärzte absolvieren den Turnus vor Beginn der Ausbildung ganz oder teilweise. Es hat sich in Österreich eingebürgert, die Ausbildung zum Allgemeinmediziner zuvor abzuleisten. Viele Krankenhäuser nehmen nur Assistenzärzte mit abgeschlossener Turnusausbildung in Dienst, da diese einen Nacht- oder Wochenenddienst alleine ableisten dürfen.\nÄrzte aus anderen EU-Staaten können um Anerkennung als \"approbierte Ärzte\" ansuchen.\n\nAm 14.&nbsp;Dezember 2010 hat die EU-Kommission in ihrem Amtsblatt C377/10 eine Änderungsmitteilung für die , Anhang 5.1.1. veröffentlicht, wonach ab diesem Zeitpunkt sämtliche Absolventen des österreichischen Medizinstudiums bereits mit der Promotion ihr Grunddiplom abgeschlossen haben und somit innerhalb des gesamten EU- und EWR-Raumes sowie der Schweiz und Liechtenstein eine selbständige Tätigkeit bzw. Ausbildung zum Facharzt unter denselben Voraussetzungen wie einheimische Mediziner aufnehmen dürfen. Bis dahin hatten Mediziner aus Österreich erst mit dem Abschließen der Ausbildung zum Allgemeinmediziner bzw. Facharzt ein Anrecht auf automatische Anrechnung ihres Diploms in den übrigen Mitgliedsstaaten.\n\nDer (niedergelassene) Arzt gehört in Österreich zu den Freien Berufen (Berufe von öffentlicher Bedeutung).\n\n\nWeiterbildung.\nÄrzte müssen in Österreich pro Jahr 50 Stunden Weiterbildung absolvieren, was alle 5 Jahre von der Ärztekammer kontrolliert wird.\n\n\nSchweiz.\n2017 arbeiteten in der Schweiz rund 36'700 (36'900, je nach Quelle) Ärzte, davon rund 15'200 (42 %) Frauen und 21'400 (58 %) Männer, 51 % im ambulanten und 47 % im stationären Sektor, rund 12'600 (34 %) waren Ausländer (d.&nbsp;h. ohne Schweizer Bürgerrecht).\n\n\nQualifikation, Fortbildung.\nIn der Schweiz ist man nach dem mit dem Staatsexamen abgeschlossenen sechsjährigen Studium zunächst eidgenössisch diplomierter Arzt und als solcher zur Arbeit als Assistenzarzt in Spitälern (Krankenhäusern) und Arztpraxen befugt.\n\nDie Weiterbildung zum zur selbständigen Berufsausübung befugten Facharzt (Spezialarzt) dauert je nach Fach zwischen 3 („praktischer Arzt“) und 8 Jahren nach dem Studienabschluss. Für einen Facharzttitel muss zudem eine Facharztprüfung abgelegt werden. Danach darf sich der Arzt „Facharzt für ⟨Fachgebiet⟩ FMH“ nennen.\n\nDie jeweilige Fachgesellschaft prüft, ob jeder Facharzt seiner Fortbildungspflicht (je nach Fachgebiet 60–100 Stunden pro Jahr) nachkommt.\"\"\n\n\nZulassung, Arztpraxen.\nDie Zulassung zur Berufsausübung zulasten der Krankenkassen wird vom Krankenkassenzentralverband Santésuisse erteilt, ist aber bei entsprechender Qualifikation nur eine Formalität.\n\nDie Erlaubnis zur Praxiseröffnung ist kantonal geregelt. Aktuell besteht aber ein Praxiseröffnungs-Stopp,\"\" welcher die Berufsausübung zulasten der Krankenkassen einschränkt. Lediglich bei Bedarfsnachweis, z.&nbsp;B. bei einer Praxisübernahme, ist eine Zulassung möglich.\"\"\n\n\nArbeitszeitgesetz für Assistenz- und Oberärzte.\nSeit dem 1. Januar 2005 gilt, nach längeren Kämpfen, für die Assistenzärzte und Oberärzte an Schweizer Spitälern das landesweit gültige Arbeitszeitgesetz und damit die darin festgelegte maximale Wochenarbeitszeit von 50 Stunden (Art. 9 ArG, Wöchentliche Höchstarbeitszeit). Sie ist zwar bedeutend höher als die allgemein übliche Arbeitszeit in der Schweiz (38,5–42,5 Stunden), doch ein gewisser Fortschritt – bis dahin waren Arbeitsverträge mit der Formulierung \"«Die Arbeitszeit richtet sich nach den Bedürfnissen des Spitals.»\" üblich, wodurch Arbeitszeiten von oft über 60 oder 70 Stunden pro Woche ohne finanziellen Ausgleich zu leisten waren. Die Entgelte der Assistenzärzte lagen deswegen auf dem Niveau der Pflegenden im oberen Kader (Pflegedienstleistungen).\n\n\nHierarchie der Spitäler, Berufskammern.\nDie Leitenden Ärzte und Chefärzte sind diesem Arbeitszeitgesetz nicht unterstellt. Auch sind sie finanziell in der Gesamtvergütung deutlich höher gestellt.\n\nDiese, vor allem auch historisch bedingte, hierarchische Trennung zeigen auch die getrennten Berufskammern der Spitalärzte VLSS und VSAO. Hingegen ist die ältere Ärztekammer FMH allen qualifizierten Ärzten offen, wie auch die fachlichen Ärzteverbände. Die Mitgliedschaft ist freiwillig, im Gegensatz zu anderen Ländern, wie Deutschland oder Österreich.\n\n\nLöhne, Einkommen.\nReferenzen: FMH / NZZ / VSAO\n\nZwar herrscht in der Schweiz (immer noch, 2017/18) kaum Transparenz bezüglich der Einkommensverhältnisse – im Allgemeinen und auch im ärztlichen Bereich. Wobei gilt – je höher gestellt, desto weniger Transparenz. Jedoch „sickern“ zuverlässige Angaben durch. So bemühen sich die Spitalleitungen neuerdings um mehr Transparenz. Wie das Zürcher Universitätsspital welches zurzeit «prüft», ob und in welcher Form es die Ärztelöhne künftig offenlegen soll.\n\nDie Hälfte der Ärzte in der Schweiz arbeiten in den Spitälern. Besonders gut bezahl sind dort Radiologen, Kardiologen, Gastroenterologen, Intensivmediziner und Urologen. Am unteren Ende der Lohnskala stehen Psychiater, Kinderärzte und Ärzte aus dem Bereich Physikalische Medizin und Rehabilitation. Die Normallöhne betragen (p.&nbsp;a.):\n\nDiese Angaben eines Beratungsunternehmens decken sich mit denjenigen des Vereins der Leitenden Spitalärzte der Schweiz (VLSS) – in einer seiner Umfragen deklarierten die Kaderärzte folgende durchschnittlichen Löhne:\n\nZu den Grundlöhnen und Boni kommen, besonders bei Kaderärzten, Zusatzhonorare aus Behandlungen von zusatzversicherten Patienten im stationären Bereich sowie bei Grund- und Zusatzversicherten im spitalambulanten Bereich. Die können bei Chefärzten bis zum 9-Fachen des Grundlohns betragen. \"«Einzelne Chefärzte kommen so auf Jahreslöhne von 2 Millionen Franken oder mehr»\", sagt ein Berater, der auch bemängelt, dass die Chefärzte oft selbst darüber bestimmen können, wie die Honorare verteilt werden.\n\n\n\n\nWeblinks.\nNationales:\n"}
{"id": "105", "url": "https://de.wikipedia.org/wiki?curid=105", "title": "Anthropologie", "text": "Anthropologie\n\nAnthropologie (im 16. Jahrhundert als \"anthropologia\" gebildet aus , und \"-logie:\" Menschenkunde, Lehre vom Menschen) ist die Wissenschaft vom Menschen. Sie wird im deutschen Sprachraum und in vielen europäischen Ländern vor allem als Naturwissenschaft verstanden. Die \"naturwissenschaftliche\" oder \"Physische Anthropologie\" betrachtet den Menschen im Anschluss an die Evolutionstheorie von Charles Darwin als biologisches Wesen.\n\nDieser naturalistischen Betrachtung des Menschen, die sich beispielsweise mit der Konstitution (früher auch mit der Rassenlehre und Humangenetik) und der Abstammung des Menschen befasst, stehen verschiedene andere Ansätze gegenüber, beispielsweise die philosophische Anthropologie. Hier wird der Mensch nicht nur als Objekt, sondern auch als Subjekt wissenschaftlich untersucht. Dabei geht es unter anderem um qualitative Eigenschaften wie die Personalität, die Entscheidungsfreiheit und die Möglichkeit zur Selbstbestimmung. Im englischen Sprachraum wird auch die Ethnologie als Kultur- beziehungsweise Sozialanthropologie als Teil der Anthropologie verstanden und ist mit der physischen Anthropologie häufig auch in gemeinsamen Fakultäten oder Instituten vereinigt. In der deutschen Wissenschaftspolitik ist die Anthropologie als Kleines Fach eingestuft.\n\n\nGeschichte der Anthropologie.\nDie Bezeichnung \"Anthropologie\" geht zurück auf den deutschen Philosophen, Arzt und Theologen Magnus Hundt (1449–1519), der in einem 1501 erschienenen Werk schrieb „Antropologium de hominis dignitate, natura et proprietatibus, de elementis, partibus et membris humani corporis“. Zu den ersten Dozenten für das Fach gehörte der Anatom und Physiologe Heinrich Palmatius Leveling, der die Anthropologie 1799 an der Ingolstädter Universität als Vorlesung anbot. Ein Lehrstuhl für „Allgemeine Naturgeschichte und Anthropologie“ wurde 1826 in München eingerichtet. Friedrich Nasse gab von 1823 bis 1826 in Leipzig die aus der \"Zeitschrift für psychische Ärzte\" hervorgegangene \"Zeitschrift für die Anthropologie\" heraus. Auf den ersten eigenständigen Lehrstuhl Deutschlands für (physische) Anthropologie wurde am 1. August 1886 Johannes Ranke berufen, dem 1917 der Schweizer Rudolf Martin (1864–1925) folgte, der 1918 Direktor des Anthropologischen Instituts und der Anthropologisch-Prähistorischen Staatssammlung wurde. Martin war 1900 zum Extraordinarius und 1905 zum Ordinarius für Anthropologie an der Universität Zürich ernannt worden.\n\n\nNaturwissenschaftlicher Ansatz.\n\nBiologische Anthropologie.\nDie biologische Anthropologie ist mit ihren Teilgebieten Primatologie, Evolutionstheorie, Paläoanthropologie, Bevölkerungsbiologie, Industrieanthropologie, Genetik, Sportanthropologie, Wachstum (Auxologie), Konstitution und Forensik ein Fachbereich der Humanbiologie. Ihr Ziel ist die Beschreibung, Ursachenanalyse und evolutionsbiologische Interpretation der Verschiedenheit biologischer Merkmale der Hominiden (Familie der Primaten, die fossile und rezente Menschen einschließt). Ihre Methoden sind sowohl beschreibend als auch analytisch.\n\nInstitutionen im deutschsprachigen Raum gibt es an Universitäten und an Museen in Tübingen, Kiel, Hamburg, Berlin, Göttingen, Jena, Gießen, Mainz, Ulm, Freiburg im Breisgau, München, Zürich und Wien. Meist ist dort die Bezeichnung nur „Anthropologie“, Zusätze wie „biologisch“ wurden in jüngerer Zeit notwendig, weil der konkurrierende US-amerikanische Begriff der auch hier bekannt ist.\n\n\nForensische Anthropologie.\nDie forensische Anthropologie ist eine der drei gerichtlichen Wissenschaften vom Menschen, neben der Rechtsmedizin und der forensischen Zahnmedizin.\n\nGebiete der forensischen Anthropologie:\n\nDie forensische Anthropologie dient mit den Mitteln der Anthropologie bei der Aufklärung von Verbrechen. Forensische Anthropologen haben vor allem mit der Identifikation von Bankräubern, Schnellfahrern etc. zu tun, aber auch häufig mit stark verwesten oder vollständig skelettierten Leichen. Nicht selten sind sie die letzte Hoffnung zur Aufklärung eines Verbrechens. In Deutschland gibt es eine starke institutionelle Dominanz der Rechtsmedizin, aber gerade das verhindert manchmal den Zugang zu der eigenständigen Kompetenz der Anthropologie.\n\n\nGeisteswissenschaftlicher Ansatz.\n\nSozialanthropologie.\nDie Sozialanthropologie gilt als Wissenschaft der kulturellen und sozialen Vielfalt – oder allgemeiner als „Wissenschaft vom Menschen in der Gesellschaft“. Sie analysiert die soziale Organisation des Menschen. Im deutschen Sprachraum war der Begriff „Sozialanthropologie“ eine seit den 1960er Jahren gebrauchte Bezeichnung für die britische oder die französische , wurde dann aber zugunsten der Fachbezeichnung „Ethnosoziologie“ aufgegeben (Fachbereich der Ethnologie). In den letzten Jahren ist jedoch eine Renaissance des Anthropologie-Begriffs zu beobachten, die einer durch Transnationalisierungs- und Globalisierungs&shy;prozesse veränderten Forschungslandschaft Rechnung tragen möchte.\n\n\nKulturanthropologie.\nDie Kulturanthropologie ist eine empirisch gestützte Wissenschaft von der Kultur (im Sinne von „menschliche Kultur“). Sie entwickelte sich im 20.&nbsp;Jahrhundert aus der Volkskunde, hat ihren Schwerpunkt im Gegensatz zu dieser aber in interkulturellen, ethnologischen und soziologischen Themen und Modellen. Unter den anthropologischen Fachrichtungen nimmt die Kulturanthropologie eine Mittelposition zwischen den biologisch und den philosophisch orientierten Richtungen ein; sie ist in ihrem Themenspektrum am weitesten gefasst.\n\nIm deutschen Sprachraum hat sich bisher keine genauere Definition des Forschungsgegenstandes durchgesetzt. In den USA dagegen bezeichnet \"cultural anthropology\" die Ethnologie (Völkerkunde). Im Deutschen wird die ungenaue englische Bezeichnung \"anthropology\" teils falsch mit „Anthropologie“ übersetzt, während eigentlich die Ethnologie gemeint ist.\n\n\nRechtsanthropologie.\nDie Rechtsanthropologie bildet eine eigenständige Unterform der Kulturanthropologie. Sie untersucht Inhalt und Funktionsweisen rechtlicher Strukturen des Menschen unterschiedlicher kultureller Traditionen von Stämmen und Völkern (siehe auch Rechtsethnologie). Zudem bezeichnet dieser Begriff eine rechtswissenschaftliche Forschungsrichtung, die sich den naturalen Grundkonstanten von Gesetzgebung und Rechtsprechung verschrieben hat. Dabei beschäftigt sich die Rechtsanthropologie vorwiegend mit dem (westlich-demokratischen) „Menschenbild der Verfassung“, das demgegenüber vom im Willen freien und eigenverantwortlich handelnden Menschen ausgeht. Dafür wählt sie zumeist einen pragmatisch-dualen Ansatz. Der Begriff Kultur, gelegentlich auch der politischere Begriff der Zivilisation, beschreibt dann die sozial-reale Welt, in der der Mensch beide Sichtweisen vereint.\n\n\nPhilosophische Anthropologie.\nDie philosophische Anthropologie ist die Disziplin der Philosophie, die sich mit dem Wesen des Menschen befasst. Die moderne philosophische Anthropologie ist eine sehr junge philosophische Fachrichtung, die erst im frühen 20.&nbsp;Jahrhundert als Reaktion auf den Verlust von Weltorientierung entstand. Mit Ausnahme von René Descartes, der bereits Mitte des 17. Jahrhunderts in seinen \"Meditationen über die erste Philosophie\" (1641) gewisse Zweifel am mittelalterlich-christlichen Weltbild hegt und Position zu Verhältnis von Körper und Seele bezieht. Er vermittelt ein neues philosophisches Gedankengut wie: „Das Denken (=Bewusstsein ) ist es; es allein kann von mir nicht abgetrennt werden; ich bin; ich existiere - das ist gewiss […] Demnach bin ich genau genommen ein denkendes Ding, d.&nbsp;h. Geist bzw. Seele bzw. Verstand […]“\n\n\nHistorische Anthropologie.\nHistorische Anthropologie bezeichnet einerseits die anthropologische Forschung in der Geschichtswissenschaft, andererseits eine transdisziplinäre Forschungsrichtung, die die historische Veränderlichkeit von Grundphänomenen des menschlichen Daseins untersucht. Dabei bezieht sie die Geschichtlichkeit ihrer Blickrichtungen und methodischen Herangehensweisen sowie die Geschichtlichkeit ihres Gegenstandes, also das Erscheinungsbild des Menschen in den unterschiedenen Epochen, aufeinander.\n\n\nTheologische Anthropologie.\nDie theologische Anthropologie als Teilbereich der Systematischen Theologie deutet den Menschen aus christlich-theologischer Sicht. Dabei beschäftigt sie sich besonders mit dem Wesen des Menschen und der Bestimmung des Menschen vor Gott. Im Unterschied dazu untersucht die Religionsethnologie als Fachgebiet der Ethnologie (Völkerkunde) die Religionen bei den weltweit rund 1300&nbsp;ethnischen Gruppen und indigenen Völkern, in Abgrenzung zur Religionssoziologie vor allem bei (ehemals) schriftlosen Kulturen.\n\n\nIndustrieanthropologie.\nDie Industrieanthropologie als Disziplin der Anthropologie untersucht die Gebrauchstauglichkeit () und Benutzerfreundlichkeit von Arbeitsplätzen, von Bedienelementen sowie von Produkten.\n\n\nMedienanthropologie.\nDie Medienanthropologie (auch Anthropologie der Medien oder Anthropologie des Medialen) ist ein junges, interdisziplinäres Forschungsgebiet zwischen Medienwissenschaft und Anthropologie. In der Medienanthropologie werden die Produktion und Nutzung von Medien sowie deren Effekte zumeist mit kulturwissenschaftlichen und ethnografischen Methoden erforscht. Medienanthropologische Forschung wird zudem oft im Zusammenhang mit Medienpädagogik diskutiert. „Medienanthropologisch verstanden sind Menschen Wesen, die sich in Medienpraktiken und -techniken artikulieren, wahrnehmen und wahrnehmbar machen, weil sie etwas darstellen und sich ihnen etwas darstellt.“\n\n\nAndere Ansätze und Mischformen.\n\nAnthropologie in den Sozialwissenschaften.\nIn den Sozialwissenschaften ist die Vorstellung weit verbreitet, dass der Mensch seinem Wesen nach in seinen Antrieben und Bedürfnissen unbestimmt ist, weshalb erst in Vergesellschaftungsprozessen eine Orientierung und Stabilisierung des Verhaltens und Antriebslebens entstehen kann. Dieses Menschenbild bildet die allgemeine anthropologische Voraussetzung für die Analyse von sozialen Prozessen, so etwa bei Karl Marx, Max Weber, George Herbert Mead oder Talcott Parsons.\n\nDarüber hinaus gibt es in den Sozialwissenschaften zwei klassische Menschenbilder, die als analytische und idealtypische Modelle fungieren: der \"homo oeconomicus\" der Wirtschaftswissenschaften und der \"homo sociologicus\" der Soziologie. Eine „realistische“ Variante des individualistischen \"homo oeconomicus\" ist das RREEMM-Modell des Menschen, allerdings wird in der sozialwissenschaftlichen Theoriebildung wegen Operationalisierungsproblemen auch weiterhin überwiegend auf die einfacheren Modelle zurückgegriffen.\n\nAusgehend von der Einbeziehung amerikanischer Sozialforscher in den Vietnamkrieg (\"Project Camelot\") wurde im Rahmen der \"Critical Anthropology\" ab 1970 eine „reflexive Anthropologie“ entwickelt (Bob Scholte 1970). Die Grundannahme der reflexiven Anthropologie besteht darin, dass sozialwissenschaftliche Aussagen nur dann einer Kritik standhalten, wenn sie die soziale und kulturelle Einbettung des Forschers und der Forschung mit bedenken (reflektieren). Gemäß dem Erkenntnisinteresse jeder Anthropologie („erkenne dich selbst“: \"gnothi seauton\") ist auf diesem Weg eine Unterscheidung möglich zwischen einer Sozialforschung als Informationsgewinnung über andere Menschen („Ausspähen“, vergleiche Informationelle Selbstbestimmung) oder als Beitrag zur Selbsterkenntnis des Forschers und seiner Auftraggeber. Bedeutende Ansätze zu einer reflexiven Anthropologie wurden von Michel Foucault und Pierre Bourdieu vorgelegt.\n\nDas Konzept der reflexiven Anthropologie von Gesa Lindemann schließt sich im Gegensatz dazu an die historisch-reflexive Richtung innerhalb der deutschsprachigen „philosophischen Anthropologie“ (Helmuth Plessner) an. Allgemeine Aussagen der philosophischen Anthropologie werden nicht als sozialtheoretisches Fundament begriffen, sondern zum Gegenstand der Beobachtung gemacht. Bei diesem Ansatz geht es um die Bearbeitung der Frage, wie in Gesellschaften der Kreis sozialer Personen begrenzt wird und welche Funktion der Anthropologie in der Moderne zukommt.\n\n\nPsychologische Anthropologie.\nIn dem verwendeten Schema kann die Psychologie des Menschen nicht gut untergebracht werden, denn die Psychologie vereint geisteswissenschaftliche, biologische, verhaltens- und sozialwissenschaftliche Konzepte und Methoden. Als Wissenschaft vom Erleben und Verhalten des Menschen einschließlich der biologischen bzw. neurowissenschaftlichen Grundlagen ist die Psychologie von vornherein interdisziplinär ausgerichtet. Wegen dieses umfassenden Blicks auf den Menschen kann die empirische Psychologie in ein besonderes Spannungsverhältnis zur Philosophischen Anthropologie geraten, die ebenfalls einen umfassenden theoretischen Ansatz hat, jedoch die empirischen Humanwissenschaften kaum noch zu integrieren vermag. Wichtige Themen der Psychologischen Anthropologie sind u.&nbsp;a. das Menschenbild, die Persönlichkeitstheorien, die Grundlagen von Motiven, Emotionen in der Neurobiologie und Psychophysiologie, die Beiträge der Kognitionswissenschaft, Sozialpsychologie und Kulturpsychologie, alle Bereiche der Angewandten Psychologie und so weiter.\n\nAuch Psychoanalyse und Psychosomatik galten als anthropologische Disziplinen.\n\n\nPädagogische Anthropologie.\nDie pädagogische Anthropologie ist der Teilbereich der Pädagogik, der sich mit dem Ertrag anthropologischer Fragen, den Zugangsweisen und den Ergebnissen innerhalb der Pädagogik befasst. Grob lassen sich hier zwei Richtungen unterscheiden: Die \"Realanthropologie\" widmet sich der empirischen Betrachtung der Wirklichkeit des Menschen unter dem Fokus, der sich aus der Pädagogik ergibt. Die \"Sinnanthropologie\" fragt nach dem Sinn und den Zielen menschlichen Handelns, die in den pädagogischen Kontext eingearbeitet werden. Die Sinnanthropologie weist so besondere Bezüge zur Bildungstheorie auf, indem sie aus einem je spezifischen Menschenbild Bildungsansprüche ableitet. Sie weist innerhalb der verschiedenen Anthropologien eine besondere Nähe zur philosophischen und theologischen Anthropologie auf. Die Realanthropologie steht besonders der biologischen, daneben auch der philosophischen Anthropologie nahe.\n\nDie Einteilung setzte sich in den 1960er Jahren fort in der Unterscheidung zwischen integrativen und philosophischen Ansätzen. Die „integrativen“ Ansätze versuchen vor allem, anthropologische Erkenntnisse verschiedener Teildisziplinen (insbesondere der Biologie, der Soziobiologie und so weiter) für pädagogische Fragen nutzbar zu machen. Vertreter dieses Ansatzes sind unter anderem Heinrich Roth und Annette Scheunpflug. Der „philosophische“ Ansatz hat sich in verschiedenen Richtungen ausdifferenziert. So besteht Otto Friedrich Bollnows Ansatz darin, anthropologische Fragen (beispielsweise nach dem Wesen des Menschen und seiner Bestimmung) für pädagogische Zusammenhänge nutzbar zu machen. Ähnlich wie andere Autoren orientierte er sich in seinen Arbeiten aber auch an der Phänomenologie. Er versuchte also nicht, aus der Philosophie (oder etwa der Biologie) ein Menschenbild zu gewinnen und es pädagogisch auszuwerten, sondern widmete sich dem pädagogischen Handeln und darin auftretenden Phänomenen wie Krise oder Begegnung unmittelbar, um sie als Bestimmungsgrößen des Menschen zu reflektieren. Der Mensch kommt bei diesen Untersuchungen im Hinblick auf Erziehung in drei Rollen vor: als Erziehender, als Zögling und als Erzieher.\n\nIn der neueren pädagogischen Anthropologie wird zum einen der integrative Ansatz fortgeführt (beispielsweise auch in der Betrachtung neuerer humanmedizinischer Ergebnisse für Pädagogik). Die philosophische Anthropologie wird heute verstärkt als historische pädagogische Anthropologie fortgesetzt, indem reflektiert wird, dass anthropologische Kenntnisse sowohl auf bestimmte Menschen in bestimmten Epochen bezogen als auch aus einer je spezifischen historischen Position heraus gewonnen werden und deshalb keine überzeitlich allgemeine Gültigkeit beanspruchen können.\n\n\nKybernetische Anthropologie.\nKybernetische Anthropologie bezeichnet den Versuch der begrifflichen Kopplung von Anthropologie und Kybernetik mit dem Vorhaben, den Gegensatz zwischen Natur- und Geisteswissenschaften zu überwinden. Die Cyberanthropologie ist ein neueres Fachgebiet der Ethnologie (Völkerkunde) oder Sozialanthropologie und untersucht transnational zusammengesetzte Online-Gemeinschaften unter Berücksichtigung kybernetischer Perspektiven.\n\n\nMedizinische Anthropologie.\nDie im 16. Jahrhundert aufgekommene medizinische Anthropologie beschäftigt sich mit der Wechselwirkung von Kultur und Medizin.\n\n\nAnthropologie als Oberbegriff und Dachwissenschaft.\nManchmal wird „Anthropologie“ als Oberbegriff für mehrere der oben genannten Einzel- und Humanwissenschaften aufgefasst. Insbesondere in den USA gibt es dementsprechende Bestrebungen, biologische Anthropologie, Kulturanthropologie, Ethnolinguistik und Archäologie unter einem Dach zu vereinen (sog. „Vier-Felder-Anthropologie“). Diese weit verbreitete Auffassung leitet sich von dem Tatbestand her, dass Anthropologie – im Gegensatz und oft in Konkurrenz zur Theologie – Selbsterkenntnis des Menschen als Mensch ist, gemäß der delphischen Maxime \"Gnothi seauton\", „erkenne dich selbst“.\n\nDie \"Systematische Anthropologie\", ein 1977 veröffentlichtes Werk der deutschen Ethnologen Wolfgang Rudolph und Peter Tschohl, bringt anthropologisch grundlegende Erkenntnisse in einen integrierten Zusammenhang. Mit Hilfe eines eigenen Begriffssystems wird ein gesamtanthropologisches Modell entwickelt, das die Grenzen und Überschneidungen von Disziplinen wie Ethnologie, Biologie, Humangenetik, Psychologie, Soziologie, Philosophie, Geschichte theoretisch auflöst (vergl. zu diesem Ansatz: Interdisziplinarität). „Ziel der Untersuchung ist eine wissenschaftliche Theorie, die dasjenige abdeckt, was systematisch sinnvoll zu einem „Mensch“ genannten Untersuchungsgegenstand gerechnet werden kann, und die damit nicht von einer einzelnen Fachrichtung beherrscht wird.“\n\nDie Untersuchung erschließt ausgehend von allgemeinen Bedingungen der Gesamtwirklichkeit die besonderen Bedingungen des biotischen und humanen Bereichs. Dafür wurde eine global orientierte Auswahl an Studien ausgewertet und die daraus entwickelte interdisziplinäre Systematik theoretisch konsequent ausformuliert. So lautet ein zentrales Untersuchungsergebnis in Kurzform: „Anthropologie ist zu explizieren als Theorie der Klassenexistenz ‚Menschliche Existenz‘ ME. Sie hat damit den vorverständlichen Gegenstandsbereich Mensch als Existenzklasse M aufzufassen und systematisch darzulegen.“ Gegenstand ist die menschliche Existenz als empirisch beschreibbare Tatsache.\n\nDie Theorie transportierte einen damals fortschrittlichen, humanen und weit gefassten Kulturbegriff. Wegen technokratisch anmutender Formulierung wurde sie aber nur in der ethnologisch und soziologisch orientierten Fachwelt rezipiert. Gerüst und Inhalt der Theorie müssten heute aktualisiert werden, bieten jedoch „eine Basis für Einzeluntersuchungen von beliebigen Ausschnitten des Gegenstandsbereichs Mensch“.\n\nDie praktische Relevanz und damit die Rezeption der \"systematischen Anthropologie\" von Rudolph und Tschohl waren bereits bei Erscheinen des Werks 1977 äußerst begrenzt. Kritiker wiesen darauf hin, dass die positivistische Begriffssystematik völlig abgehoben von den aktuellen Diskussionen in den Sozialwissenschaften entwickelt worden war. Ihr theoretischer Wert lag in der Einübung einer hierarchisch vernetzten Nomenklatur, die zwar als Ausgangspunkt für empirische Untersuchungen hätte dienen können, wenn sie allgemeine Akzeptanz gefunden hätte, aber über die Wirklichkeit menschlicher Lebensverhältnisse nicht viel mehr aussagte als ein systematisch geordneter Katalog der europäischen wissenschaftlichen Terminologie in den Humanwissenschaften. Ungeklärt blieb auch die Frage, wie die Begriffssystematik von Rudolph und Tschohl in andere Sprach- und Kultursysteme hätte übertragen werden können. Fruchtbarere Ansätze wie das Konzept der reflexiven Anthropologie (vergl. dazu Pierre Bourdieu) und Ethnomethodologie wurden dagegen aus dem anthropologischen Lehrbetrieb verdrängt.\n\nDie \"Basis-Theorie der Anthropologie\" ist ebenfalls Orientierungswissen, das Zusammenhänge zwischen den Disziplinen und Schulen der Humanwissenschaften aufzeigt. Ein Bezugsrahmen ergibt aus den vier Grundfragen der biologischen Forschung (nach Nikolaas Tinbergen): Verursachungen (= Ursache-Wirkungs-Beziehungen bei den Funktionsabläufen), Ontogenese, Anpassungswert, Phylogenese. Diese vier Aspekte sind jeweils auf verschiedenen Bezugsebenen zu berücksichtigen (vergleiche Nicolai Hartmann), beispielsweise Zelle, Organ, Individuum, Gruppe:\n\nDem tabellarischen Orientierungsrahmen aus Grundfragen und Bezugsebenen lassen sich alle anthropologischen Fragestellungen (siehe PDF-Übersichtstabelle, Absatz A), ihre Ergebnisse (siehe Tabelle, Absatz B) und Spezialgebiete zuordnen (siehe Tabelle, Absatz C); er ist Grundlage für eine Strukturierung der Ergebnisse. Mit Hilfe der Basistheorie kann die anthropologische Forschung in Theorie und Empirie vorangetrieben und fundiertes sowie spekulatives Wissen besser auseinandergehalten werden (betrifft z.&nbsp;B. den Schulenstreit in der Psychotherapie).\n\n\nLiteratur.\nAllgemein:\n\nGeschichte:\n\nMedizinische Anthropologie:\n\nVergleichende Anthropologie:\n\nPädagogische Anthropologie:\n\nSpezielle Themen:\n\n"}
{"id": "107", "url": "https://de.wikipedia.org/wiki?curid=107", "title": "Alexander der Große", "text": "Alexander der Große\n\nAlexander der Große () bzw. \"Alexander III. von Makedonien\" (* 20. Juli 356 v. Chr. in Pella; † 10. Juni 323 v. Chr. in Babylon) war von 336 v.&nbsp;Chr. bis zu seinem Tod König von Makedonien und Hegemon des Korinthischen Bundes.\n\nAlexander dehnte die Grenzen des Reiches, das sein Vater Philipp II. aus dem vormals eher unbedeutenden Kleinstaat Makedonien sowie mehreren griechischen Poleis errichtet hatte, durch den sogenannten Alexanderzug und die Eroberung des Achämenidenreichs bis an den indischen Subkontinent aus. Nach seinem Einmarsch in Ägypten wurde er dort als Pharao begrüßt. Nicht zuletzt aufgrund seiner großen militärischen Erfolge wurde das Leben Alexanders ein beliebtes Motiv in Literatur und Kunst, während Alexanders Beurteilung in der modernen Forschung, wie auch schon in der Antike, zwiespältig ausfällt.\n\nMit seinem Regierungsantritt begann das Zeitalter des Hellenismus, in dem sich die griechische Kultur über weite Teile der damals bekannten Welt ausbreitete. Die kulturellen Prägungen durch die Hellenisierung überstanden den politischen Zusammenbruch des Alexanderreichs und seiner Nachfolgestaaten und wirkten noch jahrhundertelang in Rom und Byzanz fort.\n\nLeben.\n\nFrühe Jahre (356–336 v. Chr.).\nAlexander wurde im Jahre 356 v. Chr. als Sohn König Philipps II. von Makedonien und der Königin Olympias geboren. Viele Einzelheiten seiner Biografie, vor allem aus der Kindheit, wurden bald legendenhaft ausgeschmückt oder frei erfunden. So berichtet Plutarch gut 400 Jahre später, dass Alexander ohne Zweifel seinen Stammbaum väterlicherseits auf Herakles und Karanos, den ersten König der Makedonen, zurückverfolgen konnte, wodurch Plutarch zugleich die Abstammung Alexanders vom Göttervater Zeus implizit hervorhebt.\nEbenso berichtet er, dass Olympias und Philipp Träume gehabt hätten, die ihnen der Seher Aristander so deutete, dass ihnen die Geburt eines Löwen bevorstehe. Olympias nahm für sich in Anspruch, in direkter Linie von dem griechischen Heros Achilleus und Aiakos, einem weiteren Sohn des Zeus abzustammen. Gemäß einer (wohl ebenfalls legendären) Erzählung Plutarchs soll Alexander in jungen Jahren sein Pferd Bukephalos, das ihn später bis nach Indien begleitete, gezähmt haben, nachdem es zuvor niemandem gelungen war, es zu bändigen. Alexander erkannte, was den Fehlschlägen der anderen zugrunde lag: Das Pferd schien den eigenen Schatten zu scheuen. Daraufhin habe Philipp zu ihm gesagt:\n\nAbgesehen von derlei Legenden ist wenig über Alexanders Kindheit bekannt. Makedonien war ein Land, das im Norden des Kulturraums des antiken Griechenlands lag. Es wurde von vielen Griechen als „barbarisch“ angesehen, und nur das Königsgeschlecht wurde aufgrund der behaupteten Abstammung von Herakles als griechisch anerkannt. In der Antike gab es keinen einheitlichen Staat Griechenland, sondern eine durch gemeinsame Kultur, Religion und Sprache verbundene Gemeinschaft der griechischen Klein- und Stadtstaaten. Im frühen 5. Jahrhundert v. Chr. wurden erstmals Makedonen als Vertreter der Könige zu den Olympischen Spielen zugelassen, nachdem Alexander I. eine Abstammung aus dem griechischen Argos und von Herakles in Anspruch genommen hatte. Noch heute birgt die Diskussion um die ethnische Zugehörigkeit der antiken Makedonen politischen Konfliktstoff.\n\nAus den verfügbaren Quellen ist ersichtlich, dass das Makedonische, von dem nur wenige Wörter überliefert sind, für die Griechen wie eine fremde Sprache klang. Ob das Makedonische ein nordgriechischer Dialekt oder eine mit dem Griechischen verwandte eigenständige Sprache war, ist immer noch umstritten. Kulturell und gesellschaftlich unterschieden sich die Makedonen jedenfalls recht deutlich von den Griechen: keine städtische Kultur (siehe Polis), als Binnenreich kaum Kontakte zum mediterranen Kulturraum, und eine monarchische Staatsform, was in Griechenland zu dieser Zeit nicht die Regel war. Gerade das Königtum galt den Hellenen zu dieser Zeit als eine grundsätzlich ungriechische, barbarische Regierungsform. Auf viele Griechen wird die makedonische Gesellschaft zumindest archaisch gewirkt haben. Erst im späten 6. Jahrhundert v.&nbsp;Chr. verstärkte sich der griechische kulturelle Einfluss in der makedonischen Oberschicht.\n\nAlexanders Vater Philipp II. hatte das bisher eher unbedeutende Makedonien, das vor ihm Streitobjekt der Adelsfamilien und Kleinkönige des Hoch- und des Tieflands gewesen war, geeint, seine Grenzen gesichert und es nicht zuletzt dank der Erschließung reicher Edelmetallvorkommen zur stärksten Militärmacht der damaligen Zeit gemacht. Er hatte Thessalien und Thrakien erobert und zuletzt alle griechischen Stadtstaaten mit Ausnahme Spartas in ein Bündnis unter seiner Führung gezwungen (Korinthischer Bund). Philipp begann anschließend mit den Vorbereitungen für einen Feldzug gegen die Perser.\n\nSchon an den Kriegszügen gegen die Griechen war Alexander zuletzt beteiligt, vor allem in der Schlacht von Chaironeia (338 v. Chr.), in der ein Bündnis griechischer Poleis unter Führung Athens und Thebens unterworfen wurden. Die makedonische Phalanx erwies sich dabei als ein wichtiges Element für den militärischen Erfolg, zentral war jedoch die Rolle der Hetairenreiterei, die Alexander bei Chaironeia kommandierte. Seine späteren Erfolge gehen zweifellos zu einem bedeutenden Teil auf die Militärreformen seines Vaters zurück. Philipp umgab sich außerdem mit sehr fähigen Offizieren, wie etwa Parmenion, die auch einen großen Anteil an Alexanders späteren Siegen hatten.\n\nPhilipp holte den griechischen Philosophen Aristoteles in die makedonische Hauptstadt Pella und beauftragte ihn, Alexander in Philosophie, Kunst und Mathematik zu unterrichten. Der Einfluss des Aristoteles sollte wohl nicht zu hoch veranschlagt werden, doch sicher war Alexander gebildet; seine Abschrift der \"Ilias\" hütete er laut Plutarch wie einen Schatz, und er brachte der griechischen Kultur große Bewunderung entgegen.\n\nDas Verhältnis zwischen Vater und Sohn war keineswegs frei von Konflikten, gerade in Hinsicht auf die Liebschaften des Vaters, durch die sich Alexander bedroht sah. Philipp hatte 337 v. Chr. Kleopatra, die Nichte seines Generals Attalos, als Nebenfrau geheiratet. Während eines Banketts soll Attalos Öl ins Feuer gegossen und gesagt haben, er hoffe, dass Philipp nun endlich einen legitimen Erben erhalten würde. Alexander, dessen Mutter keine Makedonin war, sei daraufhin wutentbrannt aufgefahren und habe Attalos angeschrien:\nAlexander warf einen Becher nach Attalos und wollte auf ihn losgehen. Auch Philipp erhob sich und zog sein Schwert, jedoch nicht um Alexander in Schutz zu nehmen, sondern um Attalos zu helfen. Da aber Philipp bereits betrunken war, stolperte er und fiel hin. Alexander soll ihn, so Plutarch, höhnisch angeblickt und sich den versammelten Makedonen zugewandt haben:\n\nAlexander befürchtete nun offenbar, von der Thronfolge ausgeschlossen zu werden. Schließlich floh er mit seiner Mutter über Epeiros nach Illyrien. Nach einem halben Jahr kehrte er nach Pella zurück, doch seine Thronfolge blieb weiterhin unsicher.\n\nPhilipp wurde im Sommer 336 v. Chr. in der alten Hauptstadt Aigai (auch bekannt als Vergina) während der Hochzeit seiner Tochter Kleopatra mit dem König Alexander von Epeiros von dem Leibgardisten Pausanias ermordet. Das Motiv des Täters scheint offensichtlich: Pausanias, den Freunde Alexanders sofort nach der Tat erschlugen, war ein Vertrauter Philipps gewesen und war von Attalos beleidigt worden; dabei fühlte er sich von Philipp ungerecht behandelt. Es gab aber bald Gerüchte, wonach Alexander als Drahtzieher an der Tat beteiligt gewesen war. Die Mutmaßungen über die Hintergründe des Mordes und über eine Verwicklung von Olympias und Alexander sind weitgehend spekulativ, auch wenn eine Mitwisserschaft nicht ausgeschlossen werden kann.\n\n\nRegierungsübernahme und Sicherung der Macht (336–335 v. Chr.).\nIm Jahre 336 v. Chr. folgte der zwanzigjährige Alexander seinem Vater auf den Thron. Dass es keinen nennenswerten Widerstand gab, ist offenbar Antipater zu verdanken, der das Heer dazu bewog, Alexander als König anzuerkennen. Schon in den ersten Tagen ließ er Mitglieder des Hofstaats exekutieren, die das Gerücht gestreut hatten, Alexander habe etwas mit der Ermordung seines Vaters zu tun gehabt. Als nächstes wandte er sich seinem Erzfeind Attalos zu, der sich auf der Flucht befand, jedoch von seinem Schwiegervater Parmenion getötet wurde. Sowohl Antipater als auch Parmenion standen deswegen lange in Alexanders besonderer Gunst und profitierten nicht unerheblich davon: Antipater blieb während des Asienfeldzugs als Reichsverweser in Makedonien, während Parmenion sich seine Unterstützung mit großem Einfluss im Heer vergelten ließ.\n\nNoch 336 ließ sich Alexander in Korinth die Gefolgschaft der griechischen Städte versichern. Die Völker in Thrakien und Illyrien versuchten jedoch, die Situation zu nutzen und die makedonische Herrschaft abzuwerfen. Alexander zog im Frühjahr 335 v.&nbsp;Chr. mit 15.000 Mann nach Norden ins heutige Bulgarien und Rumänien, überquerte die Donau und warf die thrakische Revolte nieder. Anschließend verfuhr er ebenso mit den Illyrern (siehe auch: Balkanfeldzug Alexanders des Großen).\n\nWährend Alexander im Norden kämpfte, beschlossen die Griechen im Süden, dass dies der Zeitpunkt sei, sich von Makedonien zu befreien. Ihr Wortführer war Demosthenes, der die Griechen davon zu überzeugen versuchte, dass Alexander in Illyrien gefallen und Makedonien herrscherlos sei. Als erste erhoben sich die Einwohner Thebens und vertrieben die makedonischen Besatzungssoldaten aus der Stadt.\n\nAlexander reagierte augenblicklich und marschierte direkt von seinem Illyrienfeldzug südwärts nach Theben. Die Phalanx seines Generals Perdikkas eroberte die Stadt, wo Alexander zur Bestrafung sämtliche Gebäude mit Ausnahme der Tempel und des Wohnhauses des Dichters Pindar zerstören ließ. Sechstausend Einwohner wurden getötet, die übrigen 30.000 wurden in die Sklaverei verkauft. Die Stadt Theben existierte nicht mehr und sollte erst zwanzig Jahre später wieder aufgebaut werden, aber nie mehr zur alten Bedeutung zurückfinden.\n\nAbgeschreckt von Alexanders Strafgericht brachen die anderen Städte Griechenlands ihre Revolte ab und ergaben sich. Von den Korinthern ließ sich Alexander von neuem die Gefolgschaft versichern und verschonte sie daraufhin, da er sie als Verbündete in seinem Persienfeldzug brauchte.\n\n\nBeginn des Persienfeldzugs (334–333 v. Chr.).\nDas Perserreich war zu Alexanders Zeit die größte Territorialmacht der Erde. Die Perserkönige hatten in den zurückliegenden Jahrhunderten die Levante, Mesopotamien, Ägypten und Kleinasien erobert und zwischen 492 und 479 v. Chr. mehrere Versuche unternommen, auch Griechenland zu unterwerfen (siehe Perserkriege). Aus Sicht von Griechen wie Isokrates ebenso wie der älteren Forschung war das Reich aber um 340 v. Chr. geschwächt und hatte seinen Zenit überschritten. In der neueren Forschung wird dies allerdings bestritten; so war den Persern wenige Jahre vor dem Alexanderzug die Rückeroberung des zwischenzeitlich abgefallenen Ägypten gelungen. Ob Persien für die Makedonen eine leichte Beute war, ist daher umstritten.\n\nAls sich Alexander 334 v.&nbsp;Chr. dem Perserreich zuwandte, wurde dies von Dareios III. aus dem Haus der Achämeniden beherrscht. Schon Alexanders Vater Philipp hatte Pläne für einen Angriff auf die Perser geschmiedet, angeblich, um Rache für die Invasion Griechenlands rund 150 Jahre zuvor zu nehmen, wobei es sich dabei eher um Propaganda handelte und machtpolitische Gründe den Ausschlag gegeben haben dürften. Eine Armee unter Parmenion, einem der fähigsten makedonischen Generäle, war bereits über den Hellespont nach Asien gegangen, wurde von den Persern aber zurückgeschlagen. Alexander überschritt den Hellespont im Mai 334 v. Chr. mit einer Armee aus etwa 35.000 Makedonen und Griechen, um in die Kämpfe einzugreifen, während rund 12.000 Makedonen unter Antipatros Makedonien und Griechenland sichern sollten.\n\nIn der Schlacht am Granikos (Mai 334 v. Chr.) kam es zur ersten Begegnung mit den persischen Streitkräften unter der Führung eines Kriegsrates der Satrapen. Der für die Perser kämpfende Grieche Memnon von Rhodos führte 20.000 griechische Söldner, doch konnte er sich im Kriegsrat mit einer defensiven Taktik nicht durchsetzen. Alexander errang auch aufgrund einer ungünstigen Aufstellung der Perser einen deutlichen Sieg. Memnon konnte mit einem Teil der Söldner entkommen. Dadurch war die Befreiung der Städte Ioniens möglich geworden, die Alexander als Motiv für seinen Feldzug genannt hatte. Nach dem Sieg ernannte Alexander eigene Statthalter für die bisherigen Satrapien und übernahm damit die politischen und wirtschaftlichen Strukturen der persischen Verwaltung Kleinasiens.\n\nIn Lydien zog Alexander kampflos in Sardes ein. Er weihte den örtlichen Tempel dem Zeus und nutzte die Reichtümer der Stadt, um seine Männer zu bezahlen. Dann zog er weiter nach Ephesos. Dort war kurz zuvor Memnon mit den Resten der Söldner vom Granikos hindurchgezogen und hatte Unruhen unter den städtischen Parteien entfacht. Alexander ließ die alten Institutionen wiederherstellen und regelte die Befugnisse des Tempels der Artemis. Nach einer Ruhe- und Planungspause brach der König mit dem Gros des Heeres nach Milet auf, der größten Stadt an der Westküste Kleinasiens. Der dortige Satrap kapitulierte als Einziger nicht, da ihm die Ankunft einer persischen Hilfsflotte von 400 Schiffen versprochen worden war. Da auch Alexander von dieser Flotte gehört hatte, wies er Nikanor, einen Bruder Parmenions, an, mit 160 Schiffen die Einfahrt zur Bucht von Milet zu versperren. Anschließend gelang ihm die Einnahme der Stadt (→ Belagerung von Milet).\n\nDie Perser, die immer noch unter dem Befehl Memnons standen (allerdings hatten Unstimmigkeiten im persischen Oberkommando einen effektiven Widerstand erschwert), sammelten sich nun in Halikarnassos, der Hauptstadt Kariens, und bereiteten die Stadt auf eine Belagerung vor. Die Kämpfe waren für Alexander sehr verlustreich. Zwischenzeitlich handelte er einen Waffenstillstand aus, um die makedonischen Gefallenen zu bergen – etwas, was er nie zuvor getan hatte und nie wieder tun sollte. Als er letztlich die Mauern durchbrach, entkam Memnon mit dem Großteil seiner Soldaten auf Schiffen aus der fallenden Stadt (→ Belagerung von Halikarnassos). Indem Alexander der karischen Satrapentochter Ada die Herrschaft über Halikarnassos versprach, sicherte er sich das Bündnis mit dem Volk Kariens. Manche Quellen sprechen davon, dass Ada Alexander adoptierte. Hier zeigte Alexander erstmals seine Taktik, Großzügigkeit gegenüber besiegten Völkern walten zu lassen, um sie nicht gegen die Makedonen aufzubringen.\n\nDas ursprüngliche Ziel des Persienfeldzugs, die Eroberung der Westküste Kleinasiens, war hiermit erreicht. Dennoch beschloss Alexander, die Expedition fortzusetzen. Entlang der Küsten Lykiens und Pamphyliens traf die makedonisch-griechische Streitmacht auf keinerlei nennenswerten Widerstand. Eine Stadt nach der anderen ergab sich kampflos. Alexander ernannte seinen Freund Nearchos zum Statthalter von Lykien und Pamphylien.\n\nIm Winter 334/333 v.&nbsp;Chr. eroberte Alexander das anatolische Binnenland. Er stieß vom Süden vor, sein General Parmenion von Sardes im Westen. Die beiden Armeen trafen sich in Gordion, der Hauptstadt der persischen Satrapie Phrygien. Hier soll Alexander der Große der Legende nach den Gordischen Knoten mit seinem Schwert durchschlagen haben, über den ein Orakel prophezeit hatte, nur derjenige, der diesen Knoten löse, könne die Herrschaft über Asien erringen. Es gibt aber auch die Version, dass Alexander mit der Breitseite des Schwertes auf die Wagendeichsel schlug, so dass der Druck den Knoten auseinanderriss.\n\nDie Makedonen blieben einige Zeit in Gordion, um Nachschub an Männern und die Einfuhr der Ernte abzuwarten. Während dieser Zeit starb Memnon, der Befehlshaber der persischen Armee, im August 333 v.&nbsp;Chr. an einer Krankheit. Zu seinem Nachfolger wurde Pharnabazos ernannt, und da sich die Perser bereits wieder formierten, brach Alexander erneut auf. In Gordion ließ er seinen General Antigonos als Statthalter Phrygiens zurück und übertrug ihm die Aufgabe, den Norden Anatoliens zu unterwerfen und die Nachschubwege zu sichern.\n\nDurch Kappadokien marschierte Alexanders Heer nach Kilikien. Dort nahm er nach einem kurzen Gefecht die Hauptstadt Tarsos ein, wo er bis zum Oktober blieb.\n\n\nSchlacht bei Issos (333 v. Chr.).\nIn Tarsos erfuhr Alexander, dass Dareios III. die Bedrohung endlich ernst genug nahm, um selbst ein Heer aus dem persischen Kernland nach Westen zu führen. Plutarch zufolge war dieses persische Heer 600.000 Mann stark – eine Angabe, die sicherlich maßlos übertrieben ist: Der berühmte Althistoriker Karl Julius Beloch, der den Quellen immer sehr skeptisch gegenüberstand, schätzte die tatsächliche Zahl der Perser auf höchstens 100.000, die Stärke des makedonischen Heeres dagegen auf ca. 25–30.000 Mann.\n\nDareios gelang es, Alexanders Armee im Norden zu umgehen und Issos zu besetzen, wodurch er die Nachschubwege blockierte. Auch ließ Dareios die in Issos zurückgebliebenen Verwundeten töten. In der Schlacht bei Issos trafen die Armeen im Kampf aufeinander, bis Dareios aufgrund der großen Verluste der Perser vom Schlachtfeld floh. Die Makedonen beklagten 450 Tote und 4000 Verwundete. Unbekannt sind die persischen Verluste, sie dürften aber weit höher gewesen sein. Insgesamt hatte die persische Führung während der Schlacht mehrere Fehler begangen, angefangen bei der Aufstellung – man hatte auf die Umgruppierungen Alexanders nicht reagiert. Auch als Symbol kam der Schlacht große Bedeutung zu: Dareios hatte sich als seinem Gegner nicht gewachsen gezeigt.\n\nZur Sicherung des Lagers der Perser sandte Alexander seinen General Parmenion nach Damaskus. Neben dem reichen Kriegsschatz befanden sich hier auch mehrere Mitglieder der königlichen Familie. Zu den Gefangenen, die in die Hände der Makedonen fielen, gehörten die Mutter des Dareios, seine Frau Stateira, ein fünfjähriger Sohn und zwei Töchter. Alexander behandelte sie mit Respekt. Außerdem wurde Barsine gefangen genommen, die Witwe des Memnon. Es kam zu einer Liebesaffäre zwischen Alexander und Barsine, aus der später ein Sohn hervorgehen sollte, der Herakles genannt wurde.\n\nSchon bald bat Dareios Alexander um den Abschluss eines Freundschaftsvertrags und die Freilassung seiner Familie. Alexander antwortete, Dareios solle zu ihm kommen und Alexander als „König von Asien“ anerkennen, dann würde seine Bitte erfüllt; andernfalls solle er sich auf den Kampf vorbereiten.\n\nNach der Schlacht gründete Alexander die erste Stadt in Asien, die er nach sich benannte: Alexandretta, das heutige İskenderun. Hier siedelte er die 4000 Verwundeten der Schlacht an.\n\n\nLage nach der Schlacht von Issos.\nDer Ausgang der Schlacht überraschte die antike Welt. Die Erwartungen der Herrscher von Karthago, in Italien, Sizilien, von Sparta bis Zypern, die Kalkulationen der Handelsherren im westlichen Mittelmeerraum, in Athen, auf Delos und in Phönizien erfüllten sich nicht: „… statt der erwarteten Siegesnachricht aus Kilikien kam die von der gänzlichen Niederlage des Großkönigs, von der völligen Vernichtung des Perserheeres.“\n\nAuch die Delegationen aus Athen, Sparta und Theben, die im Hauptquartier des Großkönigs in Damaskus den Verlauf der Feldzüge verfolgten, wurden von Alexanders Feldherrn Parmenion gefangen gesetzt.\nAlexander selbst widerstand der Versuchung, den Krieg durch einen Marsch nach Babylon rasch zu entscheiden, doch hatte er es nicht einfach, seine Befehlshaber und Gefährten von einer Defensivstrategie zu überzeugen.\n\nNach wie vor beherrschte die persische Flotte das östliche Mittelmeer – sie verfügte zwar über keine Häfen mehr in Kleinasien, jedoch nach wie vor in Phönizien. Durch die Münzgeldtribute hier waren die finanziellen Mittel der Perser noch wenig eingeschränkt, und auch Ägypten stand ihnen noch als logistische und militärische Basis zur Verfügung.\nDie kommenden Winterstürme ließen zwar keine Flottenunternehmungen mehr erwarten und damit auch keine Gefahr einer raschen Erhebung der Griechen gegen Makedonien – insbesondere des Spartanerkönigs Agis IV. –, doch kam es nun auch auf das Verhalten der phönizischen Geschwader an, die einen Großteil der persischen Flotte stellten. Zwar verblieben sie in dieser Jahreszeit noch in der Fremde, doch nahm Alexander an, dass er diese Kontingente durch eine sofortige Besetzung ihrer Heimatstädte zumindest neutralisieren könne.\n„Auch die kyprischen Könige glaubten, für ihre Insel fürchten zu müssen, sobald die phönikische Küste in Alexanders Gewalt war.“\nNach einer Besetzung Phöniziens und Ägyptens könne dann ein Feldzug nach Asien von einer gesicherten Basis aus geführt werden, obwohl die Perser natürlich auch Zeit für neue Rüstungen gewannen. Die Versammlung stimmte Alexanders Plan zu.\n\nDie Schlacht von Issos hatte noch keine grundsätzliche Entscheidung gebracht:\nEntgegen den Erwartungen wurde das makedonische Heer nicht vernichtet, und Alexander besaß mit der persischen Kriegskasse in Damaskus die Mittel zur Fortführung des Feldzuges. Eine Entscheidung des Krieges war dadurch nicht bewirkt worden.\nEingezogen wurden in Damaskus „2600 Talente in Münzgeld und 500 Pfund Silber“, die „(ausreichten), alle Soldschulden der Armee und Sold für etwa sechs weitere Monate zu bezahlen …“\n\n\nBelagerung von Tyros und das zweite Angebot des Dareios (332 v. Chr.).\nWährend die Städte in der nördlichen Hälfte Phöniziens – Marathos, Byblos, Arados, Tripolis und Sidon – sich dem Makedonen bereitwillig ergaben, war die dominierende Handelsmetropole Tyros allenfalls zu einem Vergleich bereit. Sie baute dabei auf ihre Insellage knapp vor der Küste, auf ihre vor Ort verfügbare eigene Flotte und die Unterstützung ihrer mächtigen Tochterstadt Karthago. Nachdem Alexander der Zutritt zur Stadt verwehrt worden war – sein Prüfstein war das Verlangen nach einem Opfer im Tempel des Stadtgottes Melkart, des tyrischen Herakles –, brach der König die Verhandlungen ab. Er beschloss, Tyros um jeden Preis einzunehmen, denn er plante schon den Vorstoß nach Ägypten und wollte eine feindliche Stadt, die sowohl mit den Persern als auch mit rebellischen Kräften in Griechenland kooperieren würde, nicht unbezwungen in seinem Rücken lassen. Eine von Arrian überlieferte angebliche Rede Alexanders vor seinen Offizieren, in der die strategischen Überlegungen erläutert werden, ist allerdings eine literarische Fiktion, die auf der Kenntnis des späteren Verlaufs des Feldzugs beruht. Vor dem Beginn der Belagerung bot Alexander den Tyrern Schonung an, falls sie kapitulierten. Sie töteten jedoch seine Unterhändler und warfen die Leichen von den Stadtmauern. Damit war der Weg zu einer Einigung endgültig versperrt.\n\nOhne Flotte blieb nur die Möglichkeit eines Dammbaues durch das zumeist seichte Gewässer, das die vorgelagerte Inselstadt von der Küste trennte, und der Versuch, mit Belagerungsmaschinen Teile der Mauern zu zerstören. Die Finanzierung dieser aufwendigen Methode, die eine entwickelte Technik und die dafür entsprechenden Materialien und Fachkräfte erforderte, konnte Alexander durch die Beute aus dem persischen Hauptquartier in Damaskus bewerkstelligen.\n\nEin erster Dammbau wurde von den Tyrern erfolgreich bekämpft, es gelang ihnen bei stürmischem Wetter mit einem Brander die zwei Belagerungstürme an der Spitze des Dammes zu entzünden und durch Begleitschiffe mit Geschützen jeden Löschversuch zu vereiteln. Der Sturm riss zudem den vorderen Teil des Dammes weg.\n\nDer Vorfall löste im makedonischen Heer Entmutigung aus, zumal wieder Gesandte des Dareios eintrafen und ein neues Friedensangebot des Großkönigs überbrachten, das Alexander „den Besitz des Landes diesseits des Euphrat“, 10.000 Talente Lösegeld für seine gefangene Gemahlin und die Hand seiner Tochter anbot. In diese Zeit fiel auch die – vermutlich von Kallisthenes übermittelte – Reaktion des Befehlshabers Parmenion: Wäre er Alexander, so würde er akzeptieren. Alexander entgegnete, das würde er auch tun, wenn er Parmenion wäre. Alexander ließ Dareios mitteilen, er, Alexander, werde sich nehmen, was er wolle; wenn Dareios etwas von ihm erbitten wolle, solle er zu ihm kommen.\n\nDer Damm wurde in größerer Breite wiederhergestellt und neue Türme gebaut.\nIn der Zwischenzeit – nach den Winterstürmen – trafen auch die phönizischen Flottenkontingente und die Geschwader der Könige von Zypern in ihren Heimathäfen ein und standen nun Alexander zur Verfügung; insgesamt 250 Schiffe, darunter auch Vier- und Fünfruderer.\n\nDiese Bundesgenossenschaft lag auch in der Feindschaft der kleineren Städte Phöniziens gegen Tyros begründet: Die Metropole hatte zwanzig Jahre zuvor zwar einen Aufstand unter Führung von Sidon gegen die Perser befürwortet und Hilfe zugesagt, dann jedoch den Verlauf der Auseinandersetzungen abgewartet und war von den Persern für diese Haltung belohnt worden. Nach der Niederschlagung der Erhebung und der Zerstörung von Sidon errang Tyros die Vorherrschaft unter den phönizischen Handelsstädten.\n\nWährend die neu gewonnene Flotte ausgerüstet wurde, unternahm Alexander eine Expedition durch das küstennahe Gebirge des Antilibanon, um die Festungen von Gebirgsstämmen zu bezwingen, den Nachschub (Holz für den Maschinenbau) und die Verbindung nach Damaskus zu sichern.\n\nDie Karthager konnten den Tyrern nicht helfen, da sie sich im Krieg mit Syrakus befanden. Nach weiteren wechselvollen Kämpfen um die Stadtmauern und zur See, die die Tyrer immer mehr Schiffe kosteten, war die Zeit zum Sturmangriff reif. Alexander beschloss einen kombinierten Land- und Seeangriff. Auf der durch den Damm erreichbaren Seite gelang es, Breschen in die Mauern zu schlagen und ein Landeunternehmen durchzuführen, die phönizischen Schiffe sprengten die Sperrketten im Südhafen und bohrten die dort liegenden Schiffe in den Grund, die zyprische Flotte verfuhr ebenso im Nordhafen – dort gelang es den Truppen, zusätzlich in die Stadt einzudringen. Die überlieferte Zahl von 8000 Gefallenen der Stadt soll sich auf die gesamte Belagerungszeit beziehen. Ob die anschließende angebliche Kreuzigung von 2000 Kämpfern den Tatsachen entspricht, ist umstritten. Im Vorfeld des letzten Angriffes ließ Alexander Schiffe der Karthager und seiner verbündeten Phönizier zur Evakuierung der Bevölkerung passieren. In Heiligtümer oder Tempel Geflüchtete wurden verschont.\n\nZahlreiche Einwohner – die überlieferte Zahl von 30.000 gilt allerdings als stark übertrieben – wurden in die Sklaverei verkauft. Das war in der Antike eine gängige Praxis, um die Kriegskassen aufzufüllen. Alexander soll allerdings sehr selten zu diesem Mittel gegriffen haben, da er die Bevölkerung für sich gewinnen wollte, denn er konnte sich eine ständige Bedrohung durch Aufständische in seinem kaum durchgängig besetzbaren Hinterland nicht leisten.\n\nTyros wurde wieder aufgebaut und neu besiedelt, um unter makedonischer Hoheit die beherrschende Position in Phönizien zu sichern. Die Nachricht von diesem mit modernster Kriegstechnik errungenen Sieg – die Belagerungstürme sollen eine Höhe von 45 Metern erreicht haben – machte in der antiken Welt weit über die betroffene Region hinaus einen starken Eindruck.\n\n\nEroberung von Gaza.\nAlexander, der während der Belagerung auch die Verwaltung und Logistik in den neu gewonnenen Gebieten ordnete, „brach etwa Anfang September 332 von Tyros auf.“ Die Städte und Stämme im südlichen Syrien ergaben sich bis auf die Hafenstadt Gaza.\n\nDie Stadt war seit Jahrhunderten der Hauptumschlagplatz des Gewürzhandels. Mit einer Eroberung der Stadt konnte Alexander einen der lukrativsten Handelsbereiche zwischen Ost und West unter seine Kontrolle bringen, doch standen den Makedonen damit nicht nur Perser, sondern auch arabische Söldnertruppen gegenüber. Mit entsprechender Härte wurde der Kampf geführt.\n\nEinen unmittelbaren Gewinn konnte sich Alexander von einer Eroberung nicht versprechen, denn die Gewürzhandelsgeschäfte des Jahres waren abgeschlossen, da „die Route nur einmal im Jahr befahren wurde.“ Wetterverhältnisse und „Orientierungsschwächen beschränkten die Aktivitäten mediterraner Seefahrt auf das halbe Jahr zwischen Mai und Oktober, in dem das Wetter in der Regel verläßlich gut war. […] Faktisch lag der Zeitpunkt Mitte August (Hesiod, 700 v. Chr.), denn es stand auch noch die Rückreise an.“ Organisiert war diese Fahrt bis in die Spätantike als riesiges „Kauffahrtgeschwader“ zuerst entlang der östlichen Küsten – vor allem Kornfrachter, Sklaven- und Baumaterial-Transporten sowie Postschiffen und anderen, die dann übers Meer von Kriegsschiffen begleitet wurden. Durch die Belagerung von Tyros waren die Handelsunternehmen 332 v. Chr. schon stark beeinträchtigt worden.\n\nAlexander nahm sofort den Hafen von Gaza zum Antransport der zerlegten Belagerungsmaschinen in Beschlag. Die Stadt selbst lag nahe dem Meer auf einem flachen Hügel. Gaza war auch der letzte freie Ankerplatz für die persische Flotte in Syrien und somit auch an der kompletten östlichen Mittelmeerküste. Die Flotte war mittlerweile in Auflösung begriffen, da die griechischen Kontingente nun ebenfalls – klimabedingt – im Herbst in ihre Heimathäfen zurück segelten.\n\nMit erneut hohem Aufwand schütteten die Makedonen einen Damm zur Südseite der Stadt auf, der danach mit weiteren, konzentrisch angelegten Dämmen ergänzt wurde. Die Kämpfe – vor allem mit den arabischen Söldnern – wurden als „wild“ bezeichnet, Alexander wurde zweimal verwundet; durch einen Messerstich und – gefährlicher – mit einem Katapultpfeil, der durch den Panzer in die Schulter drang. Nach zwei Monaten und dem vierten Ansturm fiel die Stadt, um die 10.000 Verteidiger sollen umgekommen sein, Frauen und Kinder wurden als Sklaven verkauft.\n\nDass der Kommandant Batis wie Hektor durch Achilles vor Troja um die Stadt geschleift worden sein soll, wird angezweifelt. „Alexander zog die Bevölkerung der umliegenden philistäischen und arabischen Ortschaften in die Stadt; eine dauernde Besatzung machte sie zu einem Waffenplatz, der für Syrien und für Ägypten gleich wichtig war.“\n\nEs wird davon ausgegangen, dass der Gewürztransport nach Gaza danach in der „Felsenstadt“ Petra – der davor liegenden Station der Weihrauchstraße – angehalten wurde. Petra war „zentrales Weihrauchlager“, da die Stadt in einem Talkessel gewaltige Lagerhallen (Höhlen) besaß. „In Petra saßen die Ökonomen, die kontrollierten, was sie zu welchem Preis an die mediterranen Küsten bringen wollten.“ Für 332 war das Geschäft allerdings schon gelaufen.\n\nDen jahreszeitlichen Bedingungen zufolge kehrten im Herbst auch die Kauffahrtsflotten zurück und trafen in Phönizien überall in Häfen ein, die von den Makedonen kontrolliert wurden. Die Auflösung der persischen Kriegsflotte im Herbst war ebenfalls eine Routineangelegenheit, doch war es allen Beteiligten klar, dass die Kontingente auf Grund der makedonischen Besetzung sämtlicher Festlandshäfen im östlichen Mittelmeer im nächsten Frühjahr nicht wieder unter persischem Kommando zusammengeführt werden würden.\n\n\nSeekrieg (332 v. Chr.).\nWährend Alexander mit dem Heer 332 v. Chr. den größten Teil des Jahres mit Belagerungen zur Vervollständigung seiner Blockade der persischen Seemacht verbrachte – und dabei die phönizischen Hafenstädte und ihren Handel unter seine Kontrolle nahm –, war die Flotte der Perser durch den bereits im Frühjahr erfolgten Abzug der phönikischen und kyprischen Kontingente geschwächt und verhielt sich defensiv.\n\nDie Admirale Pharnabazos und Autophradates versuchten – meist mit Hilfe begünstigter oder eingesetzter Machthaber – die wichtigsten Inseln unter ihrer Kontrolle zu behalten. In Griechenland, das Alexanders Statthalter Antipater bis auf die Peloponnes fest im Griff hatte, rührte sich kein Widerstand.\n\nLediglich der Spartanerkönig Agis III. setzte noch auf die persische Karte und hatte Kreta durch seinen Bruder und Mitregenten Agesilaos besetzen lassen.\n\nDoch schon im Vorjahr, noch während des Aufenthalts in Gordion 333 v. Chr. hatte Alexander „Amphoteros, den Bruder des Orestiden Krateros“ beauftragt, „‚in Übereinstimmung mit den Abmachungen des Bündnisses‘ eine neue griechische Flotte auszurüsten.“ Dank „der erbeuteten Schätze aus Sardis“ gelangen die Anfänge dazu und nach dem Sieg bei Issos und dem darauf folgenden Winter, der keine Flottenunternehmungen zuließ, stand Alexanders neue Flotte im Frühjahr 332 v. Chr. bereit.\n\nNun konnten die makedonischen Nauarchen Hegelochos und Amphoteros ihrerseits systematisch die Inseln besetzen – von Tenedos und Chios (wo der persische Admiral Pharnabazos mit der Besatzung von 15 Trieren in Gefangenschaft geriet) – bis nach Kos und schließlich Lesbos. Dort handelte der athenische Söldnerführer Chares mit zweitausend Mann freien Abzug aus und begab sich nach Tainaron, dem Hafen und Söldnermarkt südlich von Sparta.\n\nAmphoteros unterwarf zuletzt noch die kretischen Stützpunkte, während Hegelochos bereits nach Ägypten steuerte, „um selbst die Meldung vom Ausgang des Kampfes gegen die persische Seemacht zu überbringen, zugleich die Gefangenen abzuliefern […] So war mit dem Ausgang des Jahres 332 der letzte Rest einer persischen Seemacht, die das makedonische Heer im Rücken zu gefährden und dessen Bewegungen zu hindern vermocht hätte, vernichtet.“\n\n\nBesetzung Ägyptens (332–331 v. Chr.).\nNach der Eroberung von Gaza machte sich Alexander mit einem Teil seines Heeres auf den Weg nach Ägypten.\n\nÄgypten war in den vorangegangenen sieben Jahrzehnten mehrfach von den Persern angegriffen und besetzt worden und ging ihnen regelmäßig durch Aufstände wieder verloren. Erst seit drei Jahren war es wieder in der Hand des Großkönigs, doch „Ägypten war von Truppen entblößt, weil der Satrap Sabakes mit einem großen Aufgebot nach Issos gekommen und selbst dort gefallen war. […] Mazakes, vom Großkönig [..] zum (neuen) Satrapen ernannt, konnte nicht an Widerstand denken.“ Er übergab unter Auslieferung von 800 Talenten für freies Geleit die Grenzfestung Pelusion.\n\nEin Teil der makedonischen Flotte segelte nun den Nil aufwärts zur Hauptstadt Memphis während sich Alexander mit den Truppen auf dem Landmarsch über Heliopolis dorthin begab. In Memphis opferte Alexander (wie er auch den Göttern anderer eroberter Länder Opfer darbrachte) dem ägyptischen Gott Apis, anstatt ihn zu verachten wie der persische Großkönig Artaxerxes III., der den heiligen Stier des Gottes töten ließ. „Als Gegengabe scheint Alexander als Pharao des Oberen und Unteren Ägyptens gekrönt worden zu sein, wenngleich diese Ehrung nur in dem „frei erfundenen“ \"Alexander-Roman\" erwähnt wird.“ „Die Krönung kann nicht auf einen Monat genau datiert werden, bestätigt wird sie aber durch die Pharaonentitel, die ihm in ägyptischen Tempelinschriften zugeschrieben sind.“ Der Verlag veröffentlichte dazu das Foto eines Reliefs im Amun-Tempel von Luxor.\n\nAlexander zog danach am westlichen Nil entlang nordwärts und gründete im Januar 331 v. Chr. an der Mittelmeerküste Alexandria, die bedeutendste all seiner Stadtgründungen.\n\nIm März zog Alexander von Paraetonium aus 400&nbsp;km südwestwärts durch die Wüste zum Orakel von Siwa, einem dem Gott Amun geweihten Tempel. Was er dort an Botschaften empfing, ist unbekannt. Antike Quellen berichten, Alexander habe dort erfahren, dass er der Sohn des Zeus sei; so soll ihn der oberste Priester als „Sohn des Zeus“ begrüßt haben. Jedoch hatte Alexander sich schon vorher als Sohn des Zeus bezeichnet.\nVon Siwa kehrte Alexander nach Memphis zurück, verweilte dort einige Wochen und führte seine Truppen dann zurück nach Palästina.\n\n\nEroberung des persischen Kernlands (331–330 v. Chr.).\nIm Mai 331 v. Chr. kehrte Alexander nach Tyros zurück. Er befahl hier den Wiederaufbau der Stadt, die er mit befreundeten Phöniziern wieder besiedeln ließ. 15.000 zusätzliche Soldaten waren im Frühling aus Makedonien entsandt worden, und bei Tyros trafen sie im Juli mit Alexander zusammen. Seine Armee bestand nun aus 40.000 Fußsoldaten und 7000 Reitern.\n\nAlexander zog ostwärts durch Syrien und überquerte den Euphrat. Sein Plan mag gewesen sein, von hier aus südwärts nach Babylon zu ziehen, doch eine Armee unter dem persischen Satrapen Mazaeus verstellte den Weg. Alexander vermied die Schlacht, die ihn viele Männer gekostet hätte, und zog stattdessen nordwärts. Derweil zog Dareios selbst eine neue große Streitmacht in Assyrien zusammen, und dieses Heer war es, das Alexander treffen wollte. Im September 331 v. Chr. überquerte das Heer den Tigris.\nAm 20. September, unmittelbar vor der Schlacht, kam es zu einer Mondfinsternis, die die Perser verunsicherte und von ihnen als schlechtes Omen gedeutet wurde. Das Heer Alexanders lagerte 11&nbsp;km von der persischen Armee entfernt bei einem Dorf namens Gaugamela, weshalb die folgende Schlacht als Schlacht von Gaugamela bekannt wurde. Am 1. Oktober kam es zum Kampf. Wenngleich das Heer des Dareios auch diesmal den Truppen Alexanders zahlenmäßig weit überlegen war, siegte abermals Alexander. Er vermochte aber nicht, Dareios selbst zu töten oder gefangen zu nehmen. Obwohl dieser damit erneut entkommen war, war seine Armee praktisch vernichtet. Alexander dagegen hatte nun die Herrschaft über die Satrapie Babylonien gewonnen und konnte ungehindert ins reiche Babylon einziehen. Mazaeus, der sich nach der Schlacht von Gaugamela nach Babylon zurückgezogen hatte, übergab die Stadt an Alexander, der sie durch das Ischtar-Tor betrat und sich zum „König von Asien“ ausrufen ließ.\n\nWährend die Griechen die Völker Asiens zuvor als Barbaren verachtet hatten, sah Alexander sie mit anderen Augen. Fasziniert von der Pracht Babylons befahl er die Schonung aller Bauwerke. Alexander verzieh dem persischen Satrapen Mazaeus und ernannte ihn zu seinem Statthalter in Babylon.\n\nNach fünfwöchigem Aufenthalt zog Alexander weiter ostwärts, um die großen persischen Städte im Kernland anzugreifen. Susa ergab sich kampflos. Im Januar 330 v. Chr. erreichten die Makedonen die persische Hauptstadt Persepolis. Zahlreiche Einwohner begingen vor seinem Einzug Selbstmord oder flohen. Die ältere Meinung, Alexander habe die Stadt plündern und den Königspalast niederbrennen lassen, ist inzwischen von der jüngeren Quellenkritik relativiert worden. Archäologische Funde bestätigen, dass lediglich die Gebäude, die Xerxes I. errichtet hatte, brannten, was die Darstellung Arrians wahrscheinlicher macht.\n\n\nVerfolgung und Tod des Dareios (330 v. Chr.).\nZwar war Persien nun in Alexanders Hand, doch König Dareios III. war noch immer am Leben und auf der Flucht. Da Alexander mitgeteilt worden war, dass Dareios sich in Medien aufhalte, folgte er seiner Spur im Juni nach Nordwesten nach Ekbatana. Doch auch Dareios’ Anhängerschaft hatte jetzt keine Hoffnung mehr, Persien zurückzugewinnen. Die Vollkommenheit der Niederlage ließ nur die Möglichkeit zu, sich zu ergeben oder zeitlebens zusammen mit Dareios zu fliehen. Bisthanes, ein Mitglied der Königsfamilie, entschied sich, in Ekbatana zu bleiben, wo er Alexander empfing und ihm die Stadt übergab. Alexander zeigte sich wiederum großzügig und ernannte einen Perser zu seinem Statthalter in Medien. In Ekbatana entließ Alexander auch die griechischen Verbündeten und die thessalischen Reiter, was als Zeichen zu verstehen war, dass der vom Korinthischen Bund beschlossene „Rachefeldzug“ damit beendet war. Teile des Bundesheeres wurden jedoch von Alexander als Söldner angeworben.\n\nDareios setzte inzwischen seine Flucht fort. Er hoffte, Zuflucht in Baktrien zu erhalten, wo ein Verwandter namens Bessos Satrap war. Bessos aber setzte Dareios gefangen und schickte einen Unterhändler zu Alexander. Er bot ihm an, Dareios an die Makedonen zu übergeben, wenn im Gegenzug Baktrien frei bliebe. Alexander ging nicht auf die Verhandlungen ein und setzte die Verfolgung fort. Bessos tötete seine Geisel im Juli und floh seinerseits. Die Leiche des Dareios wurde von Alexander nach Persepolis gebracht und dort feierlich beigesetzt.\n\n\nVerfolgung des Bessos (330–329 v. Chr.).\nIn der Zwischenzeit hatte Alexander erkannt, dass er zur Sicherung der Herrschaft über das Perserreich die Unterstützung der persischen Adligen brauchte. Er nutzte Dareios’ Ermordung daher, die Perser zu einem Rachezug gegen Bessos aufzurufen, der sich nun den Namen Artaxerxes gegeben hatte und sich Großkönig von Persien nannte. Die Soldaten waren wenig begeistert davon, dass sie den Tod ihres Erzfeindes vergelten und zudem gemeinsam mit Persern kämpfen sollten. Außerdem war ihnen das Land im Nordosten vollkommen unbekannt. Die dortigen Provinzen Baktrien und Sogdien lagen in etwa auf den Territorien der heutigen Staaten Afghanistan, Usbekistan und Turkmenistan.\n\nIm August 330 v. Chr. brach Alexander zu einem neuen Feldzug auf und eroberte zunächst Hyrkanien, die persische Satrapie an der Südküste des Kaspischen Meeres. Unter jenen, die mit Alexander kämpften, war Oxyartes, ein Bruder des Dareios. Statt von Hyrkanien den direkten Weg nach Baktrien zu wählen, ging Alexander über Aria, dessen Satrap Satibarzanes an Dareios’ Gefangennahme beteiligt gewesen war. Alexander eroberte die Hauptstadt Artacoana, verkaufte die Einwohner in die Sklaverei und benannte die Stadt in Alexandreia um; der heutige Name der Stadt ist Herat.\n\nAuf seinem weiteren Weg kam es zu einem Zwischenfall, als Philotas, der Sohn des Parmenion, beschuldigt wurde, einen Anschlag auf Alexanders Leben unternommen zu haben. Ob dieser Versuch wirklich unternommen worden war, ist unklar. Vielleicht diente die Affäre Alexander bloß als Vorwand, sich Parmenions zu entledigen, der zum Wortführer seiner Kritiker avanciert war. Sie missbilligten Alexanders Neigung, die Perser zu ehren und ihre Gewänder zu tragen, und sahen dies als Anbiederung an ein barbarisches Volk an. Philotas wurde an Ort und Stelle mit einem Speer getötet. Ein Kurier wurde dann zu den Adjutanten des in Ekbatana gebliebenen Parmenion gesandt. Sie töteten Parmenion auf Alexanders Befehl.\n\nNach beschwerlicher Reise entlang des Flusses Tarnak erreichte Alexander im April 329 das Zentrum des heutigen Afghanistan und gründete Alexandria am Hindukusch (heute Chârikâr). Von hier aus wollte Alexander das Gebirge überschreiten und auf diesem Wege in Baktrien einfallen. Einer Legende zufolge fand man hier den Berg, an den der Titan Prometheus gekettet worden war.\n\nAls die Nachricht nach Baktrien gelangte, dass Alexander dabei war, den Hindukusch zu übersteigen, fürchteten die Einwohner von Baktra (heute Balch) die Bestrafung ihrer Stadt und vertrieben Bessos. Die beschwerliche Überquerung des Gebirges hatte die Soldaten indessen gezwungen, manche ihrer Lasttiere zu schlachten. Als sie erschöpft in Baktrien ankamen, wurde das Land ihnen kampflos übergeben. Alexander ernannte seinen persischen Vertrauten Artabazos, den Vater der Barsine, zum Satrapen.\n\nAlexander hielt sich nicht lange in Baktra auf und folgte weiterhin Bessos, der nordwärts zum Oxus (Amudarja) geflohen war. Der 75&nbsp;km lange Marsch durch wasserlose Wüste wurde vielen zum Verhängnis. Bessos hatte inzwischen alle Schiffe zerstören lassen, mit denen man den Amudarja hätte überqueren können. Die Makedonen brauchten fünf Tage, um genügend Flöße für die Überquerung des Flusses anzufertigen. Dann setzten sie in die Satrapie Sogdien im heutigen Turkmenistan über.\n\nDie Begleiter des Bessos wollten nun nicht länger fliehen. Sie meuterten gegen ihn, nahmen ihn gefangen und händigten ihn an Alexander aus. Dieser zeigte sich gnadenlos und ließ Bessos die Nase und die Ohren abschneiden. Anschließend übergab Alexander den Verstümmelten an Dareios’ Bruder Oxyartes, damit er ihn nach Medien an den Ort brächte, wo Dareios ermordet worden war. Dort wurde Bessos gekreuzigt.\n\nAlexander ging indessen weiter nach Norden und erreichte die sogdische Hauptstadt Marakanda (heute Samarkand). Alle Satrapien des Perserreichs unterstanden nun Alexander, und niemand außer ihm selbst erhob mehr Anspruch auf den Königstitel über Persien.\n\n\nAlexander in Sogdien (329–327 v. Chr.).\nNach der Einnahme von Marakanda zog Alexander noch weiter bis zum Syrdarja und gründete dort im Mai 329 v. Chr. die Stadt Alexandria Eschatê („das entfernteste Alexandria“), das heutige Chudschand in Tadschikistan. Etwa gleichzeitig erhob sich die Bevölkerung Sogdiens gegen ihn. Anführer der Rebellion, die Alexander erhebliche Schwierigkeiten bereitete, war ein Mann namens Spitamenes, der zuvor Bessos verraten und an Alexander übergeben hatte. Die Sogdier, die Alexander zunächst begrüßt hatten, nun jedoch sahen, dass eine Fremdherrschaft durch eine andere ersetzt wurde, machten die makedonischen Besatzungen nieder. Alexander zog Truppen zusammen und marschierte von einer rebellischen Stadt zur anderen, belagerte sieben von ihnen und tötete anschließend sämtliche männlichen Einwohner, wohl um ein abschreckendes Exempel zu statuieren. In der Zwischenzeit eroberte Spitamenes Marakanda zurück, doch Alexander gewann die Stadt erneut, wobei Spitamenes allerdings entkam. Da das Heer geschwächt und stark reduziert war, musste Alexander von der Verfolgung ablassen. Im Zorn brannte er Dörfer und Felder jener Bauern nieder, die die sogdische Revolte unterstützt hatten. Für den Winter 329/328 v. Chr. zog er sich nach Baktra zurück und erwartete neue Truppen, die bald darauf aus dem Westen eintrafen und bitter benötigt wurden.\n\nIm Frühling 328 v. Chr. kehrte Alexander nach Sogdien zurück. Den Quellen zufolge gründete er am Amudarja ein weiteres Alexandria, das vielleicht mit der heutigen Siedlung Ai Khanoum identisch ist. Der Kampf gegen die sogdischen Rebellen dauerte das ganze Jahr. Erst Monate später zeigte sich, dass die Anhänger des Spitamenes ihren Befehlshaber zu verlassen begannen. Das Haupt des Rebellenführers wurde Alexander schließlich im Dezember 328 v. Chr. überbracht.\n\nWährend der Sieg gefeiert wurde, kam es zu einem Streit zwischen Alexander und seinem General Kleitos. Kleitos, der altmakedonisch gesinnt war, sollte demnächst nach Baktrien aufbrechen. Grund war vermutlich sein Alter, aber Kleitos sah dies als Herabsetzung an. Es ist auch möglich, dass Kleitos bei dieser Gelegenheit Kritik an der Proskynese, einem von Alexander übernommenen persischen Hofritual, geübt hat. Die Streitenden waren zu diesem Zeitpunkt betrunken, und Kleitos hatte Alexanders Vater Philipp zu loben begonnen. Hierdurch fühlte sich Alexander so beleidigt, dass es zum Streit kam, in dessen Verlauf Alexander vergeblich nach seinen Waffen suchte, da sie vorsichtshalber von einem Leibwächter beiseitegelegt worden waren. Alexander, der möglicherweise Verrat befürchtete, rief in höchster Erregung auf Makedonisch nach einer Lanze, entriss einer Wache eine und tötete mit ihr Kleitos, seinen Lebensretter am Granikos. Als Alexander wieder bei Besinnung war, bereute er diese Tat zutiefst: Es heißt, er solle geklagt und geweint und versucht haben, sich das Leben zu nehmen. Er sah diese Tat jedenfalls als einen seiner schwersten Fehler an. Alexanders Neigung zu übermäßigem Alkoholgenuss – er trank allerdings fast ausschließlich in Gesellschaft – blieb eine Schwäche, bei der er häufig die Selbstkontrolle verlor. Das gemeinsame Trinken der Männer selbst gehörte fest zum gesellschaftlichen Leben in der griechischen Welt (siehe Symposion).\n\nIm folgenden Jahr 327 v. Chr. eroberte Alexander noch zwei sogdische Bergfestungen. Dann war niemand mehr übrig, der ihm Widerstand hätte leisten können. Zwei Jahre hatten die Sogdier sich gegen Alexander erhoben und ihn in immer neue Scharmützel verwickelt. Nach dieser Zeit waren die meisten von ihnen tot oder versklavt. Bevor Alexander nach Baktrien zurückkehrte, ließ er 11.000 Mann Besatzung in den eroberten Gebieten Sogdiens zurück.\n\n\nAlexander in Baktrien (327 v. Chr.).\nZurück in Baktra gab Alexander eine Reihe von Befehlen, die seine makedonische Generalität weiter von ihm entfremdete. Da sich baktrische Reiter bei den Feldzügen in Sogdien als hilfreich erwiesen hatten, befahl Alexander seinen Generälen, 30.000 junge Perser und Baktrier zu Phalanx-Soldaten auszubilden. Auch in die Kavallerie wurden Einheimische integriert. Die Soldaten akzeptierten die Auflagen widerstrebend, denn noch immer trauten sie den Persern nicht.\n\nAlexander heiratete in Baktra die sogdische Prinzessin Roxane, Tochter eines Mannes namens Oxyartes (nicht identisch mit dem gleichnamigen Bruder des Dareios). Durch diese politische Heirat gedachte er zur Befriedung Sogdiens beizutragen. Dafür schickte Alexander seine langjährige Geliebte Barsine und den gemeinsamen unehelichen Sohn Herakles fort. Die Heirat war auch eine Beleidigung für Alexanders Verbündeten Artabazos, den Vater der Barsine, seinen Statthalter in Baktrien.\n\nAußerdem versuchte Alexander, das persische Hofritual der Proskynese einzuführen: Jeder, der vor den König treten wollte, musste sich vor ihm verbeugen und das Gesicht auf den Boden pressen. Freie Makedonen und Griechen unterzogen sich einer solchen Unterwerfungsgeste allerdings nur vor den Göttern. Es heißt, dass mehrere von Alexanders Generälen sich weigerten, sich derart vor ihm zu erniedrigen. Fortan galt sie nur noch für Perser.\n\nAlexanders Anordnungen wurden als so befremdlich empfunden, dass es diesmal zur offenen Revolte unter den griechischen Soldaten zu kommen drohte. Im Rahmen der sogenannten Pagenverschwörung ließ Alexander auch eine Reihe von einstigen Gefolgsleuten hinrichten, darunter seinen Hofbiografen Kallisthenes.\n\n\nIndienfeldzug (326 v. Chr.).\nNach der Eroberung des gesamten Perserreichs fasste Alexander den Beschluss, sein Imperium weiter nach Osten auszudehnen. Indien war für die Griechen ein halblegendäres Land, über das sie kaum etwas wussten. Das Land, das damals Indien genannt wurde, ist nicht identisch mit dem heutigen Staat Indien. Es begann dort, wo Persien endete, im Osten Afghanistans, und umfasste Pakistan und das heutige Indien. Eine definierte Ostgrenze gab es nicht, da kein Reisender jemals weit nach Indien vorgedrungen war. Die westlichsten Teile jenes Indiens hatten zu Zeiten Dareios’ I. zu Persien gehört, wobei Indien selbst kein geeinter Staat war, sondern aus einer Vielzahl wenig bekannter Kleinstaaten bestand. Für den Indienfeldzug gab es keinerlei militärische Notwendigkeit. Die Gründe werden auch heute noch in der Forschung diskutiert, ohne dass bisher eine Einigung erzielt worden wäre. Möglicherweise waren es Alexanders Neugier und Kriegslust, eine Art irrationales Streben und Sehnsucht nach Erfolgen \"(pothos)\"; aber auch Thesen wie die von dem Bestreben, seine Autorität durch immer neue militärische Siege zu festigen, werden angeführt. Jedenfalls sollte sich der Indienfeldzug als schwere Belastungsprobe erweisen.\n\nAnfang des Jahres 326 v.&nbsp;Chr. stieß Alexander mit zwei Heeren ins Tal des Flusses Kabul vor, das damals ein Teil Indiens war. Der Vorstoß war von besonderer Grausamkeit gekennzeichnet. Immer seltener ließ Alexander gegenüber eroberten Regionen Großmut walten. Städte und Dörfer wurden zerstört und ihre Bevölkerung ermordet. Die zwei Armeen trafen einander am Indus. Alexander machte das Land zwischen Kabul und Indus zur Provinz Gandhara und ernannte seinen Gefolgsmann Nikanor zu deren Statthalter.\n\nAm anderen Ufer des Indus wurden Alexanders Truppen von Omphis empfangen, dem König von Taxila, das etwa 30&nbsp;km vom heutigen Islamabad entfernt lag. Hier traf Alexander einen Mann namens Kalanos, den er aufforderte, ihn auf seinen weiteren Feldzügen zu begleiten. Kalanos stimmte zu und wurde Alexanders Ratgeber; offensichtlich war er bei den kommenden Verhandlungen mit indischen Führern sehr von Nutzen.\n\nVom Hof des Omphis aus rief Alexander die anderen Staaten des Punjab auf, sich ihm zu unterwerfen und ihn als Gott anzuerkennen. Dies verweigerte Poros, der König von Pauravas, das von Taxila durch den Fluss Hydaspes (heute Jhelam) getrennt war. Im Mai überquerte Alexander während eines Platzregens den Hydaspes und besiegte eine berittene Einheit unter dem Sohn des Poros. Die Griechen und Perser zogen weiter ostwärts. Zahlenmäßig waren sie dem kleinen Heer des Poros, das sie erwartete, überlegen, doch kamen sie in dem üppig bewaldeten Land mit seinen ständigen Regenfällen schwer zurecht. Außerdem waren Berichte zu ihnen gedrungen, dass Poros eine Einheit von Kriegselefanten unterhielt, mit denen sich die Griechen nie zuvor gemessen hatten. In der Schlacht am Hydaspes wurden die Inder besiegt. In dieser Schlacht soll Alexanders Pferd Bukephalos im Hydaspes zu Tode gekommen sein, obwohl andere Quellen sagen, es sei schon vor der Schlacht an Altersschwäche eingegangen. Seinem langjährigen Reittier zu Ehren gründete Alexander die Stadt Bukephala (heute wahrscheinlich Jhelam in Pakistan). Poros wurde begnadigt und zu Alexanders Statthalter in Pauravas ernannt.\n\nWeiter im Osten am Ganges lag das Königreich Magadha, das selbst den Menschen des Punjab kaum bekannt war. Alexander wollte auch dieses Land erobern. Bei heftigem Monsunregen quälte sich die weitgehend demoralisierte Armee ostwärts und hatte einen Hochwasser führenden Fluss nach dem anderen zu überqueren. Ende Juli stand die Überquerung des Hyphasis (heute Beas) an, und von Magadha waren die Soldaten noch weit entfernt. Hier meuterten die Männer und weigerten sich weiterzugehen; ihr einziges Bestreben war die Heimkehr. Alexander war außer sich, wurde aber letztlich zur Umkehr gezwungen. Am Ufer des Hyphasis gründete er ein weiteres Alexandreia und siedelte hier viele Veteranen an, die damit wenig Hoffnung hegen durften, jemals wieder nach Griechenland zurückzukehren.\n\n\nRückkehr nach Persien (326–325 v. Chr.).\nDer beschwerliche Rückweg zum Hydaspes dauerte bis zum September. In Bukephala war mit dem Bau von 800 Schiffen begonnen worden, die den Fluss abwärts zum Indischen Ozean segeln sollten. Dies waren jedoch nicht genug, um Alexanders gesamte Armee zu transportieren, so dass Fußsoldaten die Schiffe am Ufer begleiten mussten. Im November brachen sie von Bukephala auf, doch nach zehn Tagen trafen sie am Zusammenfluss des Hydaspes mit dem Acesines (heute Chanab) auf Stromschnellen, in denen mehrere Schiffe kenterten und viele Griechen ihr Leben verloren.\n\nDer weitere Weg führte durch indische Staaten, die Alexander nicht unterworfen hatte. Immer wieder wurde das Heer angegriffen, und die Perser und Griechen zerstörten Städte und Dörfer, wo sie ihnen in den Weg kamen. Im Kampf gegen die Maller wurde Alexander bei der Erstürmung einer Stadt (vielleicht Multan) durch einen Pfeil schwer verletzt. Das Geschoss drang in seine Lunge; obwohl Alexander überlebte, sollte er den Rest seines Lebens unter den Folgen dieser Verwundung leiden. Vom Krankenlager aus befahl er, dass am Zusammenfluss von Acesines und Indus ein weiteres Alexandreia (nahe dem heutigen Uch) gegründet und Roxanes Vater Oxyartes zum Statthalter der neuen Provinz ernannt werden solle.\n\nAls Nächstes griff Alexander die Staaten von Sindh an, um seiner Armee den Weg nach Süden freizukämpfen. Die Könige Musicanos, Oxicanos und Sambos wurden unterworfen. Musicanos, der später eine Rebellion anzettelte, wurde letztlich gekreuzigt. Erst als der Monsun wieder einsetzte, erreichte das Heer 325 v. Chr. die Indusmündung und den Indischen Ozean. Alexander gründete hier die Stadt Xylinepolis (heute Bahmanabad) und machte die Flotte gefechtsbereit. Während etwa ein Viertel der Armee so auf dem Seeweg die Rückkehr antreten sollte, musste der Großteil über den Landweg nach Persien zurückkehren. Im August 325 v. Chr. machte sich das Landheer unter Alexanders Führung auf den Weg. Die Flotte unter dem Befehl des Nearchos brach einen Monat später überstürzt auf, da sich die Einheimischen zu erheben begonnen hatten. Praktisch unmittelbar nach dem Abzug des Heeres fielen die gerade eroberten Kleinstaaten Indiens ab und erhoben sich gegen die in den neuen Städten zurückgebliebenen Veteranen, über deren weiteres Schicksal in den wenigsten Fällen etwas bekannt ist.\n\nDas heutige Belutschistan war damals als Gedrosien bekannt. Obwohl die Perser vor der Durchquerung der gedrosischen Wüste warnten, ging Alexander dieses Risiko ein, wahrscheinlich weil dieser Weg der kürzeste war. Die Hintergründe sind in der Forschung jedoch umstritten. Ob er wirklich die sagenhafte Königin Semiramis übertreffen wollte, ist wenigstens fraglich; wenn, dann ging es Alexander wohl darum, die Rückschläge des Indienfeldzugs durch dieses Unternehmen zu relativieren. Auch die Stärke seines Heeres zu diesem Zeitpunkt ist ungewiss, von wohl sicher übertriebenen 100.000 Mann bis zu wahrscheinlich realistischeren 30.000. Die sechzigtägigen Strapazen ließen zahllose Soldaten durch Erschöpfung, Hitzschlag oder Verdursten ums Leben kommen; dabei spielte auch der Umstand eine Rolle, dass Alexanders Führer offenbar recht unfähig waren. Im Dezember erreichten die Soldaten Pura (heute Bampur), einen der östlichsten Vorposten Persiens, und waren damit in Sicherheit.\n\n\nMassenhochzeit von Susa, Revolte in Opis und Tod Hephaistions (324 v. Chr.).\nAlexander gründete im Januar 324 v. Chr. ein weiteres Alexandreia; heute \"Golashkerd\". Auf dem Weg westwärts stieß er in Susa auf Nearchos und seine Männer, die den Seeweg weitgehend unversehrt überstanden hatten. Neue Feiern wurden genutzt, um 10.000 persische Frauen mit Soldaten zu verheiraten – die Massenhochzeit von Susa.\n\nDie Ehen wurden von Alexander als Notwendigkeit gesehen, um das Zusammenwachsen von Persern und Makedonen/Griechen weiter voranzutreiben. Er selbst heiratete zwei Frauen, nämlich Stateira, eine Tochter des Dareios, und Parysatis. Er war somit nun mit drei Frauen verheiratet. Die Hochzeiten wurden nach persischem Ritual begangen. Schon Alexanders Vater hatte die Ehe mit mehreren Frauen als diplomatisches Mittel zur Stabilisierung und Ausweitung seines Machtbereiches eingesetzt.\n\nIn der Forschung wurde dies als Versuch interpretiert, eine Art „Verschmelzungspolitik“ zu betreiben (Johann Gustav Droysen). Der britische Historiker Tarn sah darin gar den Versuch einer „Vereinigung der Menschheit“; viele andere moderne Historiker wie Badian oder Bosworth lehnen dies jedoch ab.\n\nUm weitere Attribute eines persischen Staates zu übernehmen, ernannte Alexander seinen langjährigen Freund Hephaistion (und nach dessen Tod Perdikkas) zum \"Chiliarchen\" (Wesir) und seinen General Ptolemaios zum Vorkoster. Beide Titel waren im Westen unbekannt. Außerdem wurden gegen mehrere Statthalter, die sich bereichert hatten oder ihren Aufgaben nicht sachgerecht nachgekommen waren, Prozesse eröffnet. Harpalos, ein Jugendfreund Alexanders und sein Schatzmeister, befürchtete aufgrund seines Verhaltens einen solchen Prozess. Er setzte sich mit 6000 Söldnern und 5000 Talenten Silber nach Griechenland ab, wurde jedoch bald darauf auf Kreta ermordet.\n\nDie Neuerungen Alexanders vergrößerten die Kluft zwischen ihm und seiner makedonischen Generalität. Da die Zahl der Soldaten iranischer Herkunft im Heer die der Makedonen zu übertreffen begann, fürchteten sie, bald gänzlich bedeutungslos zu sein. Perser durften nun auch höhere Ränge in der Armee bekleiden, was die Makedonen als unerhört ansahen. Als die Armee die Stadt Opis am Tigris erreichte, erlaubte Alexander vielen Makedonen die Rückkehr nach Hause. Was sie vorher ersehnt hatten, sahen sie nun als Affront, da dies das erste Zeichen ihrer Ersetzung durch Orientalen zu sein schien. Quellen berichten, dass manche der Soldaten Alexander wüste Beleidigungen entgegen geschrien hätten. Alexander reagierte, indem er sie ihrer Stellungen enthob und drohte, die persischen Soldaten gegen sie zu schicken. Die Soldaten entschuldigten sich, und ihnen wurde verziehen. 11.500 griechische Soldaten wurden in den Folgetagen nach Hause geschickt.\n\nIm Herbst des Jahres 324 v. Chr. ging Alexander nach Ekbatana, wo Hephaistion nach einem von vielen Trinkgelagen erkrankte und starb. Alexander, der wohl lange Jahre Hephaistions Geliebter gewesen war (zumindest bis zum Feldzug im Iran), war außer sich vor Trauer. Er ließ laut Plutarch den Arzt seines Freundes kreuzigen, die Haare von Pferden und Maultieren abrasieren und opfern, fastete mehrere Tage und richtete dann ein monumentales Begräbnis aus. Danach ließ er sämtliche Kossaier umbringen. Die Beziehung zwischen Alexander und Hephaistion wird oft mit der zwischen Achilleus und Patroklos gleichgesetzt. Denn da sich das Geschlecht von Alexanders Mutter Olympias auf den Helden aus dem Trojanischen Krieg zurückführte, verglich Alexander selbst sich mit Achilles und seinen Freund mit Patroklos.\n\nAlexander hatte, so wie auch sein Vater Philipp und viele andere Makedonen bzw. Griechen seiner Zeit, Beziehungen sowohl zu Frauen – er hatte mehrere, deren bekannteste und wohl ernsthafteste die zu Roxane war – als auch zu Männern, wobei diese teils auch sexueller Natur waren. Gleichgeschlechtliche Beziehungen wurden zu jener Zeit nicht geächtet, es kam aber sehr wohl auf den sozialen Status der Partner an.\n\n\nAlexanders letztes Jahr und sein Tod in Babylon (323 v. Chr.).\nAlexander ließ den persischen königlichen Schatz ausmünzen und warf damit das Vermögen der Achämeniden in das Austauschsystem des Nahen Ostens, womit ein steiler Anstieg im Volumen der Markttransaktionen im Mittelmeergebiet finanziert wurde. Dass der attische Münzfuß nunmehr – außer im ptolemäischen Ägypten – allgemein in der hellenistischen Welt galt, erleichterte den internationalen Handel und die Schifffahrt.\n\nBei den Olympischen Spielen des Jahres 324 v. Chr. ließ Alexander das sogenannte Verbanntendekret verkünden, mit dem er den griechischen Poleis befahl, die jeweils aus politischen Gründen ins Exil getriebenen Bürger wieder aufzunehmen. Dies stellte einen massiven Eingriff in die Autonomie der Städte dar, führte zu heftigen Konflikten in den Gemeinwesen und war letztlich der Anlass dafür, dass sich Athen und mehrere andere Städte nach dem Tod des Königs im Lamischen Krieg gegen die makedonische Herrschaft erhoben.\n\nIm Februar 323 v. Chr. kehrte Alexander nach Babylon zurück. Hier bereitete er neue Feldzüge vor, die zur Einnahme der Arabischen Halbinsel führen sollten. Ob er überdies, wie Diodor berichtet, auch plante, anschließend den westlichen Mittelmeerraum mit Karthago zu erobern, ist seit langer Zeit umstritten. In der neueren Forschung geht man zumeist davon aus, dass Alexander in der Tat eine solche Expedition vorbereiten ließ, da den Makedonen im Jahr 322 während des Lamischen Krieges eine sehr große Flotte zur Verfügung stand, die mutmaßlich ursprünglich für das Unternehmen gegen Karthago gebaut worden war. Im Mai, kurz vor dem geplanten Aufbruch des Heeres gen Arabien, verkündete Alexander, dass sein toter Freund Hephaistion fortan als Halbgott zu verehren sei, nachdem ein Bote aus der Oase Siwa eingetroffen war, wo Alexander wegen einer Vergöttlichung Hephaistions angefragt hatte. Aus diesem Anlass veranstaltete er Feiern, bei denen er sich wieder dem unmäßigen Trunk hingab. Am nächsten Tag erkrankte er an einem Fieber, und am 10. Juni starb er schließlich.\n\nHinsichtlich der Todesursache wurden seither mehrere Thesen diskutiert, darunter eine, nach der Alexander am West-Nil-Fieber erkrankte. Auch eine Alkoholvergiftung wird immer wieder in Erwägung gezogen. Nach einer in der Antike verbreiteten Überlieferung ist er hingegen vergiftet worden (angeblich mit dem giftigen Wasser des Styx). Wahrscheinlicher ist, dass seine körperliche Schwächung durch zahlreiche Kampfverletzungen und übermäßigen Weinkonsum zu einer Krankheit geführt hat. Da die Ärzte damals auf die reinigende Wirkung von herbeigeführtem Erbrechen und Durchfall vertrauten, war es üblich, Weißen Germer in geringen Dosen zu verabreichen. Die überlieferten Symptome Alexanders sind typisch für eine Vergiftung durch Weißen Germer. Möglicherweise verschlechterten die Ärzte seinen Zustand daher durch wiederholte Gaben des Mittels.\n\nDer Leichnam Alexanders soll zur Konservierung in Honig gelegt worden sein. Entgegen dem Wunsch des Verstorbenen, im Ammonium von Siwa begraben zu werden, wurde er in Alexandria beigesetzt.\n\nAlexanders letzte Worte auf die Frage, wem er sein Reich hinterlassen werde, sollen gelautet haben:\n\"Dem Besten.\" Des Weiteren äußerte Alexander eine dunkle Prophezeiung: Er glaube, \"dass seine Freunde große Begräbnisspiele für ihn veranstalten werden\". Seinen Siegelring übergab er Perdikkas, der nach Hephaistions Tod sein engster Vertrauter gewesen war.\n\n\nAlexandergrab.\nAlexander hatte eine Beisetzung im Ammonheiligtum der Oase Siwa gewünscht. Erst nach zweijährigen Vorbereitungen setzte sich der Leichenzug in Babylon in Bewegung. Er wurde in Syrien von Ptolemaios, dem künftigen König Ptolemaios I., in Empfang genommen und nach Ägypten geleitet. Dort wurde der Leichnam aber nicht in die Oase gebracht, sondern zunächst in Memphis bestattet. Später (wohl noch in der Regierungszeit Ptolemaios’ I., spätestens einige Jahre nach seinem Tod) wurde er nach Alexandria verlegt, nachdem dort eine prächtige Grabstätte für ihn errichtet worden war. Sie wurde unter König Ptolemaios IV. durch ein neues Mausoleum ersetzt, das dann auch als Grabstätte der Ptolemäer diente, die sich wie alle Diadochen auf Alexanders Vorbild beriefen. Die mumifizierte Leiche befand sich in einem goldenen Sarkophag, der aber im 1. Jahrhundert v. Chr. von König Ptolemaios X. durch einen gläsernen ersetzt wurde, der den Blick auf den einbalsamierten Leichnam freigab. Dieser Schritt Ptolemaios' X., der später irrtümlich als Grabschändung gedeutet wurde, sollte den Alexanderkult fördern.\n\nFür Caesar, Augustus, Septimius Severus und Caracalla sind Besuche am Grab bezeugt. Möglicherweise wurde es während der Stadtunruhen in der Spätantike oder bei einer Naturkatastrophe zerstört. In den Wirren der Spätantike ging die Kenntnis über den Ort der Grabstätte verloren (zumindest die Leiche soll laut Libanios noch Ende des 4. Jahrhunderts zu sehen gewesen sein). Der Kirchenvater Johannes Chrysostomos († 407) stellte in einer Predigt die rhetorische Frage nach dem Ort des Alexandergrabs, um die Vergänglichkeit des Irdischen zu illustrieren; er konnte also mit Sicherheit davon ausgehen, dass keiner seiner Hörer wusste, wo sich das berühmte Bauwerk befunden hatte. Die Erinnerung daran blieb aber noch in islamischer Zeit erhalten; im 10. Jahrhundert wurde eine angebliche Grabstätte gezeigt. Im 15. und 16. Jahrhundert berichteten europäische Reisende von einem kleinen Gebäude in Alexandria, das als Alexandergrab ausgegeben wurde. Seit dem 18. Jahrhundert sind viele Lokalisierungsversuche unternommen worden, die bisher alle fehlgeschlagen sind.\n\n\nGeschichtlicher Ausblick.\nNach Alexanders Tod erwies sich die Loyalität zu seiner Familie, die keinen herrschaftsfähigen Nachfolger stellen konnte, als sehr begrenzt. Zwar wurde zunächst der Erbanspruch seines geistesschwachen Halbbruders und auch der seines postum geborenen Sohnes anerkannt, doch hatte diese Regelung keinen Bestand. Seine Mutter Olympias von Epirus, seine Frau Roxane, sein Sohn Alexander IV., sein illegitimer Sohn Herakles, seine Schwester Kleopatra, seine Halbschwester Kynane, deren Tochter Eurydike und sein Halbbruder Philipp III. Arrhidaios fanden einen gewaltsamen Tod. Statt der Angehörigen des bisherigen makedonischen Königsgeschlechts übernahmen Alexanders Feldherren als seine Nachfolger (Diadochen) die Macht. Da keiner von ihnen stark genug war, sich als Alleinherrscher durchzusetzen, kam es zu einer langen Reihe von Bürgerkriegen, in denen man in wechselnden Koalitionen um die Macht rang. Im Verlauf der Diadochenkriege wurde das riesige Reich in Diadochenreiche aufgeteilt. Drei dieser Reiche erwiesen sich als dauerhaft: das der Antigoniden in Makedonien (bis 148 v. Chr.), das der Seleukiden in Vorderasien (bis 64 v. Chr.) und das der Ptolemäer in Ägypten (bis 30 v. Chr.). Alexander hinterließ zahlreiche neu gegründete Städte, von denen viele seinen Namen trugen; die bedeutendste war Alexandreia in Ägypten.\n\n\nRezeption.\n\nAntike.\n\nQuellen.\nAlexander wurde schon zu Lebzeiten eine mythische Gestalt, wozu sein Anspruch auf Gottessohnschaft beitrug. Die zeitgenössischen erzählenden Quellen sind nicht oder nur in Fragmenten erhalten. Dabei handelte es sich, neben den Fragmenten der angeblichen Kanzleidokumente Alexanders \"(Ephemeriden)\", größtenteils um Berichte von Teilnehmern des Alexanderzugs. Der Hofhistoriker Kallisthenes begleitete Alexander, um die Taten des Königs aufzuzeichnen und zu verherrlichen. Sein Werk „Die Taten Alexanders“ reichte vielleicht nur bis 331 v. Chr., hatte jedoch einen enormen Einfluss auf die späteren Alexanderhistoriker. Weitere Verfasser von Alexandergeschichten waren König Ptolemaios I. von Ägypten, der als Offizier und Hofbeamter in der Nähe Alexanders gelebt hatte, Aristobulos, der für Alexander Unvorteilhaftes leugnete oder abschwächte, sowie Alexanders Flottenbefehlshaber Nearchos und dessen Steuermann Onesikritos. Die stärkste Nachwirkung unter diesen frühen Alexanderhistorikern erzielte Kleitarchos, der zwar ein Zeitgenosse, aber selbst kein Feldzugsteilnehmer war, sondern in Babylon Informationen von Offizieren und Soldaten Alexanders zusammentrug und zu einer rhetorisch ausgeschmückten Darstellung verband, wobei er auch sagenhafte Elemente einbezog. Zu diesen frühen Legenden gehörte beispielsweise die falsche Behauptung, Alexander und Dareios seien einander wiederholt im Nahkampf begegnet.\n\nIm 2. Jahrhundert n. Chr. schrieb der römische Senator Arrian auf der Grundlage der älteren Quellen, unter denen er Ptolemaios und Aristobulos bevorzugte, seine \"Anabasis\", die verlässlichste antike Alexanderquelle. Wahrscheinlich behandelte auch Strabon in seinen nicht erhaltenen \"Historika Hypomnemata\" („Historische Denkwürdigkeiten“) das Leben Alexanders; seine erhaltene \"Geographie\" enthält Informationen aus verlorenen Werken der frühen Alexanderhistoriker.\n\nWeitere Nachrichten finden sich im 17. Buch der Universalgeschichte Diodors, der sich auf Kleitarchos stützte. Plutarch verfasste eine Lebensbeschreibung Alexanders, wobei es ihm mehr auf das Verständnis des Charakters unter moralischem Gesichtspunkt als auf den historischen Ablauf ankam. Quintus Curtius Rufus schrieb eine in der Antike wenig beachtete Alexandergeschichte. Justin wählte für seine Darstellung aus seiner (verlorenen) Vorlage, der Universalgeschichte des Pompeius Trogus, vor allem Begebenheiten aus, die geeignet waren, seine Leserschaft zu unterhalten.\n\nDie Berichte von Curtius, Diodor und Pompeius Trogus hängen von einer gemeinsamen Quelle ab; das Nachrichtenmaterial, das sie übereinstimmend überliefern, stammt wohl von Kleitarchos. Diese Tradition \"(Vulgata)\" bietet teils wertvolle Informationen; Curtius wird in der französischen Forschung leicht gegenüber Arrian favorisiert. Zusätzliches Material ist bei Athenaios sowie in der Metzer Epitome und dem Itinerarium Alexandri überliefert. Nur wenige Fragmente sind von den Werken des Chares von Mytilene und des Ephippos von Olynth erhalten.\n\n\nLegende.\nAls Quelle für den historischen Alexander von relativ geringem Wert, aber literarisch von außerordentlicher Bedeutung ist der „Alexanderroman“. Mit diesem Begriff bezeichnet man eine Vielzahl von antiken und mittelalterlichen Biografien Alexanders, welche seine sagenhaften Taten schildern und verherrlichen. Im Lauf der Jahrhunderte wurde der Stoff fortlaufend literarisch bearbeitet und ausgeschmückt. Die griechische Urfassung in drei Büchern, die den Ausgangspunkt für alle späteren Versionen und Übersetzungen in viele Sprachen bildet, ist wahrscheinlich im späten 3. Jahrhundert in Ägypten entstanden. Ihr unbekannter Autor, der wohl ein Bürger von Alexandria war, wird als Pseudo-Kallisthenes bezeichnet, weil ein Teil der handschriftlichen Überlieferung das Werk irrtümlich dem Alexanderhistoriker Kallisthenes von Olynth zuschreibt. Diesem Werk lagen ältere, nicht erhaltene romanhafte Quellen, fiktive Briefe Alexanders und kleinere Erzählungen zugrunde. Der bekannteste unter den Briefen ist ein angeblich von Alexander an Aristoteles gerichtetes Schreiben über die Wunder Indiens, das in verkürzter Fassung in den Roman eingebaut wurde und auch separat überliefert ist.\n\nDie gängige Bezeichnung „Roman“ bezieht sich auf die literarische Gattung des Antiken Romans. Im Gegensatz zum modernen Roman hielten der Verfasser und seine antike und mittelalterliche Leserschaft an dem Anspruch fest, der Inhalt sei Geschichtsschreibung und nicht literarische Erfindung.\n\nDie Idee des historischen Alexander, er sei ein Sohn des ägyptischen Gottes Ammon (Amun), verfremdet der Romanautor, indem er aus Alexander ein uneheliches Kind macht. Alexanders Vater ist im Roman der aus Ägypten nach Makedonien geflohene König und Zauberer Nektanebos, der als Ammon auftritt (gemeint ist der Pharao Nektanebos II.). Nektanebos verführt die Königin Olympias während der Abwesenheit ihres Gemahls Philipp. Später tötet Alexander, der als Sohn Philipps aufwächst, seinen leiblichen Vater; erst dann erfährt er seine wahre Abstammung. So macht der ägyptische Autor Alexander zum Ägypter. Eine weitere wesentliche Neuerung des Pseudo-Kallisthenes ist die Einführung eines nicht historischen Italienzugs Alexanders, auf dem der Makedone nach Rom kommt. Rom unterstellt sich ihm ebenso wie alle anderen Reiche des Westens kampflos. Dann unterwirft er in schweren Kämpfen die Völker des Nordens, bevor er gegen das Perserreich zieht. Hier zeigt sich das literarische Bedürfnis, den Helden auch den Westen und Norden erobern zu lassen, damit seine Weltherrschaft vollendet wird. Roxane ist im Roman eine Tochter des Perserkönigs Dareios, die dieser sterbend Alexander zur Frau gibt. Das letzte der drei Bücher, das den Indienfeldzug und den Tod des Helden behandelt, ist besonders stark von Wundern und phantastischen Elementen geprägt. Es schildert auch Alexanders angeblichen Besuch bei der Königin Kandake von Meroe, wobei der König in Verkleidung auftritt, aber enttarnt wird (eine Episode, der spätere Bearbeiter des Stoffs eine ursprünglich völlig fehlende erotische Komponente verleihen). Schließlich wird Alexander vergiftet.\n\nIm frühen 4. Jahrhundert fertigte Iulius Valerius eine freie lateinische Übersetzung des Alexanderromans an \"(Res gestae Alexandri Magni)\". Dabei nahm er Hunderte von Erweiterungen, Änderungen und Auslassungen vor. Er beseitigte Ungereimtheiten und Formulierungen, die den Makedonenkönig in ein ungünstiges Licht rücken konnten, und fügte für Alexander vorteilhafte Details ein. Sein Alexander ist eine mit allen Herrschertugenden ausgestattete Idealgestalt; er begeht zwar Fehler, lernt aber daraus.\n\nEin weiterer Bestandteil der antiken Alexandersage sind fiktive Dialoge des Königs mit den indischen Brahmanen sowie Briefe, die angeblich zwischen ihnen ausgetauscht wurden. Dabei versuchen die Inder, die Überlegenheit östlicher Weisheit und einer einfachen, naturnahen Lebensweise gegenüber der griechischen Zivilisation und dem Machtstreben Alexanders aufzuzeigen. Auch dieses Schrifttum war sowohl griechisch als auch lateinisch verbreitet. Da es um grundsätzliche Fragen der Lebensführung und um Askese ging, war die Wirkung in christlicher Zeit beträchtlich.\n\n\nKult und Vorbildfunktion.\nDie Herrscher, die nach Alexanders Tod in den verschiedenen Teilen seines Reichs an die Macht kamen, waren nicht mit ihm blutsverwandt, und soweit in Makedonien Loyalität zur herkömmlichen Ordnung vorhanden war, galt sie dem Herrscherhaus insgesamt, wobei es nicht speziell auf die verwandtschaftliche Nähe zu Alexander ankam. Daher gab es in den Diadochenreichen wenig Anlass für einen offiziellen staatlichen Alexanderkult; dieser blieb den einzelnen Städten überlassen. Erst in hoch- und späthellenistischer Zeit wurde der politische Rückgriff auf Alexander zu einem wichtigen propagandistischen Mittel. Einen Sonderfall bildete jedoch Ägypten, dessen neue Hauptstadt Alexandria eine Gründung Alexanders und der Ort seines Grabes war. Die dort regierenden Ptolemäer förderten von Anfang an den Alexanderkult im Rahmen ihrer Propaganda. Er bildete aber zunächst keinen zentralen Bestandteil ihrer Herrschaftslegitimation und wurde erst von Ptolemaios X., der den Doppelnamen „Ptolemaios Alexandros“ führte, intensiv politisch instrumentalisiert.\n\nEin prominenter Gegner der Römer, König Mithridates VI. von Pontos († 63 v. Chr.), fiel durch seine mit Nachdruck betriebene Alexander-Imitation auf. Er bekleidete sich mit dem Mantel Alexanders, den er von den Ptolemäern erbeutet hatte, und illustrierte so seinen Anspruch, Vorkämpfer des Griechentums und Retter der hellenistischen Monarchie vor den Römern zu sein. Später erbeutete der römische Feldherr Gnaeus Pompeius Magnus, der Mithridates besiegte, diesen Mantel und trug ihn bei seinem Triumphzug. Mit Pompeius, dessen Beiname „der Große“ an Alexander erinnerte, begann die offenkundige römische Alexander-Imitation, zunächst als Reaktion auf die Propaganda des Mithridates. Mehrere römische Feldherren und Kaiser stellten sich propagandistisch in Alexanders Nachfolge; sie verglichen sich mit ihm und versuchten, seine Erfolge im Osten zu wiederholen. Dabei steigerte sich die Verehrung Alexanders in manchen Fällen zu einer demonstrativen Nachahmung von Äußerlichkeiten. Zu den Verehrern und Nachahmern Alexanders zählten unter den Kaisern insbesondere Trajan, Caracalla und (mit Vorbehalten) Julian. Augustus trug zeitweilig auf seinem Siegelring ein Bildnis Alexanders, Caligula legte sich den aus Alexandria geholten angeblichen Panzer Alexanders an, Nero stellte für einen geplanten Kaukasusfeldzug eine neue Legion auf, die er „Phalanx Alexanders des Großen“ nannte, Trajan setzte sich einen Helm auf, den Alexander getragen haben soll. Kaiser Severus Alexander, der ursprünglich Alexianus hieß, änderte seinen Namen in Anknüpfung an den Makedonen.\n\n\nUrteile.\nEinen sehr tiefen und dauerhaften Eindruck hinterließ in Griechenland die Zerstörung Thebens. Sie wurde nicht nur von den Zeitgenossen, sondern jahrhundertelang (noch in der römischen Kaiserzeit) als unerhörte Grausamkeit empfunden, die man Alexander zur Last legte, und als historisches Musterbeispiel einer entsetzlichen Katastrophe zitiert. Besonders die antiken Redner kamen mit Vorliebe darauf zu sprechen und nutzten diese Gelegenheit, bei ihrem Publikum starke Emotionen zu wecken. Es hieß, Alexander habe wie ein wildes Tier und als Unmensch \"(apánthrōpos)\" gehandelt. Noch in byzantinischer Zeit wurde diese Deutungstradition rezipiert.\n\nAus philosophischer Sicht wurde Alexander meist negativ beurteilt, da seine Lebensweise einen Kontrast zu den philosophischen Idealen der Mäßigung, Selbstbeherrschung und Seelenruhe bildete. Insbesondere die Stoiker kritisierten ihn heftig und warfen ihm Hochmut vor; ihre Kritik richtete sich auch gegen Aristoteles (den Gründer einer rivalisierenden Philosophenschule), der als Erzieher Alexanders versagt habe. Auch die Kyniker pflegten Alexander abschätzig zu beurteilen, wobei die Anekdote von der Begegnung des Königs mit dem berühmten kynischen Philosophen Diogenes von Sinope den Ansatzpunkt bildete. Ihr zufolge hatte Diogenes Alexander, der ihm einen Wunsch freistellte, nur gebeten: „Geh mir aus der Sonne“, und Alexander soll gesagt haben: „Wenn ich nicht Alexander wäre, wollte ich Diogenes sein.“ In der von Aristoteles gegründeten Philosophenschule der Peripatetiker war die Ablehnung Alexanders ebenfalls ausgeprägt, wenn auch nicht durchgängig. Ihr Anlass waren anscheinend ursprünglich Spannungen zwischen Aristoteles und Alexander, die noch in der römischen Kaiserzeit ein spätes Echo in einem haltlosen Gerücht fanden, wonach Aristoteles ein Gift zubereitet hatte, mit dem Alexander ermordet wurde. Das negative Alexander-Bild der Philosophen teilte auch Cicero. Er überliefert die berühmte Anekdote von dem gefangenen Seeräuber, der von Alexander wegen seiner Übeltaten zur Rede gestellt wurde, worauf der Pirat erwiderte, er handle in kleinem Maßstab aus demselben Antrieb, aus dem der König weltweit dasselbe tue.\n\nBesonders drastisch drückte Seneca die stoische Sichtweise aus. Er bezeichnete Alexander als wahnsinnigen Burschen, zum Bersten aufgeblasenes Tier, Räuber und Plage der Völker. Ähnlich äußerte sich Senecas Neffe, der Dichter Lucan. Der philosophisch orientierte Kaiser Julian, der Alexander als Feldherrn bewunderte, kritisierte ihn zugleich scharf wegen Maßlosigkeit und unphilosophischer Lebensführung.\n\nUnter den philosophisch orientierten Autoren gab es auch eine kleine Minderheit, die Alexander Lob spendete. Dazu gehörte Plutarch, der in seinen zwei Deklamationen „Über das Glück oder die Tugend Alexanders des Großen“ aus dem König einen Philosophenherrscher machte, dessen Eroberungen barbarischen Völkern Recht und Frieden brachten und die Unterworfenen so humanisierten. Bei diesen Jugendwerken Plutarchs handelte es sich allerdings um rhetorische Stilübungen, die nicht notwendigerweise seine wirkliche Auffassung spiegeln. In seiner Lebensbeschreibung Alexanders äußerte sich Plutarch weit kritischer, bemühte sich aber auch um eine Rechtfertigung Alexanders. Dion von Prusa, der den an Alexander anknüpfenden Kaiser Trajan bewunderte, würdigte die heldenhafte Gesinnung des Makedonenkönigs.\n\nBei den Römern war ein beliebtes Thema die hypothetische Frage, wie ein militärischer Konflikt zwischen dem Römischen Reich und Alexander verlaufen wäre. Der Historiker Livius befasste sich eingehend damit und kam zum Ergebnis, dass die römischen Heerführer dem Makedonenkönig überlegen waren. Alexander habe seine Siege der militärischen Untüchtigkeit seiner Gegner verdankt. Diese Einschätzung verband Livius mit einem vernichtenden Urteil über Alexanders Charakter, der durch die Erfolge des Königs verdorben worden sei. Ähnlich urteilte Curtius Rufus, der die Siege des Makedonen mehr auf Glück als auf Tüchtigkeit zurückführte und meinte, die Herausbildung tyrannischer Züge in Alexanders Charakter sei ein Ergebnis übermäßigen Erfolgs gewesen.\n\nAus jüdischer Sicht fiel das Urteil über Alexander sehr vorteilhaft aus. Der jüdische Geschichtsschreiber Flavius Josephus beschreibt Gunstbezeugungen des Makedonen für die Juden und behauptet, Alexander habe sich, als er nach Jerusalem kam, vor dem Gott, den die Juden verehrten, niedergeworfen. Dabei handelt es sich um eine jüdische Abwandlung einer griechischen Erzählung.\n\nIm 4. Jahrhundert wurden im Osten des Reichs Bronzemünzen Alexanders wie Amulette getragen.\n\nUnter den Kirchenvätern hebt sich Orosius als radikalster Kritiker Alexanders ab. In seiner auf Justin fußenden \"Historia adversus paganos\" („Geschichte gegen die Heiden“) schildert er ihn als blutdürstigen, grausamen Unmenschen und großen Zerstörer.\n\n\nMittelalter.\nDie mittelalterliche Alexander-Rezeption war außerordentlich intensiv und vielfältig. Dabei stand das Sagengut im Vordergrund. Die antike Gestalt wurde mittelalterlichen Vorstellungen angepasst; beispielsweise erhält der König eine Ritterpromotion (Schwertleite). Besonders Dichter regte der Stoff im Westen ebenso wie im Orient zur Bearbeitung an; es entstanden über 80 Dichtungen in 35 Sprachen.\n\n\nQuellen.\nDie grundlegenden antiken Quellen, die im Mittelalter in West- und Mitteleuropa zur Verfügung standen, waren neben Pseudo-Kallisthenes der eifrig rezipierte Curtius Rufus, der nur als Nebenquelle dienende Justin und der viel beachtete Orosius, dessen negative Bewertung Alexanders allerdings wenig Beachtung fand. Besonders die märchenhaften Elemente des Alexanderromans machten Eindruck und regten die Phantasie der Bearbeiter zu weiteren Ausformungen an. Der Roman wurde in zahlreiche europäische Sprachen übersetzt, wobei lateinische Fassungen die Grundlage bildeten; hinzu kamen die teils stark abweichenden Versionen in orientalischen Sprachen (Armenisch, Altsyrisch, Hebräisch, Arabisch, Persisch, Türkisch, Äthiopisch, Koptisch).\n\nEine wesentliche Rolle spielte ferner die Prophetie im biblischen Buch Daniel über den Untergang der aufeinanderfolgenden Weltreiche; in diesem Licht erschien Alexander, der nach mittelalterlicher Deutung das zweite der vier Weltreiche vernichtete und das dritte gründete, als Werkzeug Gottes. Auch dem ersten Kapitel des ersten Makkabäerbuchs war eine knappe Zusammenfassung von Alexanders Lebensgeschichte zu entnehmen; dort las man, dass er bis ans Ende der Welt gelangte und „die Welt vor ihm verstummte“. Dieser biblische Hintergrund verlieh ihm zusätzliche Bedeutung.\n\n\nHeldenkatalog.\nIm Spätmittelalter zählte man Alexander zum Kreis der Neun Guten Helden, einem in der volkssprachlichen Literatur beliebten Heldenkatalog, der für die Zeit des Alten Testaments, die griechisch-römische Antike und die christliche Zeit jeweils die drei größten Helden benannte; für die Antike waren es Hektor, Alexander und Caesar. Noch breiter als in der Literatur wurde diese Heldenreihe in der Bildenden Kunst (Skulptur, Malerei, Textilkunst) rezipiert.\n\n\nMittellateinische Literatur.\nDas Alexanderbild in der lateinischsprachigen Welt des Mittelalters war großenteils vom lateinischen Alexanderroman geprägt. Im Frühmittelalter ging die Hauptwirkung nicht von der ursprünglichen Fassung der von Iulius Valerius stammenden Übersetzung aus, von der nur drei vollständige Handschriften überliefert waren; weit bekannter war ein in mehr als 60 Handschriften erhaltener, spätestens im 9. Jahrhundert entstandener Auszug \"(Epitome)\" aus diesem Werk. Um 968/969 fertigte der Archipresbyter Leo von Neapel eine neue lateinische Übersetzung des Pseudo-Kallisthenes aus dem Griechischen an, die \"Nativitas et victoria Alexandri Magni\" („Geburt und Sieg Alexanders des Großen“), die mehrfach – zuletzt noch im 13. Jahrhundert – überarbeitet und erweitert wurde; die überarbeiteten Fassungen sind unter dem Titel \"Historia de preliis Alexandri Magni\" („Geschichte von den Schlachten Alexanders des Großen“) bekannt. Der Dichter Quilichinus von Spoleto schrieb 1237/1238 eine Versfassung der \"Historia de preliis\" in elegischen Distichen, die im Spätmittelalter populär wurde. Noch weit einflussreicher war aber die schon zwischen 1178 und 1182 verfasste \"Alexandreis\" Walters von Châtillon, ein Epos in zehn Büchern auf der Grundlage der Darstellung des Curtius Rufus, das zur Schullektüre wurde und im 13. Jahrhundert als Schulbuch Vergils Aeneis an Beliebtheit übertraf. Walter verzichtete fast gänzlich auf die Auswertung des im Alexanderroman vorliegenden Materials. Für ihn war Alexander der stets siegreiche Held, der sich selbst ebenso wie alle Feinde überwand und so unsterblichen Ruhm erlangte.\n\nDas Verhältnis dieser Autoren und ihres Publikums zu Alexander war vor allem von Bewunderung für außerordentliche Heldentaten und von Staunen über das Märchenhafte und Exotische geprägt. Besondere Beachtung fand Alexanders Tod; er bot Anlass zu unzähligen religiös-erbaulichen Betrachtungen, die auf die Endlichkeit und Nichtigkeit aller menschlichen Größe angesichts des Todes abzielten. Auf diesen Aspekt wiesen unter anderem viele Kleindichtungen hin, darunter insbesondere fingierte Grabschriften Alexanders.\n\nBesonders fasziniert waren mittelalterliche Leser von einer Erzählung von Alexanders Himmelsflug und Tauchexpedition, die Leo von Neapel nach dem griechischen Roman wiedergab. Dieser Sage zufolge wollte der König nicht nur auf der Erdoberfläche die äußersten Grenzen erreichen, sondern auch den Himmel und die Tiefe des Ozeans erkunden. Zu diesem Zweck ersann und baute er mit seinen Freunden ein von Greifen gezogenes Luftfahrzeug und ein von Ketten gehaltenes gläsernes Tauchfahrzeug. Der Himmelsflug wurde von mittelalterlichen Künstlern häufig abgebildet. Hans Christian Andersen hat die Geschichte noch im 19. Jahrhundert in seinem Kunstmärchen \"Der böse Fürst\" verarbeitet, ohne jedoch Alexander namentlich zu nennen.\n\nAus dem 12. Jahrhundert stammt das \"Iter ad Paradisum\" („Paradiesfahrt“), die lateinische Version einer jüdischen Sage über Alexanders Versuch, das irdische Paradies zu finden, den in der Genesis beschriebenen Garten Eden.\n\nNeben der Heldenverehrung kamen vereinzelt auch extrem negative Deutungen der Persönlichkeit Alexanders vor. So setzten ihn im 12. Jahrhundert die prominenten Theologen Hugo von St. Viktor und Gottfried von Admont mit dem Teufel gleich.\n\nErzählungen aus dem Alexanderroman wurden in Weltchroniken und Enzyklopädien aufgenommen, was ihre Rezeption zusätzlich erweiterte.\n\nDie lateinische Überlieferung bildete die Grundlage für die volkssprachliche Rezeption. In den volkssprachlichen Literaturen entstanden zahlreiche Prosawerke und Dichtungen über Stoffe der Alexandersage, wobei vor allem die verschiedenen lateinischen Fassungen des Pseudo-Kallisthenes, die \"Historia Alexandri\" des Curtius Rufus und die \"Alexandreis\" Walters von Châtillon verarbeitet wurden.\n\n\nRomanische Literaturen.\nAlberich von Bisinzo (Albéric de Pisançon), der im frühen 12. Jahrhundert die älteste volkssprachliche Alexander-Biografie verfasste, ein nur teilweise erhaltenes Gedicht in frankoprovenzalischem Dialekt, verwarf nachdrücklich die Legende von Alexanders unehelicher Geburt und hob seine hochadlige Abstammung von väterlicher und mütterlicher Seite hervor. Er betonte auch die hervorragende Bildung des Herrschers, die – einem mittelalterlichen Bildungsideal entsprechend – neben dem Griechischen (das der Makedone wie eine Fremdsprache lernen musste) auch Latein- und Hebräischkenntnisse umfasst habe. Nach der Mitte des 12. Jahrhunderts entstanden weitere französische Gedichte, die einzelne Episoden aus Alexanders Leben (Belagerung von Tyros, Indienfeldzug, Lebensende) behandelten. Sie wurden im späten 12. Jahrhundert zur „Standardversion“ des altfranzösischen \"Roman d’Alexandre\" (auch: \"Roman d’Alixandre\") zusammengefügt, die von allen im romanischen Sprachraum verbreiteten volkssprachlichen Bearbeitungen des Stoffs die stärkste Wirkung erzielte. Dieses Epos besteht aus über 20 000 Versen, Zwölf- und Dreizehnsilbern; vom \"Roman d’Alexandre\" erhielt dieses Versmaß später die Bezeichnung Alexandriner. Der Roman schildert Alexanders Leben durch Verknüpfung von vier Gedichten unterschiedlichen Ursprungs. Dabei kommt zum Altbestand der Alexanderlegende noch eine Reihe von frei erfundenen Personen und Begebenheiten hinzu. Der Autor stellt Alexander im Stil der Chanson de geste wie einen sehr standesbewussten, ritterlichen Lehnsherrn des Mittelalters dar. Er hebt dabei besonders die Großzügigkeit seines Helden hervor und präsentiert das Ideal eines harmonischen Verhältnisses zwischen König und Vasallen. Neben epischen Partien, besonders in den Kampfschilderungen, finden sich auch stärker romanhafte und vom Phantastischen geprägte. Mehrere Dichter fügten später Ergänzungen hinzu, insbesondere die einem Publikumsbedürfnis entsprechende Darstellung der Rache für den Giftmord an Alexander. In England schrieb Thomas von Kent im späten 12. Jahrhundert einen Alexanderroman in Alexandrinern in anglonormannischer Sprache mit dem Titel \"Le roman de toute chevalerie\". Er akzeptierte im Gegensatz zu allen älteren romanhaften Bearbeitungen des Stoffs problemlos die Vorstellung, dass Alexander aus einem Ehebruch seiner Mutter hervorging, was für die früheren Autoren ein nicht akzeptabler Makel gewesen war.\n\nIm 15. Jahrhundert entstanden Prosafassungen des \"Roman d’Alexandre\". Der altfranzösische Prosa-Alexanderroman fand weite Verbreitung. Einen Höhepunkt erreichte die Alexander-Bewunderung im Herzogtum Burgund am Hof Herzog Philipps des Guten († 1467) und seines Nachfolgers, Karls des Kühnen.\n\nDie bedeutendste spanische Bearbeitung des Stoffs ist \"El libro de Alexandre\". Dieses Epos umfasst über 10 000 Verse (Alexandriner) und ist damit die umfangreichste epische Dichtung Spaniens aus dem 13. Jahrhundert. Der unbekannte Verfasser, ein vorzüglich gebildeter Geistlicher, verfolgt ein moralisches Ziel; er will dem Leser anhand der erzählten Begebenheiten die vorbildliche Tugendhaftigkeit des Helden vor Augen stellen.\n\nIn Italien entstand eine Reihe von volkssprachlichen Werken über Alexanders Lebens in Prosa und in Versen, deren Grundlage meist die lateinische \"Historia de preliis\" war. Die älteste vollständig erhaltene italienische Alexanderdichtung ist die \"Istoria Alexandri regis\" von Domenico Scolari aus der ersten Hälfte des 14. Jahrhunderts. Scolari christianisiert seinen Helden weitgehend; Alexander ist ein frommer, geradezu heiliger Wundertäter. Als Universalmonarch beglückt er die Welt durch Recht und Frieden. Im 15. Jahrhundert erreichte das Interesse an der Alexandersage in Italien seinen Höhepunkt.\n\n\nDeutsche Literatur.\nDie deutschsprachige Alexandersage und Alexanderdichtung setzte um die Mitte des 12. Jahrhunderts mit dem Alexanderlied des Pfaffen Lamprecht ein, der sich eng an Alberichs Versroman hielt. Die drei erhaltenen, später bearbeiteten Fassungen von Lamprechts Gedicht, der „Vorauer Alexander“, der „Straßburger Alexander“ und der „Basler Alexander“, setzten jedoch in der Bewertung Alexanders unterschiedliche Akzente. Im „Vorauer Alexander“ wird deutliche Kritik am König geübt. Alexander handelt zwar nach dem Willen Gottes, wird aber als hochmütig und herrschsüchtig dargestellt; die Zerstörung von Tyros wird als schweres Unrecht verurteilt, da die Tyrer als treue Untertanen des Perserkönigs nur ihre Pflicht erfüllten. Überdies erscheint er als mitleidlos, da er nicht über den Tod der vielen Gefallenen trauert. Andererseits verfügt er aber über Umsicht, die ihn seine Neigung zu jähzorniger Unbeherrschtheit überwinden lässt, womit er ein Beispiel gibt und sich von dem sehr negativ gezeichneten Dareios abhebt. Alexander wird bewusst als zwiespältige Persönlichkeit gezeichnet. Ein einfacheres Alexanderbild entwirft ein aus ritterlich-aristokratischer Sicht wertender Autor im „Straßburger Alexander“; hier wird der König als vorbildlicher Kämpfer, Feldherr und Herrscher idealisiert. Als solcher handelt er nicht eigenmächtig, sondern sucht den Rat seiner Vasallen. Er ist klug, gerecht und gütig, und seine schon in der Antike negativ bewertete Neigung zum Jähzorn wird als einigermaßen berechtigt dargestellt. Allerdings ist er nicht frei von Hochmut; zum vollkommenen Herrscher fehlt ihm die Mäßigung, die er aber in seiner letzten Lebensphase doch noch erlangt, womit er das Ideal restlos verwirklicht. Im „Basler Alexander“ dominiert ein anderes, in der mittelalterlichen Alexander-Rezeption ebenfalls zentrales Element, die Freude am Wunderbaren, Seltsamen und Exotischen. Diese Behandlung des Stoffs zielt auf das Unterhaltungsbedürfnis eines breiten, nicht mehr primär an ritterlichen Idealen orientierten spätmittelalterlichen Publikums.\n\nIm 13. Jahrhundert verfasst der Dichter Rudolf von Ems das (allerdings unfertig gebliebene) Epos \"Alexander\". Er schildert den König als vorbildlich tugendhaften Helden und ritterlichen Fürsten, der sich durch seine moralischen Qualitäten als Herrscher legitimiert. Alexander vollzieht als Werkzeug Gottes dessen Willen. Durch ihn werden die Perser, die mit ihrem Verhalten den Zorn des Allmächtigen hervorgerufen haben, gezüchtigt. Sein Handeln ist Teil der Heilsgeschichte, er kann christlichen Herrschern als Vorbild dienen. Ulrich von Etzenbach beschreibt in seinem zwischen 1271 und 1282 entstandenen Gedicht \"Alexander\" (28.000 Verse) den König nicht nur als edlen Ritter, sondern auch als überaus frommen Mann Gottes, der seine Siege seinem gottgefälligen Verhalten und Gottvertrauen verdankt; die ihm zugeschriebenen Tugenden stammen aus der Heiligendarstellung. Ulrich missbilligt allerdings einzelne Taten wie die Ermordung Parmenions; darin unterscheidet er sich von Rudolf, bei dem Alexander makellos ist und Parmenion sein Schicksal selbst verschuldet. 1352 vollendet der nur aus seinem einzigen Werk bekannte Dichter Seifrit seine Alexanderdichtung, in der er besonders die Rolle Alexanders als Weltherrscher betont und sich bemüht, von seinem Helden den gängigen Vorwurf des Hochmuts fernzuhalten.\n\nIm 14. und im 15. Jahrhundert war der Alexanderstoff in neuen Prosabearbeitungen weit verbreitet; die eine befindet sich im \"Großen Seelentrost\" (Mitte des 14. Jahrhunderts), die andere ist Johann Hartliebs \"Histori von dem grossen Alexander\", die nach der Mitte des 15. Jahrhunderts entstand. Beide dienten einem moralischen Zweck, doch ihre Verfasser gingen dabei auf völlig entgegengesetzte Weise bewertend vor. Im Großen Seelentrost bietet Alexander das abschreckende Lehrbeispiel eines durch und durch gierigen Menschen, den seine Neugier, Besitzgier und Machtgier letztlich ins Verderben führt, denn er versucht die dem Menschen gesetzten Grenzen zu überschreiten. Bei Hartlieb hingegen ist er ein Vorbild an Mannes- und Fürstentugend und überdies von einem wissenschaftlichen Erkenntnisstreben beseelt. Für mittelalterliche Verhältnisse auffallend ist die positive Wertung der Wissbegierde, eines auf die Natur gerichteten Forscherdrangs, der Alexander zugeschrieben wird.\n\nIm 15. Jahrhundert wurden auch Alexanderdramen geschaffen und aufgeführt, doch sind ihre Texte nicht erhalten.\n\nWährend die mit literarischem Anspruch gestalteten Werke Alexander in der Regel verherrlichen oder zumindest in überwiegend positivem Licht erscheinen lassen, werden im religiös-erbaulichen und moralisch belehrenden Prosaschrifttum oft negative Züge des Makedonenkönigs betont; dort wird er als abschreckendes Beispiel für Maßlosigkeit und Grausamkeit angeführt. Sein Himmelsflug dient Geistlichen wie Berthold von Regensburg als Symbol für frevelhaften Übermut. Andererseits heben bedeutende Dichter wie Walther von der Vogelweide und Hartmann von Aue Alexanders vorbildliche \"milte\" (Freigebigkeit) hervor.\n\n\nEnglische Literatur.\nTrotz des traditionell großen Interesses am Alexanderstoff in England gab es erst im Spätmittelalter einen Alexanderroman in englischer Sprache, die mittelenglische Dichtung \"Kyng Alisaunder\", die wohl aus dem frühen 14. Jahrhundert stammt. Sie schildert den König als Helden und hebt seine Großmut hervor, verschweigt aber auch nicht seine Maßlosigkeit und Unbesonnenheit. Eine Reihe von weiteren Schilderungen von Alexanders Leben fußte auf der \"Historia de preliis Alexandri Magni\", die im mittelalterlichen England beliebt war.\n\n\nByzanz und slawische Länder.\nAuch für die volkstümliche byzantinische Alexander-Rezeption bildete der Roman des Pseudo-Kallisthenes den Ausgangspunkt. Er lag zunächst in einer mittelgriechischen Prosabearbeitung aus dem 7. Jahrhundert vor. In spätbyzantinischer Zeit entstanden mehrere Neufassungen. Hier hat Alexander die Gestalt eines byzantinischen Kaisers angenommen; er ist von Gott gesandt und mit allen Ritter- und Herrschertugenden ausgestattet, wird aber nicht zum Christen gemacht, sondern dem Bereich des Alten Testaments zugeordnet. Er ist mit dem Propheten Jeremia befreundet und wird von ihm beschützt. 1388 entstand das byzantinische Alexandergedicht.\n\nDie beliebteste Szene aus der Alexandersage war in Byzanz der Himmelsflug, der in der Bildenden Kunst oft dargestellt wurde.\n\nIn den süd- und ostslawischen Literaturen wurde der Alexanderstoff stark rezipiert, wobei der Weg des Überlieferungsguts vom griechischen Alexanderroman über kirchenslawische Bearbeitungen in die Volkssprachen führte. Eine altbulgarische Fassung des Romans \"(Aleksandria)\" wurde zum Ausgangspunkt der Rezeption in russischen Chroniken. In Russland war der Alexanderroman im Hochmittelalter in mehreren Versionen verbreitet. Im 14. Jahrhundert begann eine neue Version zu dominieren, die vom byzantinischen Volksroman ausging und sich durch stark ausgeprägte Merkmale des mittelalterlichen Ritterromans auszeichnete. Besonders beliebt war die serbische Fassung („serbischer Alexander“ oder „serbische Alexandreis“), die auch in Russland Verbreitung fand und Vorlage für die spätmittelalterliche georgische Prosaübersetzung war. In Russland, der Ukraine, Bulgarien und Rumänien setzte sich dieser Typus der Alexanderlegende durch.\n\n\nArabische Literatur.\nIn der mittelalterlichen arabischsprachigen Literatur war Alexander unter dem Namen „al-Iskandar“ bekannt, da der Anfang seines Namens mit dem arabischen Artikel \"al\" verwechselt wurde. Er wurde schon in der vorislamischen Dichtung erwähnt. Folgenreich war seine Identifizierung mit der koranischen Figur des Dhū l-Qarnain („der Zweihörnige“), von dem in Sure 18 erwähnt wird, dass er einen Damm gegen Gog und Magog errichtete (Verse 83–98). Diese Identifizierung wurde von den muslimischen Gelehrten mehrheitlich, aber nicht einhellig akzeptiert. Nach heutigem Forschungsstand ist die Ableitung der Figur Dhū l-Qarnains von Alexander sowie die Herkunft des Motivs aus der altsyrischen christlichen Alexanderlegende eine gesicherte Tatsache. Die im Orient verbreitete Bezeichnung Alexanders als „zweihörnig“ taucht schon in einer spätantiken Alexanderlegende in altsyrischer Sprache auf, wo Alexander ein christlicher Herrscher ist, dem Gott zwei Hörner auf dem Kopf wachsen ließ, womit er ihm die Macht verlieh, die Königreiche der Welt zu erobern. Den ursprünglichen Anlass zur Bezeichnung „der Zweihörnige“ bot die antike bildliche Darstellung Alexanders mit Widderhörnern, die auf seine Vergöttlichung deutete. Der Gott Zeus Ammon (Amun), als dessen Sohn Alexander sich betrachtete, wurde als Widder oder widderköpfig dargestellt.\n\nIm Koran wird die Geschichte des Zweihörnigen dem Propheten geoffenbart, denn er soll sie mitteilen, wenn er danach gefragt wird. Alexander erscheint darin als frommer Diener Gottes, dem die Macht auf der Erde gegeben war und „ein Weg zu allem“. Er gelangte bis zum äußersten Westen der Welt, wo die Sonne „in einer verschlammten Quelle untergeht“, und erlangte die Herrschaft über das dort lebende Volk (hier ist ein Nachhall von Pseudo-Kallisthenes zu erkennen, der Alexander nach Italien kommen und den gesamten Westen einnehmen ließ). Dann schlug der Zweihörnige den Weg zum äußersten Osten ein und gelangte an den Ort, wo die Sonne aufgeht (daher deuteten die mittelalterlichen Koranausleger die Zweihörnigkeit meist als Zeichen für die Herrschaft über Westen und Osten). Schließlich begab er sich in eine andere Richtung und kam in eine Gegend, wo Menschen lebten, die von Angriffen zweier Völker, der Yāǧūǧ und Māǧūǧ (biblisch Gog und Magog), bedroht waren und ihn um Hilfe baten. Zum Schutz der Bedrohten baute er, ohne einen Lohn zu verlangen, zwischen zwei Berghängen einen gigantischen Wall aus Eisen, den die Angreifer nicht übersteigen oder durchbrechen konnten. Dieser Schutzwall wird bis zum Ende der Welt bestehen. – Eine altsyrische Version der Sage von Alexanders Aussperrung von Gog und Magog (in den \"Revelationes\" des Pseudo-Methodius) wurde ins Griechische und ins Lateinische übersetzt und fand in Europa viel Beachtung.\n\nAuch die voranstehende Passage der 18. Sure (Verse 59–81) scheint von der Alexanderlegende beeinflusst zu sein, obwohl in der Version des Korans Mose statt Alexander der Protagonist ist. Ein dort erzähltes Wunder (Wiederbelebung eines getrockneten Fisches) stammt anscheinend aus dem Alexanderroman; es kommt auch in einer spätantiken altsyrischen Version der Legende vor. Es ist davon auszugehen, dass der Stoff des Alexanderromans zur Entstehungszeit des Korans bereits in arabischer Übersetzung verbreitet war.\n\nDie islamische Wertschätzung für Alexander, die sich aus seiner Schilderung im Koran ergab, führte dazu, dass einige Autoren ihn zu den Propheten zählten.\n\nDie mittelalterlichen arabischsprachigen Historiker behandelten die Regierung Alexanders eher knapp. Im Gegensatz zu den europäischen christlichen Chronisten gingen bedeutende muslimische Geschichtsschreiber wie Ṭabarī, Masʿūdī, Ibn al-Aṯīr und Ibn Chaldūn auf die Alexandersage nicht oder nur nebenbei ein; sie hielten sich primär an die Überlieferung über den historischen Alexander. Ṭabarī betrachtete seine Quellen kritisch; er stützte sich insbesondere auf die Darstellung des bedeutenden Gelehrten Ibn al-Kalbī († 819/821) und stellte die Vernichtung des Perserreichs als notwendig und berechtigt dar, da Dareios tyrannisch regiert habe. Die Auseinandersetzung mit dem Legendenstoff war kein Thema der Geschichtsschreiber, sondern ein Anliegen der Theologen, die sich mit der Koranauslegung befassten. Reichhaltiges Legendenmaterial über Alexander war im muslimischen Spanien (Al-Andalus) verbreitet; dort hieß es, er habe die Iberische Halbinsel als König beherrscht und in Mérida residiert.\n\nAußerdem kommt Alexander auch in der arabischen Weisheitsliteratur vor, wo er als Gelehrter und Musikliebhaber beschrieben wird. Sehr oft taucht sein Name in Spruchsammlungen auf, wobei die Sprüche teils ihm zugeschrieben werden, teils von ihm handeln.\n\n\nPersische und türkische Literatur.\nIm Persischen wurde Alexander \"Iskandar\", \"Sikandar\" oder \"Eskandar\" genannt. In der Spätantike war im persischen Sassanidenreich eine Legende verbreitet, wonach er der persischen Religion, dem Zoroastrismus, einen schweren Schlag versetzte, indem er religiöse Schriften vernichten ließ. Daher war Alexander bei den Anhängern dieser Religion verhasst und wurde als teuflisches Wesen betrachtet. Nach der Islamisierung wirkte sich diese Sage aber im gegenteiligen Sinne aus, denn nun machte man aus Alexander einen Vorkämpfer des Monotheismus gegen heidnische Götzendiener.\n\nDer berühmte persische Dichter Firdausī († 1020) baute eine Version der Alexanderlegende in das iranische Nationalepos \"Šāhnāmeh\" ein, wobei er in manchen Einzelheiten von Pseudo-Kallisthenes abwich. Für ihn war Alexander ein „römischer Kaiser“ und Christ, der unter dem Kreuzeszeichen kämpfte; offenbar dachte er dabei an die byzantinischen Kaiser. Außerdem machte er – wie schon Ṭabarī, der persischer Abstammung war – Alexander zu einem Halbbruder des Dareios, womit er ihn für das Persertum vereinnahmte; aus der Vernichtung des Perserreichs wurde ein Bruderzwist innerhalb der iranischen Herrscherfamilie.\n\n1191 schuf der persische Dichter Nezāmi das \"Eskandar-Nāme\" („Alexander-Buch“). Sein Alexander ist völlig islamisiert; er ist ein monotheistischer Held, der den Zoroastrismus der Perser mit Feuer und Schwert ausrottet und dafür den Beifall des Dichters erhält. Er unterwirft nicht nur Indien, sondern auch China und gelangt im Westen bis nach Spanien. Wie schon bei Firdausī sucht Alexander auch Mekka auf und reinigt dort die Kaaba. Außerdem ist er auch Philosoph und ein großer Förderer der Wissenschaft; er befiehlt den Gelehrten, das Wissen aller Völker zusammenzutragen. Das \"Eskandar-Nāme\" wurde zum Vorbild für einige spätere Dichtungen ähnlicher Art.\n\nDie Handschriften der persischen Alexander-Bücher wurden trotz islamischer Bilderverbote ab dem 14. Jahrhundert mit Buchmalerei geschmückt. In Nordindien sorgten die Mogul-Kaiser des 16. Jahrhunderts für die Bebilderung solcher Bücher.\n\nIm Jahr 1390 verfasste der türkische Dichter Tāǧ ed-Dīn Ibrāhīm Aḥmedī das türkische Alexanderepos \"Iskendernāme\", die erste türkische Bearbeitung des Alexanderstoffs. Dafür bildete Nezāmis „Alexanderbuch“ die Grundlage, doch verfügte Aḥmedī auch über andere Quellen, aus denen er zusätzliches Sagenmaterial bezog. Sein Werk war im Osmanischen Reich lange berühmt und gelangte auch nach Iran und Afghanistan.\n\n\nHebräische Literatur.\nDie jüdische Alexanderrezeption war von dem Umstand geprägt, dass der Makedone schon in der Antike als Freund des jüdischen Volkes und Diener Gottes betrachtet wurde. In der mittelalterlichen hebräischen Alexanderliteratur floss Material aus unterschiedlichen Traditionen zusammen. Einerseits handelte es sich um Stoff aus dem griechischen Alexanderroman bzw. der \"Historia de preliis\", andererseits um einzelne Sagen jüdischer Herkunft (Verhalten Alexanders in Jerusalem, seine Schutzmaßnahme gegen Gog und Magog, sein Aufenthalt im irdischen Paradies und weitere Geschichten).\n\nDie hebräische Überlieferung wurde nicht nur von der griechischen und lateinischen beeinflusst, sondern wirkte auch ihrerseits auf die westeuropäische Alexandersage ein. Weit verbreitet war in der lateinischsprachigen Welt eine von Petrus Comestor eingeführte Variante der Erzählung von Gog und Magog, wonach Alexander nicht die wilden Völker Gog und Magog, sondern die zehn jüdischen Stämme aussperrte, um sie für ihre Abwendung vom wahren Gott zu bestrafen.\n\n\nÄthiopische Alexanderlegende.\nIns christliche Äthiopien gelangte der Alexanderroman auf dem Umweg über eine arabische Fassung. Der Stoff wurde für die Bedürfnisse eines geistlich orientierten Publikums stark umgestaltet. Alexander wird zu einem christlichen König, der den christlichen Glauben predigt. Er lebt keusch und ist ein Vorbild der Tugendhaftigkeit. Er stirbt wie ein Einsiedler, nachdem er sein Vermögen an die Armen verteilt hat. Durch diese besonders weitreichende Umarbeitung des Romans wird er zu einem Erbauungsbuch.\n\n\nHumanismus und Frühe Neuzeit.\nPetrarca behandelte in seinem Werk „Über berühmte Männer“ auch Alexander, wobei er sich an Curtius Rufus hielt, dessen negative Äußerungen er herausgriff; Positives verschwieg er.\n\nDie außerordentliche Bekanntheit der Legendengestalt Alexander hielt auch in der Frühen Neuzeit an. So schrieb der Chronist Johannes Aventinus († 1534), es sei „kein Herr, kein Fürst unseren Leuten, auch dem gemeinen ungelehrten Mann, so bekannt“ wie Alexander. Andererseits drangen aber in der Renaissance die Humanisten zum historischen Alexander vor und taten die Alexandersage als Märchen ab. Die Wiederentdeckung griechischer Quellen (insbesondere Arrians), die im Mittelalter unbekannt waren, ermöglichte einen neuen Zugang zur Epoche Alexanders. Schon der Portugiese Vasco da Lucena, der 1468 am Hof Karls des Kühnen von Burgund die erste französische Übersetzung der Alexanderbiografie des Curtius Rufus anfertigte, übte scharfe Kritik an der Legende, in deren Übertreibungen und Wunderglauben er eine Verdunkelung der wahren historischen Leistung Alexanders sah.\n\n1528/29 schuf der Maler Albrecht Altdorfer sein berühmtes Gemälde Die Alexanderschlacht. Charles Le Brun malte ab den frühen sechziger Jahren des 17. Jahrhunderts eine Reihe von Szenen aus Alexanders Leben für König Ludwig XIV.\n\nAuf Dichter und Romanautoren übte die Gestalt Alexanders weiterhin eine starke Faszination aus. Ab dem 17. Jahrhundert handelt es sich allerdings großenteils um Werke, deren Handlung sich – ganz im Gegensatz zur traditionellen Alexandersage – um frei erfundene erotische Verwicklungen dreht und nur noch geringe Ähnlichkeit mit dem ursprünglichen Legendenstoff aufweist.\n\nHans Sachs schrieb 1558 eine \"Tragedia von Alexandro Magno\", die in sieben Akten die ganze Geschichte Alexanders darstellt. In Frankreich verfasste Jacques de la Taille 1562 die Tragödien \"La Mort de Daire\" und \"La Mort d'Alexandre\", und Alexandre Hardy wählte dieselben Titel für zwei seiner Tragödien (\"La Mort d'Alexandre\", 1621, und \"La Mort de Daire\", 1626). Im weiteren Verlauf des 17. Jahrhunderts folgten zahlreiche Tragödien und Tragikomödien, darunter Racines \"Alexandre le Grand\" (Uraufführung 1665). Noch intensiver war die Rezeption in italienischer Sprache. Antonio Cesti komponierte die Oper \"Alessandro vincitor di se stesso\" (Uraufführung Venedig 1651), Francesco Lucio ein „dramma musicale“ \"Gl'amori di Alessandro Magno e di Rossane\" (Libretto von Giacinto Andrea Cicognini, 1651); zahlreiche Dramen, Melodramen, Opern und Ballette folgten. Unter den Opern waren besonders erfolgreich \"Alessandro Magno in Sidone\" von Marc’Antonio Ziani (1679, Libretto von Aurelio Aureli), die „tragicommedia per musica“ \"Alessandro in Sidone\" von Francesco Bartolomeo Conti (1721, Libretto: Apostolo Zeno) und das vielfach vertonte Libretto \"Alessandro nell’Indie\" von Pietro Metastasio (1729, Erstvertonung: Leonardo Vinci) sowie vor allem \"Alessandro\" von Händel (Uraufführung in London 1726, Libretto von Paolo Antonio Rolli). Gluck verwertete Elemente des Alexanderstoffs sowohl in seiner Oper \"Poro (Alessandro nell’India)\" (Uraufführung: Turin 1744, Libretto von Metastasio) als auch in dem Ballett \"Alessandro\".\n\nZu Beginn des 17. Jahrhunderts schrieb in Spanien der Dichter Lope de Vega die Tragikomödie \"Las grandezas de Alejandro\".\n\nDer englische Schriftsteller John Lyly schrieb die Komödie \"Campaspe\" (Uraufführung 1584), die auch unter dem Titel \"Alexander and Campaspe\" bekannt ist und von einem Aufenthalt Alexanders in Athen handelt. John Dryden dichtete 1692 die Ode \"Alexander’s Feast\", welche die Basis für das Libretto des 1736 vollendeten und uraufgeführten gleichnamigen Oratoriums von Georg Friedrich Händel (HWV 75) bildete.\n\nIn Griechenland wurde von 1529 bis ins frühe 20. Jahrhundert die Alexanderlegende in gedruckten Volksbüchern verbreitet, zunächst vorwiegend in Versform (\"Rimada\", 14 Drucke von 1529 bis 1805), ab dem 18. Jahrhundert meist in Prosa \"(Phyllada)\". Von insgesamt 43 Drucken der \"Phyllada\" aus dem Zeitraum von ca. 1680 bis 1926 erschienen 20 in der zweiten Hälfte des 19. Jahrhunderts.\n\n\nRezeption in Nordmazedonien.\nSeit der Unabhängigkeitserklärung der früheren jugoslawischen Teilrepublik Mazedonien, der heutigen Republik Nordmazedonien, im Jahr 1991 knüpfte der neue souveräne Staat demonstrativ an die Tradition des antiken Reichs Makedonien an und betrachtete diese als einen wesentlichen Aspekt seiner nationalen Identität. Von offizieller mazedonischer Seite wurde behauptet, es gebe eine ethnische und kulturelle Kontinuität vom antiken Makedonien zum heutigen Mazedonien. Im Rahmen solcher Traditionspflege förderten mazedonische Behörden auch auf kommunaler Ebene die Verehrung Alexanders des Großen, was sich unter anderem in der Errichtung von Alexander-Denkmälern und in der Benennung von Straßen äußert. Im Dezember 2006 wurde der Flughafen der mazedonischen Hauptstadt Skopje nach Alexander benannt \"(Aerodrom Skopje „Aleksandar Veliki“);\" dort wurde eine große Alexander-Büste aufgestellt. 2009 wurde die Errichtung einer zwölf Meter hohen Reiterstatue auf einem zehn Meter hohen Sockel im Zentrum von Skopje beschlossen, die Alexander nachempfunden war. Im Juni 2011 wurde dieser Beschluss, der in Griechenland Irritation auslöste, umgesetzt.\n\nVon griechischer Seite wird die Behauptung einer kulturellen Kontinuität zwischen den antiken Makedonen und den heutigen Staatsbürgern der Republik Mazedonien nachdrücklich zurückgewiesen. Daher erscheint auch die mazedonische Alexander-Rezeption aus griechischer Sicht als Provokation, da die gesamte Alexander-Tradition ausschließlich ein Teil des griechischen kulturellen Erbes sei.\n\nIm Februar 2018 beschloss die neue mazedonische Regierung angesichts von Fortschritten bei den Verhandlungen mit Griechenland zum mazedonischen Namensstreit, den Flughafen von Skopje und eine Autobahn, die den Namen „Alexander von Mazedonien“ trug, wieder umzubenennen.\n\n\nModerne Belletristik.\nIn der Moderne hat sich die Belletristik stärker als früher um Nähe zum historischen Alexander bemüht. Zu den bekannteren historischen Romanen aus der ersten Hälfte des 20. Jahrhunderts gehören \"Alexander in Babylon\" von Jakob Wassermann (1905), \"Alexander. Roman der Utopie\" von Klaus Mann (1929), der Alexander als gescheiterten Utopisten darstellt, und \"Iskander\" von Paul Gurk (1944). Weitere belletristische Darstellungen von Alexanders Leben stammen von Mary Renault, Roger Peyrefitte, Gisbert Haefs und Valerio Massimo Manfredi. Arno Schmidt lässt in seiner Erzählung \"Alexander oder Was ist Wahrheit\" (2005) den Ich-Erzähler Lampon eine Wandlung vom Verehrer zum Gegner Alexanders durchmachen. Iron Maiden widmete ihm den in der Metal-Szene sehr populär gewordenen Titel \"Alexander the Great\", der 1986 im Album Somewhere in Time erstmals veröffentlicht wurde.\n\n\nBeurteilung in der modernen Forschung.\nDen Ausgangspunkt der modernen wissenschaftlichen Auseinandersetzung mit Alexander bildete die 1833 erschienene „Geschichte Alexanders des Großen“ von Johann Gustav Droysen. Droysen betonte die aus seiner Sicht positiven kulturellen Folgen von Alexanders Politik einer „Völkervermischung“ statt einer bloßen makedonischen Herrschaft über unterworfene Barbaren. Er lobte die Wirtschaftspolitik, die Städtegründungen und die Förderung der Infrastruktur und meinte, auf religiösem Gebiet habe Alexanders Politik die Entstehung einer Weltreligion vorbereitet. Dieser Sichtweise war eine starke Nachwirkung beschieden. Im englischen Sprachraum war ihr Hauptvertreter im 20. Jahrhundert William W. Tarn, dessen 1948 erschienene Alexander-Biografie den Eroberer als Idealisten beschreibt, der eine zivilisatorische Mission erfüllen wollte.\n\nDieser Einschätzung, deren Grundidee schon bei Plutarch auftaucht, steht eine dezidiert negative Wertung gegenüber, welche Kernpunkte der antiken Alexanderkritik aufgreift. Die Vertreter dieser Richtung (siehe bereits die negative Charakterisierung durch Karl Julius Beloch sowie später Ernst Badian und ähnlich Fritz Schachermeyr, daran anschließend Albert B. Bosworth, Ian Worthington, Wolfgang Will) unterscheiden sich hinsichtlich der Gewichtung verschiedener Einzelaspekte. Grundsätzlich aber sehen sie in dem Eroberer Alexander primär einen Zerstörer, dessen Fähigkeiten sich auf Militärisches beschränkten. Politisch sei er an seinen Fehlern gescheitert. Er habe impulsive, irrationale Entscheidungen getroffen und sich mit den Säuberungen unter seinen Vertrauten und Offizieren schließlich in die Isolation manövriert, da er niemandem mehr vertrauen konnte.\n\nDie militärischen Leistungen Alexanders, die früher einhellige Anerkennung fanden, werden von den modernen Kritikern relativiert; so charakterisiert Badian den Rückmarsch aus Indien als eine von Alexander verschuldete militärische Katastrophe. Waldemar Heckel hingegen hob in jüngerer Zeit Alexanders strategische Fähigkeiten hervor und wandte sich zugleich gegen ein romantisierendes Alexanderbild. Vor einer überzogenen Kritik, wodurch sozusagen das Pendel von der Heldenverehrung Alexanders in das andere Extrem umzuschlagen droht, warnte etwa Frank L. Holt, der diesen Trend als „new orthodoxy“ bezeichnete.\n\nNeben diesen stark wertenden Darstellungen stehen Untersuchungen vor allem aus neuerer und neuester Zeit, deren Autoren von vornherein darauf verzichten, die Persönlichkeit Alexanders zu erfassen, ein Werturteil über sie abzugeben und seine verborgenen Motive zu erkunden (was aufgrund der Quellenlage sehr schwierig ist, worauf u.&nbsp;a. Gerhard Wirth hingewiesen hat). Diese Forscher untersuchen vielmehr Alexanders Selbstdarstellung, deren Wandel und die sich daraus ergebenden politischen Folgen.\n\n\n\n\n\n\n\n\n"}
{"id": "108", "url": "https://de.wikipedia.org/wiki?curid=108", "title": "Antike", "text": "Antike\n\nDie Antike (von ) war eine Epoche im Mittelmeerraum, die etwa von 800 v.&nbsp;Chr. bis ca. 600 n.&nbsp;Chr. reicht, wobei der Beginn teilweise noch deutlich früher angesetzt wird. Die klassische Antike unterscheidet sich von vorhergehenden und nachfolgenden Epochen durch gemeinsame und durchgängige kulturelle Traditionen. Sie umfasst die Geschichte des antiken Griechenlands, des Hellenismus und des Römischen Reichs. Insbesondere das Römische Reich vereinte den Mittelmeerraum seit dem 1. Jahrhundert n.&nbsp;Chr. politisch und kulturell.\n\nIn einem erweiterten Sinne umfasst die Antike auch die Geschichte der altorientalischen nahöstlichen Hochkulturen Ägyptens, Mesopotamiens, Assyriens, Persiens und Kleinasiens, die etwa mit dem Beginn der Schriftlichkeit um 3500 v. Chr. einsetzt. Der größere Zeitraum von etwa 3500 v.&nbsp;Chr. bis zum Ende der Antike wird bevorzugt als \"Altertum\" bezeichnet. Die darauffolgende Epoche ist das Mittelalter (mit einem breiten, regional unterschiedlichen Übergangszeitraum, siehe Spätantike und Frühmittelalter).\n\n\nZeitliche und begriffliche Abgrenzungen.\nIm Sinne der klassischen Altertumswissenschaften bezeichnet der historische Begriff \"Antike\" meist die Zeit von der allmählichen Herausbildung der griechischen Staatenwelt bis zum Ende des weströmischen Reichs im Jahr 476 bzw. bis zum Tod des oströmischen Kaisers Justinian 565. Seit den Arbeiten des belgischen Historikers Henri Pirenne wird oft auch das Jahr 632, also der Tod Mohammeds und die darauf folgende islamische Expansion, als Datum für das Ende der Antike vorgeschlagen.\n\nDer Anfang der antiken griechisch-römischen Kultur im klassischen Sinne wird im Allgemeinen mit der Entstehungszeit der homerischen Epen und dem Beginn der griechischen Kolonisation des Mittelmeerraums im 8. Jahrhundert v. Chr. angesetzt. Die Griechen verbreiteten ihre Kultur in den folgenden Jahrhunderten im gesamten Mittelmeerraum und an den Küsten seiner Nebenmeere und seit Alexander dem Großen auch im Orient und nach Zentralasien hinein. Die Römer brachten die antike Zivilisation bis nach Mittel- und Nordwesteuropa, wo sie sich seit dem Frühmittelalter zur christlich-abendländischen Kultur wandelte.\n\nJe nach Forschungsrichtung werden aber auch die minoische und mykenische Kultur von etwa 1900 bis 1100 v. Chr. sowie die so genannten „Dunklen Jahrhunderte“ 1200 bis 750 v. Chr. zur Antike gerechnet.\n\nAuch zwischen Antike, Völkerwanderung und Mittelalter lässt sich – wie bei allen Periodisierungen in der Geschichtswissenschaft – keine für alle Regionen, staatlichen und kulturellen Traditionen gültige Trennlinie ziehen. Je nach Betrachtungsweise sind unter anderem folgende Jahre als Epochengrenzen zwischen der Spätantike und dem Frühmittelalter vorgeschlagen worden:\n\nIn der neueren Forschung wird meistens ein später Zeitpunkt favorisiert (565 bzw. die Zeit um 600 n. Chr.). Generell erscheint es ohnehin sinnvoll, von einem Übergangszeitraum ab ca. 500 bis 7. Jahrhundert n. Chr. auszugehen, anstatt feste Daten zu wählen.\n\nDer Begriff Antike wurde lange Zeit räumlich mit der griechischen, hellenistischen und später römischen Welt gleichgesetzt. In diesem Sinne wurde der griechisch-römische Kulturraum von den umgebenden Räumen so abgegrenzt, wie schon antike griechische und später römische Gelehrte sich von den Regionen der „Barbaren“ abgrenzten (siehe auch Barbaricum). Griechen wie Römer betrachteten etwa die Kelten, Germanen oder Reitervölker nicht als Teil der zivilisierten Welt. Eine Sonderrolle spielte das Perserreich (siehe Achämenidenreich, Partherreich und Sassanidenreich), das kulturell hoch entwickelt war.\n\nÜber die recht enge Definition der römisch-griechischen Welt, die durch die Klassische Altertumswissenschaft geprägt wurde, geht der universalhistorische Antike-Begriff hinaus, der unter anderem von dem Historiker Eduard Meyer im 19. Jahrhundert gefordert wurde. In jüngerer Zeit wurde er von dem deutschen Althistoriker Josef Wiesehöfer wieder aufgegriffen. Die Mehrheit der heutigen Forscher ordnet jedoch den Alten Orient und das alte Ägypten zwar dem „Altertum“, nicht aber der „Antike“ zu.\n\n\nUrsprünge der antiken Kultur.\nDie Ursprünge der europäischen Antike liegen im Dunkeln. Ihre Vorgeschichte ist etwa in der Zeit von ca. 2000 bis ca. 1600 v. Chr. im Mittelhelladikum anzusiedeln. Zu Beginn dieses Zeitabschnitts – teils auch schon im letzten Abschnitt des Frühhelladikums FH III ca. 2200–2000 v. Chr. – wanderten wahrscheinlich indogermanische Stämme, von Norden kommend, in Griechenland ein. Offenbar unter dem Einfluss der minoischen Kultur auf Kreta, der ersten Hochkultur Europas, die ihre Blüte von ca. 1900 bis 1450 v. Chr. hatte, entwickelte sich auf dem Festland aus der Kultur des Mittelhelladikums die mykenische Kultur (ca. 1600 bis 1050/00 v. Chr.). Sie hatte ihren Ausgangspunkt vermutlich in der Argolis und erscheint unvermittelt mit reichen Schachtgräbern ab ca. 1600 v. Chr. Unter anderem übernahm die mykenische Kultur von der minoischen die Schrift. Die auf Kreta (unter anderem) verwendete sog. Linear A-Schrift des 17. bis 15. Jahrhunderts v. Chr. wurde zur sog. Linear B-Schrift (15. bis 12. Jahrhundert v. Chr.) weiterentwickelt. Dieser begegnet man auf zahlreichen Tontäfelchen unter anderem der Paläste in Pylos, Theben, Mykene auf dem griechischen Festland und in den zu jener Zeit mittlerweile mykenisch beherrschten Zentren Kydonia und Knossos auf Kreta.\nBekannt sind die prächtigen Zentren der mykenischen Kultur. Zu den bedeutenden Fundorten gehören Mykene, Pylos und Tiryns auf der Halbinsel Peloponnes, Orchomenos und Gla (letzteres kein Palastzentrum) in Boiotien sowie das stark mykenisch geprägte Milet in Westkleinasien. Die Zentren hatten Oberstädte (Akropolen), Burgen genannt, die im 13. Jahrhundert v. Chr. in einigen Fällen stark befestigt bzw. deren Befestigungen stark ausgebaut wurden (Mykene, Tiryns, Athen). Reiche Kuppelgräber, feine, teils reich bemalte Keramik, kunstvolle Gold-, Silber- und Fayence-Arbeiten usw. zeugen vom Reichtum und von der Spezialisierung des Wirtschaftssystems, das in Teilen Griechenlands ab ca. 1400 v. Chr. von mächtigen Palastzenten, die größere Regionen beherrschten, zentral gesteuert wurde (so in Böotien, Attika, Messenien und in der Argolis; s. dazu auch Mykenische Palastzeit). Intensive Handelskontakte wurden mit dem Nahen Osten, Assyrien und Ägypten gepflegt. Mykenische Keramik war in weiten Teilen des Mittelmeergebiets beliebt; möglicherweise ließen sich in manchen Siedlungen Süditaliens (Roca Vecchia, Punta Meliso, Scoglio del Tonno) sogar Handwerker nieder.\n\nEtwa für den Zeitraum 1200 bis 750 v. Chr. setzt man traditionell das \"Dunkle Zeitalter\" an, aus dem vergleichsweise wenig überliefert ist. Zu Beginn dieser Phase wurden viele der Zentren des griechischen Festlands zerstört, womit die Grundlage der Palastkultur unterging. Die mykenische Kultur bestand jedoch noch etwa 150&nbsp;Jahre weiter, erlebte in einigen Regionen ab Mitte des 12. Jahrhunderts sogar eine gewisse Nachblüte, bevor der Übergang in die sogenannte Protogeometrische Periode (ca. 1050/00–900 v. Chr.) erfolgte. Ungefähr zur gleichen Zeit, als sich um 1200 v. Chr. in Griechenland – und auch an anderen Regionen des östlichen Mittelmeerraums (s. auch Ende des Hethiterreichs, Seevölker) – Zerstörungen und Umwälzungen ereigneten, entstanden auf Zypern und einigen Orten Südkleinasiens (z. B. Tarsus und Mersin) mykenisch geprägte Siedlungen. Westhandel, speziell mit Italien und Sardinien, wurde auch im 12. Jahrhundert v. Chr. weiterhin betrieben, teilweise auch noch im 11. Jahrhundert v. Chr. Der Überlieferung nach setzte ca. 1050 v. Chr. die sehr umstrittene \"Ionische Wanderung\" ein, in deren Verlauf die Einwohner des griechischen Festlandes die Inseln der Ägäis und die Westküste Kleinasiens kolonisierten. Auf dem griechischen Festland bietet sich ein diffuses Bild: Wenige Siedlungen wurden bisher entdeckt und die meisten machen einen – im Vergleich zur mykenischen Zeit – ärmlichen Eindruck. Ganz anders hingegen Lefkandi auf Euböa: dort wurden neben einer Siedlung mit einem großen Gebäude des Fürsten von Lefkandi Gräber gefunden, die sehr reich ausgestattet waren.\n\nDas Dunkle Zeitalter hellt sich in den letzten Jahrzehnten – dank vieler neuer Funde, vor allem, aber nicht nur, aus der mykenischen Spätphase des 12./11. Jahrhunderts v. Chr. – immer mehr auf. Nach Annahme der Homer-Forschung spiegeln unterschiedliche Passagen der Ilias die Verhältnisse dieser Zeit wider. Sie war offenbar auch für die Entwicklung der griechischen Gesellschaft zur Polis hin wichtig. Ab dem 8.&nbsp;Jahrhundert waren die Kontakte zum Vorderen Orient wieder sehr intensiv, und es entstanden Handelsstationen auf Zypern (Kition) und in Syrien (Al Mina). Vermutlich bereits im späten 9. Jahrhundert v. Chr. hat man von den Phöniziern das Alphabet vermittelt bekommen.\n\n\nGriechenland und die hellenische Welt.\n\nAnfänge des klassischen Griechenlands.\nMit dem so genannten archaischen Zeitalter begann im frühen 8. Jahrhundert v. Chr. die eigentliche Antike. Seit dem Jahr 776 v. Chr. ist die Siegerliste der Olympischen Spiele überliefert. Von etwa 770 bis 540 v. Chr. breiteten sich die Griechen während der Großen Kolonisation im westlichen Mittelmeer (vor allem Sizilien und Unteritalien, siehe auch Magna Graecia, und bis Marseille), an der nördlichen Ägäis und am Schwarzen Meer aus. In Kleinasien waren Griechen bereits vorher ansässig. In dieser Zeit (etwa zwischen 750 und 650 v. Chr.) wurden vermutlich auch die Homerischen Epen (\"Ilias\" und \"Odyssee\") schriftlich fixiert, die ältesten Literaturdenkmäler des Abendlands. Die ältesten tatsächlich erhaltenen Papyrusfragmente dieser Texte stammen aus dem 3. Jahrhundert v. Chr., die ältesten Codices mit längeren Textpassagen tauchen im Mittelalter (ca. 10. Jahrhundert n. Chr.) auf, wie generell der Großteil der erhaltenen antiken Literatur vor allem in mittelalterlichen Handschriften überliefert ist. Hesiod wirkte ebenfalls etwa in der Zeit um 700 v. Chr.\n\n\nEntstehung der Polis.\nDie klassische Periode war eine Zeit großer kultureller und wissenschaftlicher Entfaltung. Zugleich bildete sich das System der griechischen Stadtstaaten, der Poleis, heraus, wobei diese in der Mehrzahl nur eine sehr kleine Bevölkerung umfassten. Der werdende Militärstaat Sparta im Süden der Peloponnes unterwarf zwischen 720 und 600 v. Chr. Messenien und kontrollierte damit den gesamten südwestlichen Teil der Halbinsel. Die Stadt mit ihrer oligarchischen Verfassung kann als das erste Beispiel für die fortan herrschende Polis-Struktur gelten.\n\nAuch in vielen anderen griechischen Stadtstaaten regelten Verfassungen das Zusammenleben der Bürger, aber auch die Tyrannis, wie sie um 650 v. Chr. beispielsweise in Korinth und Megara bestand, war keine Seltenheit. In Athen bildete sich unter wechselnden Voraussetzungen schließlich ein demokratisches System heraus. Nach den Gesetzgebungen Drakons (621 v. Chr.) und Solons (594/593 v. Chr.) gelang es Peisistratos und seinen Söhnen etwa zwischen 561 und 510 v. Chr. zwar noch einmal, eine Tyrannis zu errichten. Bis 501 v. Chr. brachten die Reformen des Kleisthenes von Athen aber den Durchbruch für die Attische Demokratie.\n\n\nBlütezeit Athens.\nMit Athens Unterstützung der kleinasiatischen Griechenstädte im Ionischen Aufstand um 500 v. Chr. begann ein annähernd zweihundertjähriger Konflikt mit dem Perserreich, zunächst in Gestalt der drei Perserkriege, die der Historiker Herodot, der „Vater der Geschichtsschreibung“ (mit ihm lässt man traditionell die griechische Geschichtsschreibung beginnen, vgl. \"Liste der griechischsprachigen Geschichtsschreiber der Antike\"), in seinen \"Historien\" geschildert hat, wenngleich nicht immer zuverlässig. Als die Perser zu einer Strafexpedition in Griechenland einfielen, wurden sie 490 v. Chr. von den Athenern in der Schlacht bei Marathon besiegt. Zehn Jahre später unterlag der persische Großkönig Xerxes I. der athenischen Flotte unter Themistokles in der Schlacht von Salamis und 479 v. Chr. den vereinigten Heeren der griechischen Poleis in der Schlacht von Plataiai. Die Perser waren vorerst zurückgedrängt, die griechischen Stadtstaaten in Kleinasien aus der Abhängigkeit befreit.\nNach der erfolgreichen Verteidigung und mit der Gründung des Attischen Seebunds 477 v. Chr. unter der auf die eigene Seemacht gestützte Vorherrschaft Athens setzte eine etwa 50-jährige Blütezeit der Stadt (die Pentekontaetie) ein, die bis zum Ausbruch des Peloponnesischen Krieges 431 v. Chr. (bzw. bis zum Tod des leitenden Staatsmannes Perikles im Jahr 429 v. Chr.) reichte. Die Akropolis mit dem Parthenon&shy;tempel wurde damals unter der Regie des Phidias zum glanzvoll-repräsentativen Zentrum der Seemacht Athen ausgebaut. Die klassischen Tragödien von Aischylos, Sophokles und Euripides kamen – meist im Rahmen festlicher Dichterwettbewerbe – im Theater zur Aufführung. Kaufleute und Gewerbetreibende, Künstler und Gelehrte zog die Metropole an. Auf der Agora wirkte neben den Sophisten der Philosoph Sokrates auf seine Mitbürger ein, dessen Lehren Platon später zu einem Werk von herausragender philosophie&shy;geschichtlicher Bedeutung verarbeitete. Athen mit seinen zu gleichberechtigter politischer Mitwirkung gelangten (männlichen) Vollbürgern beanspruchte nunmehr, die „Schule von Hellas“, zu sein. Seine durchaus auch aggressive äußere Machtentfaltung in und mit dem Attischen Seebund führte allerdings schon während der Pentekontaetie zu Spannungen, vor allem gegenüber der konkurrierenden griechischen Großmacht Sparta.\n\n\nKampf um die Hegemonie.\nDie zunehmende Rivalität zwischen der Seemacht Athen und der Landmacht Sparta mündete 431 v. Chr. in den fast 30&nbsp;Jahre währenden Peloponnesischen Krieg, den die zeitgenössischen Historiker Thukydides und (im Anschluss an Thukydides) Xenophon eindringlich beschrieben haben. Der sehr wechselhaft verlaufende und mit einer als beispiellos empfundenen Brutalität geführte Konflikt endete, auch auf Grund der Unterstützung Spartas durch das Perserreich, 404 v. Chr. mit der vollständigen Niederlage Athens und mit der Errichtung einer zeitweiligen spartanischen Hegemonie über Griechenland.\n\nIn der ersten Hälfte des 4. Jahrhunderts v. Chr. führten die griechischen Städte einen fast permanenten Krieg gegeneinander und in wechselnden Koalitionen, auch unter fortwährender Einmischung der Perserkönige. Die Sehnsucht nach einem Allgemeinen Frieden wurde auch zu propagandistischen Zwecken eingesetzt (Königsfrieden von 386 v. Chr.). 371 v. Chr. löst Theben unter Epaminondas nach der Schlacht bei Leuktra Sparta als Hegemon ab. Doch auch Thebens Vorherrschaft bestand nur bis rund 362 v. Chr. und endete mit dem Tod Epaminondas.\n\nInsgesamt schwächte der Peloponnesische Krieg die griechischen Polis so stark, dass Philipp II. von Makedonien dem andauernden Machtkampf ein Ende setzen konnte, indem er Griechenland gewaltsam mit seinem hervorragend geschulten Heer einigte. Der von Athenern wie Demosthenes als nicht-griechischer Barbar betrachtete König errang mit seinem geschulten Heer in der Schlacht von Chaironeia 338 v. Chr. die Hegemonie über Hellas, die im Jahr darauf im Korinthischen Bund bekräftigt wurde.\n\nAuf Sizilien behauptete sich derweil das mächtige Syrakus gegenüber der Handelsrepublik Karthago, welche mit den griechischen Poleis \"(Westgriechen)\" seit dem frühen 5. Jahrhundert v. Chr. im Konflikt lag. Auf Sizilien hielt sich zudem, im Gegensatz zum Mutterland, in vielen Städten die Tyrannis als Regierungsform (Dionysios I. von Syrakus, Agathokles von Syrakus und andere).\n\n\nHellenistische Zeit (336 bis 30 v. Chr.).\nNach der Ermordung Philipps 336 v. Chr. führte sein Sohn Alexander der Große ein griechisch-makedonisches Heer nach Asien und eroberte in wenigen Jahren mit dem Perserreich ein Weltreich. Der Alexanderzug bahnte der griechischen Kultur im ganzen damals bekannten Orient den Weg, von Ägypten über Mesopotamien und Persien bis zu den Grenzen Indiens und Turkestans. Nach Alexanders Tod 323 v. Chr. in Babylon teilten seine Nachfolger, die Diadochen, in lange währenden Kriegen das Reich unter sich auf. In allen Teilreichen war die Kultur in den folgenden Jahrhunderten von einer gegenseitigen Durchdringung von griechischen und indigenen Elementen geprägt.\n\nDas Zeitalter des Hellenismus kennzeichnet ein nahezu ständiger Kampf der drei Großmächte (Ptolemäer, Seleukiden und Antigoniden) um die Vorherrschaft. Dennoch wuchs die Bevölkerung im gesamten Mittelmeerraum stetig und ermöglichte so das Wachstum größerer Städte und Metropolen mit Einwohnern über 100.000 Menschen. Auch breitete sich in dieser Zeit der Fernhandel (bis hin nach China) und die Güterproduktion für große städtische Märkte aus. Verschiedene Wissenschaften blühten auf, bspw. in Alexandria. Zu Beginn des 2. Jahrhunderts v. Chr. tauchte erstmals Rom als bedeutende Macht in Griechenland auf und dehnte nach und nach seinen Einfluss aus. 146 v. Chr. unterstellte das Römische Reich die Mitglieder des unterlegenen Achaiischen Bundes faktisch der neuen Provinz \"Macedonia\"; Korinth als führende Polis wurde zerstört. Doch blieben viele Poleis wie Athen und Sparta zumindest vorerst formell unabhängig.\n\nBald darauf folgte der Erwerb Pergamons durch Rom und 64/63 v. Chr. die Beseitigung der Überreste des Seleukidenreiches. Als letzter Nachfolgestaat des Alexanderreichs wurde im Jahre 30 v. Chr. das ptolemäische Ägypten, dessen letzte Herrscherin Kleopatra VII. war, ins Römische Reich eingegliedert. Damit war die hellenistische Staatenwelt als machtpolitischer Faktor ausgelöscht. 27 v. Chr. wurde Griechenland zur Provinz \"Achaea\". Die griechische Kultur lebte jedoch im Römischen Reich sowie später im Byzantinischen Reich noch lange fort, und die griechische Sprache blieb die \"lingua franca\" im Osten des Mittelmeerraumes.\n\n\nRömisches Reich.\nNach den Griechen wurden die Römer zu den zweiten Trägern und Vermittlern der antiken Kultur und prägten diese für mehrere hundert Jahre. Je weiter sie als Eroberer in außeritalische Länder vordrangen, desto stärker ließen sie sich von deren Kultur inspirieren und beeinflussen. Sie adaptierten teilweise lokale Gebräuche. Literatur, Philosophie, Kunst, Architektur und Alltagskultur der Griechen und der Länder der Levante, Waffentechniken der Gallier oder Germanen und religiöse Einflüsse aus Ägypten wurden von den Römern aufgenommen. Nicht zuletzt durch die kulturelle Ausstrahlung und Heterogenität der Stadt Rom, die sich in der römischen Kaiserzeit zur Millionenstadt entwickelte, wurden solche Einflüsse im Imperium verbreitet.\n\n\nUrsprünge Roms.\nRom, der Legende nach 753 v. Chr. gegründet, entstand neueren Forschungen zufolge erst gegen Ende des 7. Jahrhunderts v. Chr. aus dem Zusammenschluss mehrerer dörflicher Siedlungen an einer Furt am Unterlauf des Tibers. Politisch und kulturell stand Rom lange unter etruskischem Einfluss. Die Etrusker wiederum unterhielten schon früh Kontakt mit griechischen Kolonisten.\n\n\nRömische Republik (ca. 500 bis 27 v. Chr.).\nUm 500 v. Chr. befreiten sich die Römer vom etruskischen Stadtkönigtum und bildeten im Verlauf der folgenden Jahrzehnte eine republikanische Regierungsform aus. In den Zwölftafelgesetzen, die wohl um 450 v. Chr. entstanden, wurden die ersten zivil-, straf- und prozessrechtlichen Normen des römischen Rechts festgehalten. Die Verfassung sah von da an ein Zusammenwirken der drei Institutionen Senat, Magistratur und Volksversammlung vor, die sich in ihrer Macht theoretisch gegenseitig beschränkten. Die offizielle Bezeichnung der Republik lautete \"S.P.Q.R.\" für \"Senatus Populusque Romanus\" (dt.: Senat und Volk von Rom). Machtpolitisch dominierte der Senat, der sich anfangs aus Angehörigen der adligen Familien, der Patrizier zusammensetzte, bevor sich im 4. Jahrhundert eine neue, meritokratisch legitimierte Führungsschicht entwickelte, die Nobilität. Aus ihr gingen auch die Konsuln hervor, die beiden auf ein Jahr gewählten obersten Magistrate der Republik. Das wichtigste nur den Plebejern zugängliche Amt war das des Volkstribunen, der ein Veto&shy;recht gegen Senatsbeschlüsse besaß. Seit 287 v. Chr. besaßen die Beschlüsse der von den Tribunen geleiteten plebejischen Volksversammlung Gesetzeskraft.\nMit der Legion entwickelten die Römer eine effektive Streitmacht. Bis zum Jahr 272 v. Chr. unterwarfen sie ganz Italien südlich der Poebene. Mit den Punischen Kriegen gegen die Seemacht Karthago im 3. und 2. Jahrhundert v. Chr. begann der Aufstieg Roms zur antiken Weltmacht, die für die folgenden Jahrhunderte die gesamte Mittelmeer&shy;welt beherrschen sollte. Nach 200 v. Chr. nahm Rom zunehmend Einfluss auf die Politik der hellenistischen Großmächte und wurde zur Protektoratsmacht im östlichen Mittelmeerraum. 148 v. Chr. wurde das Makedonien der Antigoniden, 63 v. Chr. das Reich der Seleukiden, und schließlich 30 v. Chr. das Ägypten der Ptolemäer römische Provinz.\n\nDie Römische Republik ermöglichte durch die Herstellung von innerem Frieden ein weiteres, kontinuierliches Bevölkerungswachstum, auch durch die ständige Neugründung von Kolonien in eroberten Ländern. Durch die Ansiedlung von Veteranen aus den Legionen vorheriger Kriege konnte die Republik zudem einen verlässlichen Einfluss in diesen Ländern gewinnen und gleichzeitig mit einem stetigen Bevölkerungszuwachs neue Gebiete kultivieren. Handel und Verkehr konnten auch dank der Römerstraßen zunehmen, welche zunächst häufig aus militärischen Gründen angelegt wurden und die wachsenden Reichsstädte und Kolonien miteinander verbanden. Entlang der Straßen entwickelten sich Streckenposten und Marktflecken zu Städten. Mit diesen infrastrukturellen Neuerungen ging im Reich ein Wachstum der wirtschaftlichen Produktion und somit auch der verfügbaren Steuermittel einher.\n\nMit dem Wachstum der Republik an Größe, Macht und Wohlstand kam es jedoch im Inneren zu einer Reihe von Krisen, da die Ungleichheit innerhalb der Oberschicht wuchs. Die Nobilität begann, an Integrationskraft zu verlieren, und die Rivalität innerhalb der Führungsschicht eskalierte. Den Optimaten, die an der Vorherrschaft des Senats festhielten, standen die Popularen gegenüber, die versuchten, sich mit Hilfe der Volksversammlung gegen ihre Rivalen durchzusetzen. In der Epoche der Bürgerkriege erreichte diese Krise der Römischen Republik ihren Höhepunkt, und es zeichnete sich ab, dass die Republik als Staatsform die Erfolge nicht mehr meistern konnte, die sie gezeitigt hatte: So wurde der Prinzipat möglich, also die Umwandlung der Republik in eine Alleinherrschaft mit republikanischer Fassade. Bereits der populare Politiker Gaius Iulius Caesar hatte als Diktator auf Lebenszeit \"(dictator perpetuus)\" eine quasi-monarchische Stellung erlangt. Als erster römischer Kaiser gilt jedoch sein Großneffe und Erbe Augustus, dem es gelang, mit dem Prinzipat eine dauerhafte monarchische Staatsordnung an die Stelle der zerstörten Republik zu setzen, wobei jedoch die entmachteten Staatsorgane der Republik, z.&nbsp;B. der Senat, noch sehr lange fortbestanden.\n\n\nPrinzipat (27 v. Chr. bis 284 n. Chr.).\nDas von Augustus errichtete Kaisertum (Prinzipat) wurde von ihm und seinem Nachfolger Tiberius für rund 60&nbsp;Jahre sicher geführt. Augustus bewahrte noch bewusst eine republikanische Fassade, während unter Tiberius das Kaisertum zur Normalität wurde. Unter Caligula, Claudius und Nero traten jedoch zeitweilig Zerfallserscheinungen auf. Nach dem Krisenjahr 68/69 (Vierkaiserjahr) traten die Flavier (Vespasian, Titus, Domitian) die Regierung an, die sowohl außen- als auch innenpolitisch insgesamt recht erfolgreich herrschten. Nach der Ermordung Domitians, der 96 einer Verschwörung zum Opfer fiel, folgte eine weitere kurze Krise des Herrschaftssystems, die jedoch unter den so genannten Adoptivkaisern weitgehend behoben werden konnte.\n\nDas Imperium erlebte seine größte Blüte und Ausdehnung dann auch unter ebendiesen „Adoptivkaisern“ (das Kaisertum war auch weiterhin formal nicht erblich) in der ersten Hälfte des 2. Jahrhunderts: Einer Expansion unter Trajan (vor allem im Balkanraum und im Osten gegen das Partherreich) folgte eine Rücknahme und Sicherung der Grenzen unter Hadrian. Bald nach der Mitte des 2. Jahrhunderts n. Chr. wuchs jedoch der Druck auf die ausgedehnten Reichsgrenzen. Im Norden und Nordosten bedrängten die Germanen, im Osten die Parther (die sich trotz mancher Niederlage behaupten konnten) das Reich. Mark Aurel, der „Philosophenkaiser“ im Geiste der Stoa, sah sich bald nach Übernahme der Herrschaft nahezu ständig zur kriegerischen Verteidigung der Reichsgrenzen genötigt. Mit seinem Tod endete 180 n. Chr. ein als Blütezeit betrachtetes Zeitalter des Imperiums.\n\nNach dem schwachen Commodus, der 192 ermordet wurde, stabilisierten die Kaiser aus dem Hause der Severer, hervorzuheben ist besonders Septimius Severus, die Grenzen wenigstens teilweise. Kaiser Caracalla gewährte 212 mit der Constitutio Antoniniana allen freien Reichsbürgern das Bürgerrecht. Nach der Ermordung des Severus Alexander 235 kam es jedoch unter den so genannten Soldatenkaisern zur Reichskrise des 3. Jahrhunderts, die aber erst um 260 ihren Höhepunkt erreichte. Dieser Zeitraum war geprägt von raschen Regierungswechseln, zeitweiligen und regional unterschiedlichen ökonomischen Problemen, zentrifugalen Tendenzen im Inneren (zeitweilige Abspaltung des \"Imperium Galliarum\"; Verlust mehrerer Provinzen an Palmyra) und dem stetig wachsenden Druck auf die Grenzen. Neben den verschiedenen Germanenstämmen (wie den Alamannen und Goten), übte nun vor allem das Sassanidenreich im Osten einen enormen Druck aus: Nach dem Sturz des letzten Partherkönigs im Jahr 224 (bzw. 226), erneuerten die Sassaniden das Perserreich und erwiesen sich in der Regel als den Römern gleichwertige Gegner, wenngleich auch sie mit einer gefährdeten Grenze konfrontiert waren (im , siehe Iranische Hunnen). Die Zeit der Soldatenkaiser wird allerdings in der neueren Forschung keineswegs mehr als eine reine Krisenzeit begriffen, sondern vielmehr als eine (wenngleich teils von Krisensymptomen begleiteten) Transformationsphase.\n\nSpätantike (284 bis 565/632 n. Chr.).\nMit der Einführung der Tetrarchie (293) und zahlreichen inneren Reformen gelang es Kaiser Diokletian (seit 284 Kaiser) gegen Ende des 3. Jahrhunderts noch einmal, das Reich zu stabilisieren. Diese Zeit der beginnenden Spätantike ist gekennzeichnet von Umbrüchen, die zum Teil eine Abkehr von bis dahin wesentlichen Bestandteilen der antiken Kultur darstellten. Dazu gehört vor allem die von Kaiser Konstantin I. initiierte Anerkennung und Privilegierung des Christentums, das unter Diokletian noch verfolgt worden war. Die Hinwendung zu dem neuen Glauben ging schließlich mit der Ablehnung des religiösen Pluralismus der Antike einher. Ein letzter Versuch, die alten Kulte durch die Verbindung mit neuplatonischem Gedankengut wieder zu beleben, scheiterte mit dem Tod Kaiser Julians im Jahr 363; alle nachfolgenden Kaiser waren Christen. Teilweise stießen auch bestimmte Formen der Philosophie auf Ablehnung, wenngleich das Christentum nun selbst stark von der griechischen Philosophie geprägt wurde und zwischen 300 und 600 eine massive Transformation durchlief, bspw. mit dem Ersten Konzil von Nicäa. Die Platonische Akademie in Athen, oft als „Hort des Heidentums“ bezeichnet, wurde 529 geschlossen, während die bereits christianisierte Schule von Alexandria noch bis zum Beginn des 7. Jahrhunderts bestehen blieb.\n\nKaiser Valentinian I. festigte den Westen des Reiches, doch kam es 378 unter seinem Bruder Valens zur Niederlage von Adrianopel und zu einer neuen Krise. In diesem Zusammenhang gehört das Auftauchen der Hunnen (nur eines von zahlreichen Reitervölkern aus der eurasischen Steppenzone, die teils eine wichtige Rolle spielten) und der Beginn der sogenannten Völkerwanderung. Kaiser Theodosius I. wiederum konnte den Osten des Reiches stabilisieren und war zugleich der letzte Kaiser, der \"de facto\" über das gesamte \"Imperium Romanum\" herrschte. Er erklärte das Christentum schließlich 392 zur Staatsreligion und verbot alle heidnischen Kulte wie die Olympischen Spiele. Allerdings lassen sich noch bis mindestens in das 6. Jahrhundert hinein bedeutende heidnische Minderheiten auf dem Boden des Imperiums nachweisen.\nNach der faktisch endgültigen Teilung des Reiches unter den beiden Söhnen des Theodosius 395 erwies sich letztlich nur das von Konstantinopel, dem früheren Byzantion, aus regierte Oströmische Reich auf die Dauer eines weiteren Jahrtausends als lebensfähig. Es bewahrte viele antike Traditionen; unter anderem blieb das Lateinische in dem überwiegend griechischsprachigen Reich noch bis ins 7. Jahrhundert Amtssprache. Das so genannte Weströmische Reich hingegen zerbrach aufgrund endloser innerer Kriege, gepaart mit äußerem Druck. Germanische Kriegerverbände traten an die Stelle der kollabierenden Reichsregierung und ergriffen, zunächst als \"foederati\", seit dem 5. Jahrhundert direkt Besitz von weströmischen Provinzen. Ihre Anführer traten hier oft an die Stelle der römischen Autoritäten. Rom selbst wurde 410 von den Westgoten und 455 von den Vandalen geplündert, von der Millionenstadt der hohen Kaiserzeit schrumpfte sie auf schätzungsweise 200.000 Einwohner zum Ende des 5. Jahrhunderts.\nDie Spätantike sah auch das langsame Verschwinden der klassisch-antiken Stadt (\"polis\" bzw. \"civitas\"). In der Forschung ist umstritten, ob es sich hierbei um einen Niedergang oder eher um einen Wandel handelt – diese Frage stellt sich auch für viele andere Aspekte der Epoche (z.&nbsp;B. im wirtschaftlichen Bereich, wobei viele Provinzen weiterhin aufblühten). Im Westen (das Ostreich war davon nicht betroffen und durchlief erst im 7. Jahrhundert eine Krisenzeit, siehe unten) lösten sich im 5. Jahrhundert zunehmend die politischen Strukturen auf, während das reguläre Heer (zumindest nach Ansicht der älteren Forschung) immer stärker „barbarisiert“ wurde und die Bedeutung der nichtrömischen \"foederati\" besonders im Westen immer mehr zunahm. Die geringer werdenden Steuereinnahmen durch den Verlust von Provinzen und Steuermitteln führten dazu, dass die Regierung in Ravenna immer hilfloser wurde; die kaiserliche Autorität schwand dahin, während die eigentliche Macht nun meist bei hohen Militärs wie Aetius oder Ricimer lag, die gegeneinander oft blutige Bürgerkriege führten und das Westreich so weiter schwächten.\n\n476 setzte der General Odoaker, der Kommandeur der föderierten Truppen in Italien, dann den letzten Westkaiser Romulus Augustulus ab, da dieser überflüssig geworden sei, und unterstellte sich der nominellen Oberherrschaft des oströmischen Kaisers. Die Geschichtswissenschaft sah in diesem von den Zeitgenossen nur wenig beachteten Akt früher oft das Ende der Antike. Heute wird dagegen auch das 6. Jahrhundert noch zur Antike gezählt, da vor allem im Osten römisch-antike Strukturen fortbestanden und dem oströmischen Kaiser Justinian (527–565) für kurze Zeit noch einmal eine Rückeroberung großer Teile des Westreiches gelang. Dass diese letztlich dennoch scheiterte, hatte auch mit dem Druck zu tun, den die Sassaniden seit 540 erneut auf die Ostgrenze des Reiches ausübten (siehe auch Römisch-Persische Kriege und Herakleios). Im Oströmischen Reich lebten antike Kultur und Geisteswelt zwar noch bis weit ins Mittelalter fort. Die islamische Expansion des 7. Jahrhunderts führte allerdings auch hier zu erheblichen Veränderungen und gilt als der entscheidende Einschnitt, der das Ostrom der Spätantike vom Byzantinischen Reich des Mittelalters trennt.\n\n\nBedeutung und Nachwirken der Antike.\nAntike Traditionen hatten starke und prägende Auswirkungen auf den weiteren Verlauf der Weltgeschichte, insbesondere auf die Entwicklung der westlichen Welt, die in der Antike ihre Wurzeln hat. Neuzeitliche Aufklärer, Philosophen, Staatstheoretiker, Wissenschaftler, Künstler und andere knüpften immer wieder an die Ionische Naturphilosophie, die attische Demokratie, das römische Recht, den religiösen Pluralismus, das antike Schönheitsideal und andere Hinterlassenschaften der Antike an.\n\nAntike Traditionen gerieten auch im Mittelalter nie völlig in Vergessenheit. In den Klöstern des Abendlandes wurde umfangreiches antikes Schriftgut bewahrt. Auch die Romidee blieb im Heiligen Römischen Reich lebendig. Im 8. Jahrhundert kam es zur ersten, sogenannten Karolingischen Renaissance. Auch byzantinische und arabische Gelehrte stützten sich auf antikes Wissen und gaben es indirekt an das mittelalterliche Europa weiter.\n\nAls man im Italien des 15. Jahrhunderts die – meist römischen – Überreste der Antike neu zu schätzen lernte und in der Kunst nachahmte, bezeichnete man dies als \"Renaissance\". Die \"Wiedergeburt\" der Antike und des antiken Geistes setzte der jahrhundertelangen Dominanz religiösen Denkens in Europa ein Ende und mündete schließlich in das Zeitalter der europäischen Aufklärung und in die Moderne. Fast alle Ideen der neuzeitlichen Aufklärung haben antike Vorläufer. Ohne griechische Wissenschaft und Philosophie, ohne die damals entstandenen politischen Ideen, ohne das römische Recht, ohne Architektur und Kunst der Griechen und Römer wäre die westliche Kultur der Neuzeit undenkbar. \n\nSo trat infolge der Arbeiten von Johann Joachim Winckelmann seit dem 18. Jahrhundert die „klassische“ griechische Kunst – oder vielmehr das, was man idealisierend für diese hielt – zunehmend ins Zentrum des Interesses. Im 19. Jahrhundert sprach man im Zusammenhang mit den Arbeiten von Architekten und Künstlern wie Karl Friedrich Schinkel, Leo von Klenze und Bertel Thorvaldsen von einer Renaissance der griechischen Antike, heute vom Neuhumanismus.\n\nErst nach dem Zweiten Weltkrieg verlor die griechisch-römische Zivilisation zunehmend die Vorbildfunktion, die man ihr in Europa und Nordamerika jahrhundertelang zugesprochen hatte. Ein entscheidender Einschnitt war hier das Verschwinden des griechischen und stark auch des lateinischen Unterrichtsfaches von den Sekundarschulen. Ein weiterer Aspekt war, dass in der ersten Hälfte des 20. Jahrhunderts Elemente der antiken Tradition von Anhängern totalitärer Ideologien willkürlich aufgegriffen und so zweckentfremdet wurden. Der Führerkult des faschistischen Regimes in Italien griff direkt auf das antike Rom zurück und knüpfte (nach dem Verständnis des Regimes) an den Caesarenkult an, wobei bereits der Terminus \"fascismo\" vom lateinischen Begriff \"fasces\" abgeleitet ist. Benito Mussolini wurde als Nachfolger des Augustus in eine Reihe mit den römischen Caesaren gestellt, und es wurde eine „Wiedererrichtung“ des antiken Römischen Reiches angestrebt. Auch das NS-Regime in Deutschland orientierte sich teils an antiken Vorbildern, so etwa im Zusammenhang mit der ideologisch begründeten Lobpreisung Spartas.\n\nDer Bedeutungsverlust nach dem Ende des Zweiten Weltkrieges hat für die Altertumswissenschaften allerdings immerhin den Vorteil, dass nun ein unverstellterer, neutraler Blick auf die Antike leichter möglich ist.\n\nBis heute erhaltene Zeugnisse der Antike sind – neben überlieferten Texten philosophischer, literarischer oder historischer Natur – zahlreiche Objekte der griechischen und römischen Kunst: von großen Skulpturen bis zur Kleinkunst, Töpferei, Münzen etc. Wichtige Antikensammlungen befinden sich in Rom, Athen, Neapel, Paris, London, München, Sankt Petersburg, Wien und Berlin. Für die Kenntnis des antiken Alltags sind vor allem archäologische Ausgrabungen wie die in Pompeji, Olympia, Delphi oder Pergamon von Bedeutung.\n\n\nQuellenlage.\nDer Großteil der antiken Literatur (und damit auch der Geschichtsschreibung) ist nicht erhalten, sodass unser Wissen über die Antike durch die Überlieferungslage beeinflusst wird (siehe auch Antike Geschichtsschreibung und hinsichtlich der griechischen Geschichtsschreibung die Liste der griechischsprachigen Geschichtsschreiber der Antike). Es wurde geschätzt, dass uns kaum 10 % der griechischen Literatur überliefert ist. Andere Forscher sind noch weit pessimistischer und gehen eher von einer Verlustrate um 99 % aus. In Teilen sieht es besonders trostlos aus (Archaik, Hellenismus), in anderen Bereichen etwas besser (klassische Zeit Griechenlands sowie Spätantike). Insgesamt ist die Quellenlage jedoch problematisch; man muss in allen Bereichen davon ausgehen, dass vieles spurlos verloren ist und sich auch viele Ereignisse und Zusammenhänge unserer Kenntnis entziehen. Neben den erzählenden Quellen müssen daher natürlich auch Inschriften und Papyri sowie archäologische und numismatische Quellen etc. herangezogen werden. Eine Zusammenfassung mit ausführlichen Angaben bieten die jeweiligen Artikel (Geschichtsschreibung u.&nbsp;ä.) in den entsprechenden Lexika (siehe unten).\n\nIm Folgenden seien einige der wichtigsten antiken Geschichtsschreiber und ihre (oft nur teilweise) erhaltenen Texte genannt:\n\nSiehe auch die online verfügbaren Quellensammlungen wie LacusCurtius oder das Perseus Project.\n\n\nSiehe auch.\nVerschiedenes:\n\nRom:\n\nListen:\n\n\nQuellenausgaben.\nQuellenausgaben mit Übersetzungen bieten neben anderen Reihen die Sammlung Tusculum und die Loeb Classical Library. Eine äußerst wichtige Sammlung der erhaltenen Reste ansonsten verlorener griechischer Geschichtsschreiber stellt der \"Jacoby\" dar:\n\n\nLiteratur.\nAllgemein: Aufgrund der Masse an Fachpublikationen kann an dieser Stelle nur eine sehr beschränkte Auswahl genannt werden. Das zentrale bibliographische Nachschlagewerk der Altertumswissenschaft stellt immer noch die \"L’Année philologique\" dar (L’Année Philologique. Bibliographie critique et analytique de l’Antiquité greco-latine, hrsg. von J. Marouzeau und J. Ernst, Paris 1923ff.). Kostenlos nutzbar ist zudem die umfangreiche Gnomon-Datenbank. Ausführliche Angaben sind außerdem entweder den Bibliographien der unten genannten Werke (besonders sei dabei auf The Cambridge Ancient History und Oldenbourg Grundriss der Geschichte hingewiesen) zu entnehmen oder den Bibliographien, die in der ausführlichen HU-Linkliste aufgeführt sind (siehe beispielsweise KU Eichstätt (kommentiert)).\n\nEs sei außerdem auf die hier verlinkten Artikel verwiesen, wo sich zahlreiche weiterführende Literaturangaben finden.\n\n\n\n\n\n\n\n\nSpezielle Literatur.\nNur in Auswahl. Es sei auch auf die oben genannten Fachlexika verwiesen.\n\nAllgemein\n\nGriechenland – Hellas\n\nRom\n\nPersien/Iran\n\nGermanen und Völkerwanderung\n\nKelten\n\nEtrusker\n\nPhönizier/Punier\n\nSkythen, Hunnen und andere Steppenvölker\n\nIndien und China\n\nGeschichtsschreibung\n\nMilitärgeschichte\n\nReligionsgeschichte\n\n\nEntdeckungsfahrten\n\nWirtschaftsgeschichte\n\nNachwirkungen\n\n"}
{"id": "109", "url": "https://de.wikipedia.org/wiki?curid=109", "title": "Anthony Hope", "text": "Anthony Hope\n\nAnthony Hope war das Pseudonym von Sir Anthony Hope Hawkins (* 9. Februar 1863 in London; † 8. Juli 1933 in Walton-on-the-Hill, Surrey), einem britischen Rechtsanwalt und Schriftsteller.\n\nAnthony Hope war ein Sohn von Reverend Edward Connerford Hawkins, einem anglikanischen Geistlichen, und Jane Isabella Grahame. Er verließ die Universität Oxford 1885 mit einem first-class degree und wurde Anwalt in London. Er heiratete 1903, hatte zwei Söhne und eine Tochter. Während des Ersten Weltkrieges arbeitete er im Ministry of Information. 1918 wurde er für seine Verdienste während des Krieges zum Ritter geschlagen.\n\nSein erstes Buch war \"A Man of Mark\" (1890), später schrieb er \"The Dolly Dialogues\" (1894). Den größten Erfolg hatte er mit \"The Prisoner of Zenda\" (dt. „Der Gefangene von Zenda“). Anschließend verfasste er \"Rupert of Henzau\" (1898) und viele weitere Bücher.\n"}
{"id": "110", "url": "https://de.wikipedia.org/wiki?curid=110", "title": "Ångström (Einheit)", "text": "Ångström (Einheit)\n\nDas Ångström [] (nach dem schwedischen Physiker Anders Jonas Ångström) ist eine Maßeinheit der Länge. Das Einheitenzeichen ist&nbsp;Å (A mit Ring). Ein Ångström entspricht dem zehnmillionsten Teil eines Millimeters. Das Ångström ist keine SI-Einheit.\n\nDas Ångström wird insbesondere in der Kristallographie und der Chemie benutzt, um mit „einfachen“ Zahlenwerten arbeiten zu können. So ist 1&nbsp;Å die typische Größenordnung für Atomradien sowie Abstände von Atomen in Kristallstrukturen und Bindungslängen in Molekülen. Der Radius isolierter neutraler Atome beträgt zwischen 0,3 und 3&nbsp;Å. Daher wird das Ångström oft als Einheit für Abstände in atomaren Größenordnungen verwendet, z.&nbsp;B. für die Dicke sehr dünner Schichten, für die Angabe der verwendeten Wellenlänge der Röntgenstrahlung bei ihrer Ermittlung in Röntgenbeugungsexperimenten wie der Kristallstrukturanalyse, sowie für die Porengröße von stationären Phasen in flüssigchromatographischen Säulen für die Hochleistungsflüssigkeitschromatographie (HPLC).\n\nIn der Thermodynamik wird die mittlere freie Weglänge der sich bewegenden Moleküle häufig in Ångström angegeben. Auch in der Optik und der Astronomie wird es zur Angabe einer Wellenlänge genutzt (allerdings weniger in deutschsprachigen, sondern eher in englischsprachigen Fachpublikationen).\n\nEinen ähnlichen Versuch, zu einfach handhabbaren Zahlenwerten zu kommen, unternahm 1925 Manne Siegbahn mit der Definition der X-Einheit, die etwa 10&nbsp;Meter entsprach. Das Ångström setzte sich aber durch.\n\nDa das Ångström nicht in der Einheitenrichtlinie aufgeführt wird, ist es in der EU keine gesetzliche Einheit, nach der schweizerischen Einheitenverordnung auch nicht in der Schweiz. In DIN&nbsp;1301-3 ist sie ausdrücklich als nicht mehr zugelassene Einheit aufgelistet.\n\n\nDarstellung in Computersystemen.\nLaut Unicode-Standard soll die Längeneinheit Ångström durch den Großbuchstaben Å (codice_1) dargestellt werden. Unicode enthält zwar auch ein Zeichen namens ANGSTROM SIGN (Ångströmzeichen, codice_2: Å), dieses wurde jedoch lediglich zur Kompatibilität mit älteren Zeichenkodierungsstandards aufgenommen und soll in neu erstellten Texten \"nicht\" verwendet werden.\n"}
{"id": "111", "url": "https://de.wikipedia.org/wiki?curid=111", "title": "Ampere", "text": "Ampere\n\nDas Ampere [] mit Einheitenzeichen&nbsp;A, benannt nach dem französischen Mathematiker und Physiker André-Marie Ampère, ist die SI-Basiseinheit der elektrischen Stromstärke und zugleich SI-Einheit der abgeleiteten Größe „magnetische Durchflutung“.\n\nObwohl man den Nachnamen des Namensgebers mit \"accent grave\" schreibt („Ampère“), wird die SI-Einheit im deutschen und englischen Sprachraum üblicherweise \"ohne\" Akzent geschrieben, also „Ampere“.\n\n\nDefinition.\nEin Ampere entspricht einem Fluss von 1&nbsp;Coulomb (C) pro Sekunde durch den Leiterquerschnitt:\n\nDas Coulomb ist im Internationalen Einheitensystem über die festgelegte Elementarladung formula_2 definiert. Ein Ampere entspricht daher genau einem Strom von formula_3 Elementarladungen pro Sekunde, bei einem Fluss von Elektronen sind dies ca. 6,2&#8239;·&#8239;10 (6,2&nbsp;Trillionen) Elektronen pro Sekunde.\n\nEin Fluss von 1&nbsp;A über eine Spannung von 1&nbsp;Volt (V) bedeutet eine Leistung von 1&nbsp;Watt (W).\n\n\nHistorisches.\nBevor das Ampere als internationale Einheit der Stromstärke festgelegt wurde, gab es eine Reihe von unterschiedlichen Einheiten und Definitionen. In Deutschland und einigen anderen Ländern war die „Webersche Einheit“ der Stromstärke in Gebrauch, dabei war 1 Weber-Einheit = 0,1 Ampere. In Großbritannien schlug man zunächst vor, die Einheit der Stromstärke mit „Galvat“, nach dem italienischen Biophysiker Luigi Galvani, zu benennen, die in etwa dem heutigen Ampere entsprochen hätte. Später wurde ebenfalls eine „Weber-Einheit“ für die Stromstärke eingeführt, die aber einen zehnmal so hohen Wert hatte wie die in Deutschland gebräuchliche. Noch verwickelter wurde es dadurch, dass der Name Weber auch für die Einheit der elektrischen Ladung benutzt wurde, so dass dann die Stromstärke gleich „Weber-Einheit/Sekunde“ war. Zeitweise gab man der Weber-Einheit auch den Namen „Farad“, womit später die Einheit der elektrischen Kapazität benannt wurde.\n\n\nStromstärke als Basiseinheit.\nWürde man die Stromstärke mit einer abgeleiteten Einheit messen, wie das etwa beim CGS-Einheitensystem geschieht, so ließen sich die elektrischen Größen durch die Basiseinheiten nur mit nicht ganzzahligen Exponenten ausdrücken. Um das zu vermeiden, wurde schon 1939 die Einheit der Stromstärke als weitere Basiseinheit vorgeschlagen.\n\n\nDefinition 1898.\n1898 wurde 1 Ampere im „Gesetz, betreffend die elektrischen Maßeinheiten“ des Deutschen Kaiserreichs als die Stärke desjenigen Stromes definiert, der aus einer wässrigen Silbernitrat-Lösung mittels Elektrolyse in einer Sekunde 1,118&nbsp;mg Silber abscheidet. Das so definierte Ampere ist später als internationales Ampere bezeichnet worden; das mit den restlichen Basiseinheiten kompatible dagegen als absolutes Ampere.\n\n\nDefinition 1948.\n1948 wurde das Ampere wie folgt über die Lorentzkraft zweier Leiter aufeinander definiert:\n\nMit dieser Definition wurde zugleich der Wert der magnetischen Feldkonstante μ festgelegt.\n\n\nDefinition seit 2019.\nAm 16. November 2018 wurde auf der 26. Generalkonferenz für Maß und Gewicht beschlossen, das Ampere und andere SI-Basiseinheiten mit Wirkung zum 20. Mai 2019 neu zu definieren. Mit dieser Neudefinition des Internationalen Einheitensystems basiert das Ampere auf der Elementarladung, der ein fester Zahlenwert zugewiesen wurde. Seitdem hängt die Definition des Amperes nur mehr von der Definition der Sekunde ab, nicht mehr jedoch vom Meter und vom Kilogramm. Die Neudefinition wurde vorgenommen, da diese leichter zu realisieren ist.\n\nDie magnetische Feldkonstante μ ist seitdem eine mit einer Messunsicherheit behaftete Messgröße, die experimentell bestimmt werden muss.\n\n"}
{"id": "112", "url": "https://de.wikipedia.org/wiki?curid=112", "title": "Acre", "text": "Acre\n\nAcre (Plural deutsch \"Acre\" oder \"Acres;\" , Plural \"acres\") ist eine von den Britischen Inseln stammende angloamerikanische Maßeinheit zur Flächenbestimmung von Grundstücken und entspricht grob 4047&nbsp;m² beziehungsweise 40,47 Ar. Neben dem Acre wird im heutigen angloamerikanischen Flächenmaßsystem zur Land- und Grundvermessung praktisch nur noch die Flächeneinheit Square Foot verwendet; in den Vereinigten Staaten wird die Größe von Grundstücken allein mit diesen beiden Flächeneinheiten angegeben. Weit verbreitet sind diese Einheiten auch in Großbritannien, Kanada, Indien, Australien und anderen Commonwealth-Staaten, obwohl dort heute ein metrisches Hauptmaßsystem besteht.\n\n\nBegriff.\n\nDefinition.\nHeutzutage wird der Acre direkt mit 43.560 Square Feet definiert, weil die zur ursprünglichen Definition verwendeten Längenmaße Furlong und Rod mittlerweile ungebräuchlich sind.\n\nUnter Einbezug der Meile sowie den heutzutage nicht mehr gebräuchlichen Flächenmaßen Rood und Square Rod (auch als Square Perch, Perch, Square Pole, Pole bezeichnet) ergibt sich die folgende Umrechnung:\n\nPraktische Verwendung.\nDer Acre ist das Hauptmaß für Grundstücksflächen. In der Regel werden nicht mehr als zwei Nachkommastellen angegeben, womit eine Genauigkeit von ±20&nbsp;m² vorliegt. Sobald genauere Angaben erforderlich sind, beispielsweise bei Bauland, wird die Flächeneinheit Square Foot verwendet.\n\nBei landwirtschaftlich genutzten Grundstücken werden die Flächen in \"Workable Acre\" und \"Non Workable Acre\" unterteilt. Damit gemeint sind die tatsächlich nutzbare Fläche und die für landwirtschaftliche Zwecke nicht nutzbare Fläche, wie beispielsweise Ödland.\n\nAuch sehr große Grundflächen werden in Acre angegeben, beispielsweise 87.000&nbsp;ac (≈ 350&nbsp;km²). Eine Umrechnung in Quadratmeilen erfolgt in der Regel nicht.\n\n\n\nGeschichte.\n\nUrsprüngliche Definition.\nDie Einheit \"acre,\" von altenglisch \"æcer\" ‚Acker, Feld‘, bezeichnete ursprünglich den Landstreifen, der mit einem Ochsengespann in einem Tag gepflügt werden konnte. Unter König Eduard&nbsp;I. sowie erneut unter Eduard&nbsp;III. und Heinrich&nbsp;VIII. wurde der Acre gesetzlich als ein Stück Land mit der Länge von 40 Rods (oder Perches; =&nbsp;1&nbsp;Furlong oder 660 Feet) und der Breite von 4&nbsp;Rods (oder Perches; = 66 Feet oder [seit 1620] 1&nbsp;Chain) beziehungsweise 160 Square Rods (Quadratruten) bei welcher Grundstücksform auch immer definiert.\n\nMit seiner Größe von grob 40&nbsp;Ar ist der Acre typologisch vergleichbar mit dem Morgen, dem Tagewerk (oder \"Tagwan\"), dem Joch (oder Juchart) und dem Mannwerk.\n\n\nU.S. Survey Foot.\nIn den USA basierten die Landflächeneinheiten bis 2022 auf dem ersatzlos abgeschafften \"U.S.&nbsp;survey&nbsp;foot\". Der aus dem Survey&nbsp;Foot abgeleitete Acre war mit einer Fläche von ca. 4046,8726099&nbsp;m² um etwa 162&nbsp;cm² geringfügig größer als der auf dem Standard-Foot basierende Acre. \n\n\nHistorische Einheiten.\nObgleich auf den Britischen Inseln die Größe des Acres seit dem Hochmittelalter mit 160 Square Rods definiert war, war dessen Fläche je nach Ort und Zeit uneinheitlich, da die Längeneinheit Rod oder Perch verschiedenen Fuß-Äquivalenten entsprach. Erst mit der Neudefinition der \"Imperial Units\" durch den \"Weights and Measures Act\" von 1824 wurde ein für das gesamte Britische Weltreich einheitlicher Acre geschaffen.\n\nVor der Einführung des Imperial Standard Acre (Statute Acre) gab es unter anderem den alten schottischen Acre, den neuen schottischen Acre, auch als Cunningham Acre bezeichnet, oder den irischen bzw. Plantation Acre. Beispielsweise hat der Cunningham Acre etwa die 1,3-fache Größe, der Plantation Acre grob die 1,6-fache Größe des heutigen Acres. Einige dieser veralteten Maße waren teilweise bis ins 20.&nbsp;Jahrhundert gebräuchlich, so in abgelegenen Gebieten Irlands.\n\n"}
{"id": "113", "url": "https://de.wikipedia.org/wiki?curid=113", "title": "Apostilb", "text": "Apostilb\n\nApostilb (Einheitenzeichen asb) ist eine veraltete Einheit der Leuchtdichte selbstleuchtender Körper. 1942 wurde Blondel (Einheitenzeichen: blondel) als weiterer Name vorgeschlagen, der Name wurde zum Andenken an den französischen Physiker André-Eugène Blondel gewählt. Eine Benutzung oder offizielle Festlegung dieses Namens ist derzeit nicht nachweisbar.\nSeit 1978 ist das Apostilb keine offizielle Einheit mehr. Es ist eine Untereinheit des Stilb (Einheitenzeichen: formula_1) und darüber verknüpft mit dem Lambert (Einheitenzeichen: formula_2):\n\nDie entsprechende SI-Einheit ist cd/m² (nicht SI-konforme Alternativbezeichnung: Nit (nt)):\n\n"}
{"id": "114", "url": "https://de.wikipedia.org/wiki?curid=114", "title": "Ar (Einheit)", "text": "Ar (Einheit)\n\nDas oder der Ar, in der Schweiz die Are, ist eine Flächenmaßeinheit im metrischen System von 100&nbsp;m mit dem Einheitenzeichen a (oft jedoch nicht oder falsch abgekürzt: Ar bzw. ar). 100&nbsp;a ergeben 1&nbsp;ha. Ein Quadrat mit dem Flächeninhalt von 1 a hat eine Kantenlänge von zehn Metern, man spricht daher auch von einem Quadratdekameter&nbsp;(dam).\n\nDas Ar ist keine SI-Einheit; im Gegensatz zum Hektar ist sie nicht einmal zum Gebrauch mit dem SI zugelassen.\n\nIn der EU und der Schweiz ist der Ar bzw. die Are gesetzliche Einheit für die Angabe der Fläche von Grund- und Flurstücken.\n\n\nGeschichte.\nIm Jahr 1793 wurde in Frankreich das Meter als der 10-millionste Teil des Erdquadranten auf dem Meridian von Paris festgelegt. Zugleich wurde die Einheit \"are\" in Anlehnung an das lateinische Wort \"ārea\" (Fläche, freier Platz) für die Fläche von 100&nbsp;m neu geschaffen. Sie war anfangs die einzige gebräuchliche metrische Flächeneinheit, samt ihren Teilen und Vielfachen Zentiar (1&nbsp;ca = 1&nbsp;m) und Hektar (1&nbsp;ha = 100&nbsp;a).\n\nIm Jahr 1868 wurde die Maßeinheit unter der Bezeichnung Ar auch in Deutschland amtlich eingeführt: Die entsprechende Norddeutsche Maß- und Gewichtsordnung trat 1872 für das gesamte Deutsche Reich in Kraft.\n\n\n\nVielfache und Teile.\nAußer Ar und Hektar sind diese Vielfachen und Teile im deutschen Sprachraum ungebräuchlich und nur noch von historischem Interesse.\n\nDas Dekar wird als Flächenmaß in der bulgarischen Landwirtschaft, in Griechenland (Stremma), in der Türkei und einigen Staaten des Nahen Ostens (metrisches Dunam) verwendet.\n\n"}
{"id": "115", "url": "https://de.wikipedia.org/wiki?curid=115", "title": "Arbeit (Sozialwissenschaften)", "text": "Arbeit (Sozialwissenschaften)\n\nArbeit ist eine zielbewusste und sozial durch Institutionen (Bräuche) abgestützte besondere Form der Tätigkeit, mit der Menschen seit ihrer Menschwerdung in ihrer Umwelt zu überleben versuchen.\n\n\nZur Anthropologie der „Arbeit“.\nEs ist bereits strittig, ob man zielgerichtete körperliche Anstrengung von Tieren (zum Beispiel den instinktiven Nestbau oder das andressierte Ziehen eines Pfluges) als „Arbeit“ bezeichnen kann. Die philosophische Anthropologie geht zumeist davon aus, dass „Arbeit“ erst im Tier-Mensch-Übergangsfeld erscheint (vgl. zum Beispiel Friedrich Engels’ \"Anteil der Arbeit an der Menschwerdung des Affen\", MEW 20). Dabei wird meist angenommen, dass die Resultate menschlicher Arbeit (als „Gegenstände“) wie in einem Spiegel dem Menschen sich selbst zeigen, so dass er angesichts ihrer des Selbstbewusstseins mächtig wird. Das könnten aber auch andere menschliche Tätigkeiten bewirken, so dass „Arbeit“ gerade in ihren ursprünglichen Formen schwer von anderen menschlichen Überlebensstrategien wie Spiel oder Kunst zu trennen ist. Seit der Urgeschichte ist (so Karl Bücher) ein Basiszusammenhang von Arbeit und Rhythmus anzunehmen (vgl. das Arbeitslied).\n\nIm Vergleich zu modernen Erwerbstätigen hatten Jäger und Sammler laut zahlreichen Studien mehr Zeit zur freien Verfügung. \"Siehe hierzu\": Abschnitt „Alltag und Lebenserwartung“ im Artikel „Jäger und Sammler“.\n\n\nEinstellung zur Arbeit: Kulturelle Unterschiede.\nDie Auffassung, welche Tätigkeiten als Arbeit betrachtet werden und welche Einstellung die Menschen dazu haben, sind kulturell sehr unterschiedlich und unterliegen einem ständigen sozialen Wandel.\n\nIn den industrialisierten Kulturen haben Arbeit und Beruf einen hohen Stellenwert, da das marktwirtschaftlich organisierte Wirtschaftssystem und der erwünschte Fortschritt auf leistungswillige Arbeitnehmer angewiesen ist. Das war nicht immer so: Vor der industriellen Revolution lebte ein Großteil der Menschen von autonomer Subsistenzwirtschaft. Dies wandelte sich dann in kurzer Zeit zu einer stark reglementierten und hierarchisch organisierten Arbeitswelt, der von den Arbeitern einen erheblich höheren Zeitaufwand erforderte als die Selbstversorgung. In dieser Zeit entstand die Bewertung der Arbeit als »Leid und Mühsal«. Seitdem haben sich die zeitliche und körperliche Belastung, die Entlohnung sowie die rechtliche Stellung der Arbeitnehmer kontinuierlich verbessert. Andererseits wird heute jedoch viel mehr Flexibilität bezüglich der Fortbildung (bis hin zur Umschulung), der Arbeitsplätze und -zeiten erwartet. Im Westen wird Arbeit heute vielfach als »notwendiges Übel« gesehen, welches allerdings Rang und Ansehen garantiert und unter Umständen ein Weg zur Selbstverwirklichung werden kann. Der fortschreitende Wandel führt dabei zu einer stetig neuen Auseinandersetzung mit dem Stellenwert der Arbeit.\n\nDemgegenüber gibt es Gesellschaften, in denen Menschen, die von unselbstständiger Lohnarbeit leben (ähnlich wie während der Industriellen Revolution im Westen), nur geringes Ansehen genießen und ihre Leistung nur widerwillig erbringen, weil der Lohn gering ist und die Arbeitszeit einen Großteil des Tagesablaufes bestimmt. In Ländern, wo die Bevölkerung noch vorwiegend autonom von traditionellen Subsistenzformen lebt, wird Lohnarbeit nur geschätzt, da ihre Bedingungen dem Einzelnen weitaus weniger Möglichkeiten (bisweilen auch mehr Freizeit) eröffnen als dem eigenständigen Bauern oder Jäger. Dies gilt auch dort, wo die Reziprozität (gegenseitige, unentgeltliche Hilfe innerhalb einer lokalen Gemeinschaft) noch eine größere Rolle spielt als die Geldwirtschaft. Die selbstbestimmte Arbeit wird hier ungleich höher geschätzt: sie wird oftmals begrifflich nicht von der Freizeit unterschieden und gilt nicht als mühevoller Überlebenskampf, sondern als »sinngebende Lebensaufgabe«. Bei einigen naturverbundenen Ethnien ist die traditionelle Arbeit eine religiöse Handlung, die das Bündnis zwischen Menschen, Ahnen und Umwelt aufrechterhält. Da diese tiefe Bedeutung bei der Lohnarbeit fehlt, mangelt es häufig auch an ausreichender Motivation zur Arbeit. Westliche Arbeitgeber empfinden das als Faulheit oder mangelnde Bereitschaft zur Entwicklung bis hin zur Rückständigkeit. Dies gilt besonders für streng egalitäre Gesellschaften, bei denen jegliche Arbeit negativ gesehen wird, weil sie etwa mit Habgier, egoistischem Streben oder Reichtum auf Kosten Anderer gleichgesetzt wird.\n\n\nWortgeschichte.\nDas Wort \"Arbeit\" ist gemeingermanischen Ursprungs (\"*arbējiðiz\", got. \"arbaiþs\"); die Etymologie ist unsicher; evtl. verwandt mit indoeurop. \"*orbh-\" „verwaist“, „Waise“, „ein zu schwerer körperlicher Tätigkeit verdungenes Kind“ (vgl. Erbe); evtl. auch verwandt mit aslaw. \"robota\" („Knechtschaft“, „Sklaverei“, vgl. Roboter).\n\nIm Alt- und Mittelhochdeutschen überwiegt die Wortbedeutung „Mühsal“, „Strapaze“, „Not“; redensartlich noch heute \"Mühe und Arbeit\" (vgl. Psalm 90, lateinisch \"labor et dolor\").\n\nDas französische Wort \"travail\" hat eine ähnliche, sogar noch extremere Wortgeschichte hinter sich: es steht im Zusammenhang mit einem frühmittelalterlichen Folterinstrument.\n\nDas italienische \"lavoro\" und englische \"labour\" (amerikanisch \"labor\") gehen auf das lateinische \"labor\" zurück, das ebenfalls primär „Mühe“ bedeutet.\n\nViele Redensarten sind mit ihr verbunden. So wurde harte körperliche Arbeit früher als \"Kärrnerarbeit\" bezeichnet, und eine \"Schweinearbeit\" bedeutet unangenehm viel Arbeit: \"Wer die Arbeit kennt und sich nicht drückt, | der ist verrückt.\"\n\n\nGeschichtsschreibung.\nDie Geschichtsschreibung der Arbeit begann erst im 20. Jahrhundert (zuerst in Frankreich, England und den USA) zu entstehen. Eine frühe Ausnahme innerhalb der deutschen Historiker war Karl Lamprecht (1856–1915). Ein neueres Buch stammt von Arne Eggebrecht und anderen.\n\n\nTheoriegeschichte.\n\nAntike.\nAristokratische Autoren wie Xenophon, Platon, Aristoteles und Cicero würdigten den Großteil der täglichen Arbeit (Handwerker, Bauern, Kaufleute) herab. Sie galt ihnen (insbesondere körperliche Arbeit) als Zeichen der Unfreiheit. Sklaven (\"dúloi\") und Handwerker (\"bánausoi\") waren „der Notwendigkeit untertan“ und konnten nur durch diese als „unfrei“ verstandene Arbeit ihre Lebensbedürfnisse befriedigen. Geistige Arbeit blieb der \"scholé\" (gespr. \"s|cholé\") vorbehalten, was etwa „schöpferische Muße“ beschrieb, wovon das deutsche Wort Schule herrührt.\n\n\nMittelalter.\nIn Europa blieben – vor allem in der Landwirtschaft – Formen unfreier Arbeit von Männern und Frauen, auch Kindern und Alten, lange erhalten (Fron, Lasswirtschaft), am längsten im Russischen Reich; im Deutschen Reich wurden deren letztes Überbleibsel (die Schollengebundenheit in den beiden Mecklenburgs) erst durch die Novemberrevolution 1918 beseitigt. Noch heute existieren in großen Teilen der Welt unterschiedliche Erscheinungsformen unfreier Arbeit, von der Arbeitspflicht bis hin zur Arbeitsversklavung und Zwangsarbeit.\n\nEine positive Bewertung von Arbeit als „produktiver Betätigung zur Befriedigung eigener oder fremder Bedürfnisse“ war im Rittertum und in der Mystik angelegt. Durch Reformation und Aufklärung rückte sie in den Vordergrund: Eine neue Sicht der Arbeit als sittlicher Wert und Beruf (als Berufung verstanden) des Menschen in der Welt wurde von Martin Luther mit seiner Lehre vom allgemeinen Priestertum ausgeprägt. Schärfer noch wurde im Calvinismus die Nicht-Arbeit überhaupt verworfen \"(siehe auch: Protestantische Ethik)\". \n\n\nNeuzeit.\nIn der Frühphase der Aufklärung wurde Arbeit zum Naturrecht des Menschen erklärt (Jean-Jacques Rousseau). Damit wurde das feudalistische Prinzip der Legitimation kritisiert. Eigentum entsteht \"einzig\" durch Arbeit, niemand hat ein von Gott gegebenes Anrecht auf Eigentum. Güter, die nicht durch menschliche Arbeit entstanden sind, sind Gemeinbesitz.\n\nAdam Smith unterscheidet produktive und unproduktive Arbeit. \"Produktive\" Arbeit nennt er die Arbeit, deren Resultat ein verkäufliches Produkt ist. Dazu wird nicht nur der eigentliche Wertschöpfungsprozess (beim Schmied: der Vorgang des Schmiedens selbst) gerechnet, sondern auch alle Arbeiten, die indirekt zur Vervollkommnung des Gutes beitragen (beim Schmied: das Erhalten der Glut, das Pflegen von Hammer und Amboss). \"Unproduktiv\" ist hingegen die Arbeit, die nicht in einem verkäuflichen Produkt resultiert (zum Beispiel die mütterliche Hausarbeit). Andere Arbeiten sind von diesem Standpunkt aus nicht unnütz, da sie notwendig sind, um produktive Arbeit leisten zu können, und werden heute zum Beispiel als \"reproduktiv\" bezeichnet (beispielsweise Beamte, Verwalter, Soldaten).\n\nDer Frühsozialist Charles Fourier proklamierte 1808 ein \"Recht auf Arbeit.\"\n\nIn der deutschen Philosophie (Immanuel Kant, Johann Gottfried Herder, Georg Wilhelm Friedrich Hegel, Johann Gottlieb Fichte) wird die Arbeit zur Existenzbedingung und sittlichen Pflicht erklärt. Kant räumte in seiner \"Anthropologie in pragmatischer Hinsicht\" (1798, §87) jedoch ein, dass Faulheit eine Schutzfunktion habe: „\"Denn die Natur hat auch den Abscheu für anhaltende Arbeit manchem Subjekt weislich in seinen für ihn sowohl als andere heilsamen Instinkt gelegt: weil dieses etwa keinen langen oder oft wiederholenden Kräfteaufwand ohne Erschöpfung vertrug, sondern gewisser Pausen der Erholung bedurfte.\"“\n\nNach Karl Marx' Werttheorie ist die „menschliche Arbeitskraft“ als alleinige Kraft fähig, das Kapital (als eine Ansammlung geronnener Arbeit) zu vergrößern (Mehrwert zu akkumulieren). Sie tut dies im Kapitalismus unausweichlich.\n\nPraktisch spiegelt dies wider, dass in der Phase der Industrialisierung freie Arbeit augenfällig zur Ware wurde und vorwiegend die düsteren Züge der damaligen Lohnarbeit annahm. So zum Beispiel in Gestalt der Kinderarbeit, des Arbeiterelends (der „Verelendung“), der Arbeitsunfälle und -krankheiten, der drückenden Akkordarbeit – alles dies sind Merkmale der allgemein so empfundenen „Sozialen Frage“\n\nDeren Folgen wurden schon seit Hegel als „Entfremdung“ charakterisiert: Der Arbeiter hat zu seinem eigenen Arbeitsprodukt, aber auch zu dem Unternehmen, für das er arbeitet, nur noch das bare Lohnverhältnis und kann dem gemäß nicht mehr stolz auf sie sein – in diesem 'Spiegel' erkennt er sich selbst jedenfalls nicht mehr wieder.\n\nFür Ernst Jünger war Arbeit nicht Tätigkeit schlechthin, sondern der Ausdruck eines „besonderen Seins, das seinen Raum, seine Zeit, seine Gesetzmäßigkeit zu erfüllen sucht“ („\"Der Arbeiter\"“). Daher kenne Arbeit auch keinen Gegensatz außer sich selbst. Das Gegenteil von Arbeit sei nicht Ruhe oder Muße, da es keinen Zustand gebe, der nicht als Arbeit begriffen werden könne.\n\nNeben der „produktiven“ Eigenschaft der Arbeit wird neuerdings (Lars Clausen) ihre „destruktive“ Seite hervorgehoben: am auffälligsten als (harte, lebensgefährliche) Arbeit der Soldaten, aber auch bei selbst-, mitmenschen- oder umweltzerstörerischer Arbeit ist Destruktives seit je Wesensbestandteil aller Arbeit. (Anders die „vernichtende Tätigkeit“, die alltags als Vandalismus auftreten kann und einen organisatorischen Höhepunkt im KZ hatte.)\n\n\nArbeit und Fortschritt der Technik.\nDer Soziologe Rudi Dutschke und der Politologe Bernd Rabehl meinten 1967 in einem Gespräch mit Hans Magnus Enzensberger, der technische Fortschritt könne die Erwerbsarbeit in Zukunft erheblich reduzieren: „Dabei muß man bedenken, dass wir fähig sein werden, den Arbeitstag auf fünf Stunden zu reduzieren durch moderne Produktionsanlagen, dadurch dass die überflüssige Bürokratie wegfällt. Der Betrieb wird zum Zentrum der politischen Selbstbestimmung, der Selbstbestimmung über das eigene Leben. Man wird also im Betrieb täglich debattieren, es wird langsam ein Kollektiv entstehen, ein Kollektiv ohne Anonymität, begrenzt auf zwei- bis dreitausend Leute, die also immer noch eine direkte Beziehung zueinander haben.“\n\nIn der Zeit der 1950er und 1960er Jahre gab der technische Fortschritt sogar in der calvinistisch geprägten nordamerikanischen Gesellschaft tatsächlich wieder dem Gedanken Raum, dass Fortschritt zu mehr Freizeit führen könne. Zeugnisse für die Hoffnungen gaben die Schöpfungen einer bunten Pop-Kultur mit ihren Science-Fiction-Träumen wie beispielsweise der Zeichentrickserie „Die Jetsons“, in der technikgestütztes Faulenzen ohne moralische Bedenken als Ideal dargestellt werden konnte.\n\nAngesichts global unterschiedlicher Entwicklungen zeigte sich jedoch, dass ein Ausruhen auf erreichtem Wohlstand in einer Region als Gelegenheit zum wirtschaftlichen Aufholen in anderen Regionen verstanden wurde. In jenem Zeitraum wurde besonders in \"Japan\" technischer Fortschritt in erster Linie als Weg begriffen, große wirtschaftliche Fortschritte zu erzielen. Bis heute begrenzt somit ein Wettbewerb, in dem der verliert, der zuerst bremst, die Möglichkeit, aus technischem und technologischem Fortschritt mehr selbstbestimmte freie Zeit zu gewinnen.\n\nZudem prägte Robert Solow in der Wirtschaft bereits 1956 mit seinem Wachstumsmodell die Auffassung, dass technologische Innovation in erster Linie als ein Multiplikator des Faktors Arbeit aufträte, womit er in der Dogmengeschichte der Wirtschaft einen Ankerpunkt schuf, der bis heute den Raum des Denkbaren gegenüber möglichen Alternativen wirkungsvoll abgrenzt. So schafft in der heutigen Arbeitswelt technischer Fortschritt dort, wo er Freiräume erweitert, vorwiegend und sogar mit zunehmender Geschwindigkeit immer neue Arbeit. Dort, wo Technik schon vor Beginn des Industriezeitalters die Menschen von Arbeit befreite, wurden sie oft nicht freier, sondern arbeitslose Geächtete.\n\nIn Deutschland nahm zwischen 1960 und 2010 das Arbeitsvolumen pro Kopf kontinuierlich um 30 Prozent ab.\n\n\nArbeit heute.\nNach wie vor wird \"Erwerbsarbeit\" nicht mit \"Arbeit\" überhaupt gleichgesetzt. Wo Arbeit auch heute noch nicht \"Ware\" ist, sind zwei wesentliche Aspekte hervorzuheben:\n\n\nIn den wohlhabenden Staaten der Welt (zu denen auch Deutschland zählt), wird die Erwerbsarbeit knapp. Es findet eine zunehmende Flexibilisierung, Virtualisierung, Automatisierung und Subjektivierung der Arbeit statt, prekäre Arbeitsverhältnisse nehmen zu. Inhaltlich verschiebt sich die Arbeit immer mehr in den tertiären Sektor (Dienstleistungen) und in Niedriglohnländer (Offshoring), zumal da die Jugend- und Langzeit-Arbeitslosigkeit die Arbeit trotz ihres zentral wichtigen Charakters als Überlebenstätigkeit aus dem Feld der Erfahrung Vieler rücken.\n\nIn ärmeren Ländern herrschen zugleich – zum Teil – Verhältnisse, die mit denen in der Industrialisierungsphase Europas vergleichbar sind: Kinderarbeit, Billiglohnarbeit und fehlende soziale Absicherung sind dort häufig anzutreffende Bestandteile der Arbeitswelt.\n\n\nSystematik der Arbeitsverhältnisse.\nDort, wo Arbeit für andere verrichtet wird, ist nach wie vor der Unterschied bedeutsam\nEin Wandel einer Tätigkeit von der unentgeltlichen zur entgeltlichen Form wird auch als Kommerzialisierung bezeichnet.\n\n\nUnentgeltliche Arbeit.\nDie \"unentgeltliche Arbeit\" umfasst also historisch sehr viele Formen, die auch heute vorkommen, aber nicht immer als \"Arbeit\" betrachtet werden.\n\nBeispiele sind:\n\n\nErwerbsarbeit.\nUnter Erwerbsarbeit versteht man eine Arbeitsleistung gegen Entgelt (Arbeitslohn) im Gegensatz zu unentgeltlicher Arbeit (wie Subsistenzarbeit, Sklavenarbeit, Hausarbeit oder ehrenamtlicher Arbeit).\n\nErwerbsarbeit wird in einem Beschäftigungsverhältnis (Lohnarbeit) oder in selbständiger und neuerdings auch in scheinselbständiger Form geleistet. \n\nBeispiele sind:\n\nDas deutsche Privatrecht unterscheidet hier analog zwischen Werkvertrag (der Erfolg wird geschuldet) und Dienstvertrag (der Dienst wird geschuldet).\n\n\nMischformen.\nZu den Mischformen (auch als \"Atypische Arbeit\" bzw. \"Atypische Beschäftigung\" bezeichnet) gehören zahlreiche freiwillige oder gesetzlich vorgesehene Arbeiten, die gering entgolten werden. Teils sind die Arbeitenden zur Verrichtung der betreffenden Tätigkeiten rechtlich verpflichtet, teils fühlen sie sich ethisch hierzu verpflichtet. Zu den Mischformen gehören auch solche ehrenamtlichen Tätigkeiten, für die eine Aufwandsentschädigung gezahlt wird, die über den tatsächlichen Aufwand hinausgeht.\n\n\nKritik der Arbeit.\nWas die zentrale Stellung der Arbeit in kollektiven Wertsystemen angeht, sagen Kritiker der Arbeit, unterscheiden sich Staatsformen und Herrschaftsmodelle erstaunlich wenig.\n\nAls Kritiker der Arbeit war Paul Lafargue, Autor des Pamphlets \"Le droit à la paresse\" (‚Das Recht auf Faulheit‘; 1883), in der alten Arbeiterbewegung ein Außenseiter. Lafargue verstand sich als revolutionärer Sozialist und dementsprechend schätzte er die kapitalistische Arbeitsethik ein. „Die kapitalistische Moral, eine jämmerliche Kopie der christlichen Moral, belegt das Fleisch des Arbeiters mit einem Bannfluch: Ihr Ideal besteht darin, die Bedürfnisse des Produzenten auf das geringste Minimum zu reduzieren, seine Genüsse und Leidenschaften zu ersticken und ihn zur Rolle einer Maschine zu verurteilen, aus der man ohne Rast und ohne Dank Arbeit nach Belieben herausschindet.“ Lafargues Manifest erschien 1887 auf Deutsch. Lafargue zitierte Lessing:\nDie radikalen Kritiker der Arbeit lehnen den Arbeitszwang ab – für Reiche wie für Arme. Damit unterscheiden sie sich von Sozialisten, die sich über den Müßiggang der Reichen empören und fordern, dass alle arbeiten müssen. Hintergrund der Ablehnung des Arbeitszwangs ist die reale Möglichkeit der Aufhebung der Arbeit. Schon Lafargue meinte, dass 3 Stunden Arbeit ausreichen müssten. \"Aufhebung der Arbeit\" bedeutet jedoch nicht nur Verringerung der Arbeitszeit durch Automation und Abschaffung der Produktion von Gütern, die nur um des Profits willen hergestellt werden.\n\nUnter kapitalistischen Bedingungen sind Arbeitslose wie abhängig Beschäftigte und auch diejenigen, die auf das sogenannte Berufsleben vorbereitet werden, gleichermaßen dem System der Lohnarbeit unterworfen. Auch wer freie Zeit hat, kann diese nicht frei nutzen, sei es weil andere, mit denen man etwas zusammen tun möchte, arbeiten müssen, sei es weil die gesamte Umwelt von kommerziellen Zwängen geprägt ist. Aufhebung der Arbeit bedeutet, dass auch weiterhin notwendige Tätigkeiten wie zum Beispiel die Pflege gebrechlicher Menschen, einen anderen Charakter annehmen, wenn sie in einem anderen nicht-hierarchischen Kontext ausgeübt werden. Dass die Menschen ohne den Zwang zu Arbeit einfach nichts tun und verhungern würden, ist nach Ansicht der Kritiker der Arbeit nicht zu erwarten, da sie ja bereits unter kapitalistischen Bedingungen freiwillig konstruktiv zusammenarbeiten.\n\nDie Tradition der Ablehnung der Arbeit wurde nach dem Zweiten Weltkrieg von einer Gruppe junger Menschen in Paris wiederbelebt. Unter ihnen war Guy Debord. Der Slogan „Ne travaillez jamais“ (‚Arbeitet niemals‘) kehrte dann im Pariser Mai 1968 wieder. Die Ablehnung der Arbeit spielte auch in Italien in den Kämpfen der 1960er und 1970er Jahre eine zentrale Rolle.\n\nDer Postanarchist Bob Black rief 1985 die Proletarier dieser Welt auf, sich zu entspannen, da niemand jemals arbeiten solle. Bob Black versteht sich als Antimarxist und postleftistischer (Individual-)Anarchist. Er ruft dazu auf, alle Arbeitsplätze so umzugestalten, dass sie wie ein Spiel sind. Er findet es merkwürdig, dass die einen sich auf dem Feld abrackern, während andere in ihrer Freizeit, welche nur das ebenfalls fremdbestimmte und durchorganisierte Gegenstück zur Arbeit sei, bei der Gärtnerei entspannen. Zentral in seiner Kritik ist neben diesem Punkt(en) auch der Charakter der Fremdbestimmtheit der Arbeit, ob nun im Staatssozialismus oder im Kapitalismus. Im Anschluss an Michel Foucault kritisiert er Disziplinierung und die Disziplinargesellschaft und betont die zentrale Rolle der Arbeit bei der Disziplinierung: Gefängnisse und Fabriken seien zur selben Zeit entstanden, die Schulen seien dafür da, Leistungsgedanken und -bereitschaft und Gehorsam einzuüben und es gebe „mehr Freiheit in jeder einigermaßen entstalinisierten Diktatur als an einem gewöhnlichen amerikanischen Arbeitsplatz“. Eine ähnliche Kritik hatte allerdings auch schon Gustav Landauer vorgetragen. Auch er wollte den Arbeitstag ähnlich neu gestalten.\n\nVon einer deutschen Tradition der Arbeitskritik kann man dennoch kaum reden. Seit den 1990er Jahren bemüht sich allerdings die wertkritische Gruppe \"Krisis\" um eine Erneuerung der Kritik der Arbeit. Sie veröffentlichte ein \"Manifest gegen die Arbeit\". Die Krisis versteht sich als postmarxistisch, bzw. grenzt sie sich ab vom traditionellen Marxismus.\n\nAktuell in der Kritik der Arbeit ist die Kritik der Identifikation mit der Arbeit als zentralem Element männlicher Identität.\n\n\nReportagen, Feldforschung und Darstellung der Arbeit in der Literatur.\nIm Bereich der Feldforschung wurde eine Studie der Österreichischen Wirtschaftspsychologischen Forschungsstelle berühmt. Sie hieß \"Die Arbeitslosen von Marienthal\" (1933) und beschäftigt sich mit den Folgen plötzlich eintretender Arbeitslosigkeit für eine Dorfgemeinschaft.\n\nDarstellungen und Schilderungen der täglichen Arbeit am unteren Rand der Gesellschaft finden sich innerhalb der Belletristik etwa bei den österreichischen Autoren Franz Innerhofer und Gernot Wolfgruber, dem Deutschen Hans Dieter Baroth und bei George Orwell (\"Erledigt in Paris und London\").\n\n\n\n\n\n"}
{"id": "116", "url": "https://de.wikipedia.org/wiki?curid=116", "title": "Atomare Masseneinheit", "text": "Atomare Masseneinheit\n\nDie atomare Masseneinheit (Einheitenzeichen: u für unified atomic mass unit,\" veraltet amu für atomic mass unit\") ist eine Maßeinheit der Masse. Ihr Wert ist auf der Masse eines Atoms des Kohlenstoff-Isotops C festgelegt. Die atomare Masseneinheit ist zum Gebrauch mit dem Internationalen Einheitensystem (SI) zugelassen und eine gesetzliche Maßeinheit.\n\nSie wird bei der Angabe nicht nur von Atom-, sondern auch von Molekülmassen verwendet. In der Biochemie, in den USA auch in der organischen Chemie, wird die atomare Masseneinheit auch als Dalton bezeichnet (Einheitenzeichen: Da), benannt nach dem englischen Naturforscher John Dalton.\n\nDie so gewählte atomare Masseneinheit hat die praktisch nützliche Eigenschaft, dass alle bekannten Kern- und Atommassen nahe bei ganzzahligen Vielfachen von u liegen; die Abweichungen betragen in allen Fällen weniger als 0,1&nbsp;u. Die betreffende ganze Zahl heißt Massenzahl des Kerns oder Atoms und ist gleich der Anzahl der Nukleonen im Kern.\n\nAußerdem hat die Atom- oder Molekülmasse in der Einheit u (bzw.&nbsp;Da) den gleichen Zahlenwert wie die Masse eines Mols dieses Stoffs in Gramm. Die Masse großer Moleküle wie Proteine, DNA und anderer Biomoleküle wird oft in Kilodalton charakterisiert, da es zahlenmäßig keine Unterschiede zur Angabe in kg/mol gibt. \n\n\nDefinition.\n1 u ist definiert als ein Zwölftel der Masse eines isolierten Atoms des Kohlenstoff-Isotops C im Grundzustand. Der aktuell empfohlene Wert ist\n\nDie Umrechnung in die SI-Einheit Kilogramm ergibt\n\nDa der Kern des C-Atoms 12 Nukleonen enthält, ist die Einheit u annähernd gleich der Masse eines Nukleons, also eines Protons oder Neutrons. Deshalb entspricht der Zahlenwert der Atommasse in u annähernd der Massenzahl oder Nukleonenzahl, also der Zahl der schweren Kernbausteine des Atoms.\n\n\nBeziehung zur molaren Masse.\nBis zur Neudefinition der SI-Einheiten im Jahr 2019 war das Mol als die Stoffmenge definiert, die aus ebenso vielen Einzelteilchen besteht, wie Atome in 12&nbsp;g Kohlenstoff C enthalten sind. Die atomare Masseneinheit und das Mol waren also über dasselbe Atom C definiert. Dadurch ergab sich für die Masse eines Teilchens in u und dessen molare Masse in g/mol exakt der gleiche Zahlenwert. Oder anders ausgedrückt: 1&nbsp;u&nbsp;·&nbsp;\"N\"&nbsp;=&nbsp;1&nbsp;g/mol. Die Avogadro-Konstante N, also die Anzahl Teilchen pro Mol, musste nach dieser Definition experimentell bestimmt werden und war mit einer Messunsicherheit behaftet.\n\nSeit 2019 ist N nicht mehr über die Masse des C-Atoms bestimmt, sondern per Definition exakt festgelegt. Daher haben die Masse eines Teilchens in u und die molare Masse in g nicht mehr exakt denselben Zahlenwert. Die Abweichung ist aber extrem klein und in der Praxis irrelevant:\n\n\nFrühere Definitionen.\nBis 1960 war die atomare Masseneinheit definiert als der Masse eines Sauerstoff-Atoms. Dabei bezogen sich die Chemiker auf die durchschnittliche Masse eines Atoms im natürlich vorkommenden Isotopengemisch des Elements O, die Physiker aber auf die Masse des Atoms des Hauptisotops O.\n\nDie Differenz zwischen der „chemischen“ Definition und der „physikalischen“ Definition (+2,8·10) war Anlass, eine vereinheitlichte Definition einzuführen. Über die Verhandlungen in den zuständigen Gremien wird anekdotisch berichtet, dass die Chemiker zunächst nicht bereit gewesen seien, auf die Definition der Physiker mit O einzuschwenken, da dies „erhebliche Verluste“ beim Verkauf von chemischen Substanzen zur Folge gehabt hätte. Schließlich überzeugten die Physiker die Chemiker mit dem Vorschlag, C als Basis zu nehmen, wodurch der Unterschied zur chemischen Definition nicht nur viel geringer war (−3,7·10), sondern auch in die „richtige Richtung“ ging und sich positiv auf die Verkaufserlöse auswirken würde.\n\nZwischen dem neuen und den beiden veralteten Werten der Einheit gilt die Beziehung\n\nDie Differenz zwischen der alten physikalischen und der heutigen Definition ist auf den Massendefekt zurückzuführen, der bei O höher ist als bei C.\n\n\nVerwendung.\nIn der Broschüre des Internationalen Büros für Maß und Gewicht („SI-Broschüre“) ist die Atomare Masseneinheit in der Liste der „zur Verwendung mit dem SI zugelassene Nicht-SI-Einheiten“ aufgeführt. In der 8.&nbsp;Auflage (2006) wurde der Einheitenname „Dalton“ erstmals hinzugefügt, gleichrangig als Synonym zum u. Die 9.&nbsp;Auflage (2019) nennt nur das Dalton und weist in einer Fußnote darauf hin, dass die „Atomare Masseneinheit (u)“ eine alternative Bezeichnung für dieselbe Einheit ist. In den gesetzlichen Regelungen der EU-Richtlinie 80/181/EWG für die Staaten der EU und im Bundesgesetz über das Messwesen in der Schweiz kommt der Ausdruck „Dalton“ nicht vor.\n\nSowohl für die atomare Masseneinheit als auch für das Dalton ist die Verwendung von Vorsätzen für dezimale Vielfache und Teile zulässig. Gebräuchlich sind das Kilodalton, 1&nbsp;kDa = 1000&nbsp;Da, sowie das Megadalton, 1&nbsp;MDa = 1.000.000&nbsp;Da.\n\n"}
{"id": "118", "url": "https://de.wikipedia.org/wiki?curid=118", "title": "Anglizismus", "text": "Anglizismus\n\nAls Anglizismus bezeichnet man einen sprachlichen Ausdruck, der aus dem Englischen in eine andere Sprache eingeflossen ist. Betroffen davon sind alle Bereiche eines Sprachsystems, von der Lautung über die Formenlehre, Syntax, Semantik bis zum Wortschatz, sowie die Bereiche Sprachgebrauch und Sprachebene (Fachsprache, Alltagssprache, Slang und anderes).\n\nFindet die Übernahme Akzeptanz von Seiten der Sprachgemeinschaft, werden die Ausdrücke als Fremd- und Lehnwort bzw. als neue Bedeutung eines Wortes oder als neue Satzkonstruktion übernommen. Im Laufe des Generationenwechsels kann sich sowohl diese Wertung als auch der Gebrauch von Anglizismen ändern. Insbesondere in der Jugendsprache verschwinden viele Ausdrücke mit der nächsten Generation wieder, da sie nicht mehr als neu und der Jugend vorbehalten empfunden werden.\n\nDer Begriff Anglizismus umfasst alle englischen Sprachvarietäten; Einflüsse speziell aus dem britischen Englisch werden auch \"Britizismen\" und solche aus dem amerikanischen Englisch \"Amerikanismen\" genannt.\n\n\nAnglizismen in der deutschen Sprache.\n\nErscheinungsformen.\nIm Deutschen treten Anglizismen am häufigsten auf der lexikalischen Ebene in Erscheinung. Man kann folgende Phänomene unterscheiden:\n\nWeitere Übernahmeerscheinungen sind auf anderen Sprachebenen zu verzeichnen:\n\n\nAnzahl und Häufigkeit.\nSprachwissenschaftliche Untersuchungen der Universität Bamberg stellen anhand von Material aus der Zeitung Die Welt eine Zunahme von Anglizismen in der deutschen Sprache fest. So hat sich von 1994 bis 2004 die Verwendung von Anglizismen\nEntgegen der allgemeinen Annahme, dass es beim Sprachkontakt vorwiegend zur Übernahme von Substantiven komme, wurden im untersuchten Zeitraum insgesamt etwa gleich viele Wörter aus jeder dieser drei Wortarten vom Englischen ins Deutsche entlehnt, allerdings bleiben die Substantive durchschnittlich länger im Gebrauch erhalten.\n\nDie Anzahl der Anglizismen hat zugenommen; ebenso die Häufigkeit, mit der diese verwendet werden. Klassifiziert man die Anglizismen nach Bereichen, lässt sich feststellen, dass der Bereich „Wirtschaft“ am stärksten gewachsen ist, vor allem im Marketing und Vertrieb (siehe Geml/Lauer, 2008). Einzige Ausnahme bildet der Bereich „Wissenschaft und Technik“, in welchem eine Abnahme um den Faktor 1,6 zu verzeichnen ist. Insgesamt lässt sich festhalten, dass der Gebrauch von Anglizismen in zehn Jahren um den Faktor 1,7 zugenommen hat. Hingegen hat die Entlehnungshäufigkeit im Vergleich zum Zeitraum 1954–1964 abgenommen. Das heißt, es werden mehr Anglizismen verwendet, die Geschwindigkeit der Übernahme hat aber abgenommen. Der Grund hierfür könnte ein Sättigungsprozess sein.\n\nIn einer weiteren Untersuchung wurde ein großes Textkorpus der Gegenwart (1995–2004) mit insgesamt 381191 Lemmata ausgewertet; darunter wurden 13301 = 3,5 % Anglizismen festgestellt. Das Textkorpus hat einen Umfang von rund 10.3 Millionen Token (= einzelne Wortformen), darunter 52647 = 0,5 % Anglizismen. Von den 13301 Anglizismen sind 12726 (95,68 %) (48190 Token = 91,53 %) Substantive, 307 (2,30 %) (1654 Token = 3,14 %) Adjektive, 255 (1,92 %) (2371 Token = 4,50 %) Verben und 13 (0,10 %) (432 Token = 0,82 %) Adverbien.\n\n\nEntwicklung der Anglizismen im Deutschen.\nAngaben dazu, wann welcher Anglizismus ins Deutsche gelangt ist, kann man vor allem aus Herkunftswörterbüchern (= etymologischen Wörterbüchern) gewinnen. Sie haben den Nachteil, dass sie nur einen Kernbestand des Wortschatzes enthalten, und zwar vor allem den Teil, der etymologisch besonders interessant ist. Es stellt sich also die Frage, ob der Trend der Entlehnungen, der in einem solchen Wörterbuch nachweisbar ist, auch für die Gesamtsprache repräsentativ ist. Dies muss man sich bewusst machen; mangels anderer Möglichkeiten bleibt aber nichts anderes übrig, wenn man sich eine Vorstellung von dem Verlauf der Entlehnungen machen will.\n\nEine solche Untersuchung hat Körner am Beispiel von \"Duden. Das Herkunftswörterbuch 2001\" durchgeführt, indem sie alle Entlehnungen erfasste, für die nach Auskunft dieses Wörterbuchs festgestellt werden kann, in welchem Jahrhundert sie aus welcher Sprache ins Deutsche gelangt sind. Speziell für die aus dem Englischen stammenden Entlehnungen kam Körner zu folgendem Ergebnis:\n\nDas Wörterbuch enthält 16781 datierbare Stichwörter, darunter 5244 Entlehnungen (Lehnwörter und Fremdwörter). Unter den Entlehnungen sind 519 datierbare Anglizismen. Man sieht, dass diese Entlehnungen aus dem Englischen erst recht spät einsetzen und dann aber eine erhebliche Dynamik entwickeln. Im 20. Jahrhundert erreichen die Anglizismen 3,1 % des gesamten erhobenen Wortschatzes beziehungsweise 9,9 % der Entlehnungen.\n\nStatt die Übernahme von Anglizismen im Deutschen generell zu untersuchen, kann man sich auch auf ihre Ausbreitung in speziellen Bereichen, etwa in bestimmten Presseorganen, konzentrieren. Eine solche Perspektive hat Gnatchuk am Beispiel der österreichischen \"KLEINE ZEITUNG\" durchgeführt und konnte zeigen, dass auch in diesem Fall der Übernahmeprozess dem Piotrowski-Gesetz entspricht.\n\n\nAnpassung an deutsche Sprachgewohnheiten.\nBesonders schon vor längerer Zeit entlehnte Wörter haben eine Anpassung der Schreibweise erfahren, etwa \"Keks\" gegenüber älterem \"Cakes\". Bei vor allem über den schriftlichen Verkehr übernommenen Anglizismen kann sich die Aussprache bei gleichbleibendem Schriftbild nach deutschen Aussprachegewohnheiten richten; so wird \"Jute\" heute im Deutschen gewöhnlich [] ausgesprochen, während ältere Wörterbücher noch die Aussprache [] verzeichnen.\n\n\nKritik und Kontroversen.\nWerden die englischen Einflüsse nicht allgemein akzeptiert, etwa weil sie auf einen Jargon oder die Jugendsprache beschränkt sind, spricht man von Neudeutsch oder abwertend von Denglisch.\n\nEine repräsentative Umfrage über die Verständlichkeit von zwölf gebräuchlichen englischen Werbeslogans für deutsche Kunden ergab im Jahr 2003, dass einige der Slogans von weniger als 10 % der Befragten verstanden wurden. Acht der zwölf untersuchten Unternehmen hätten ihre Werbeslogans seitdem geändert. 2008 störten sich in einer Umfrage der Gesellschaft für deutsche Sprache 39 % der Befragten an Lehnwörtern aus dem Englischen. Die Ablehnung war in den Bevölkerungsgruppen am größten, die Englisch weder sprechen noch verstehen konnten (58 % Ablehnung bei der Gruppe der über 59-Jährigen, 46 % Ablehnung bei ostdeutschen Umfrageteilnehmern).\n\nMitunter wird auch eine unzureichende Kenntnis der englischen Sprache für die Vermischung und den Ersatz bestehender Worte durch Scheinanglizismen verantwortlich gemacht. So sprechen einer Studie der GfK zufolge nur 2,1 Prozent der deutschen Arbeitnehmer verhandlungssicher Englisch. In der Gruppe der unter 30-Jährigen bewerten jedoch über 54 Prozent ihre Englischkenntnisse als gut bis exzellent. Für bessere Sprachkenntnisse könne demzufolge effizienterer Englischunterricht beitragen, und statt der Ton-Synchronisation von Filmen und Serien solle eine Untertitelung der englischsprachigen Originale mit deutschem Text erfolgen. Dies würde zugleich zu einer besseren Abgrenzung zwischen den Sprachen und einer Wahrung deutscher Sprachqualität beitragen.\n\nIm Dezember 2014 forderte der Europapolitiker Alexander Graf Lambsdorff, neben Deutsch die englische Sprache als Verwaltungs- und später als Amtssprache in Deutschland zuzulassen, um die Bedingungen für qualifizierte Zuwanderer zu verbessern, den Fachkräftemangel abzuwenden und Investitionen zu erleichtern. Einer repräsentativen YouGov-Umfrage zufolge würden es 59 Prozent der Deutschen begrüßen, wenn die englische Sprache in der gesamten Europäischen Union den Status einer Amtssprache erlangen würde.\n\nÄhnliche Kritik wie gegenüber den Anglizismen traf bereits ab Ende des 19. Jahrhunderts die aus dem Französischen, Lateinischen oder Griechischen stammenden Begriffe. Vereine wie der Allgemeine Deutsche Sprachverein versuchten im Rahmen des deutschen Sprachpurismus, diese Begriffe durch deutsche zu ersetzen. So sind französische, lateinische oder griechische Fremdwörter durch deutsche Wortschöpfungen ersetzt worden, z.&nbsp;B. \"Fahrkarte\" für \"Billet\", \"Abteil\" für \"Coupé\" und \"Bahnsteig\" für \"Perron\". Im Postwesen wurden auf Geheiß Bismarcks vom Generalpostmeister Heinrich von Stephan über 700 französischsprachige Begriffe durch deutsche Neuschöpfungen ersetzt. Zwar war die damalige Öffentlichkeit empört und man verhöhnte ihn als »Generalsprachmeister«, trotzdem sind Begriffe wie \"eingeschrieben\", \"postlagernd\" und \"Empfangsschein\" heute in den allgemeinen Sprachgebrauch übergegangen und ersetzen die Fremdwörter \"rekommandiert\", \"poste restante\" und \"Rezepisse\".\n\nViele Unternehmen setzen Anglizismen in Stellenangeboten bzw. -beschreibungen ein. Kritiker vermuten, dass weniger attraktive Stellen dadurch aufgewertet werden sollen. Häufig verwendete Begriffe sind \"Area-Manager\" (weniger als der klassische Abteilungsleiter), Facility-Manager (Hausmeister), \"Key Account Manager\" (Betreuer wichtiger Kunden) oder \"Case Manager\" (ein Fallbearbeiter, siehe Fallmanagement). Um diese Entwicklung zu karikieren, wird gelegentlich der Euphemismus \"WC-Manager\" (Klomann/-frau) genannt. In Frankreich stoßen Lehnwörter und Anglizismen noch stärker auf Kritik und sollen auch durch gesetzgeberische Maßnahmen wie die Loi Toubon eingedämmt werden. Eine aktive Sprachpolitik, wie sie unter anderem in Frankreich und Island betrieben wird, um eine Anreicherung der Sprache mit Anglizismen zu unterbinden, findet in Deutschland seit Mitte des 20.&nbsp;Jahrhunderts nicht mehr statt.\n\nDer Sprachwissenschaftler Rudolf Hoberg sah 2013 keine Bedrohung durch Anglizismen. Die deutsche Sprache habe schon immer englische Ausdrücke aufgenommen: „Nach der letzten Duden-Ausgabe haben wir etwa 3,5 Prozent Anglizismen, aber 20 Prozent andere Fremdwörter, über die sich die Leute meistens gar nicht aufregen“. Ebenso lehnt er gesetzliche Regelungen wie Sprachquoten in Frankreich oder Verfassungsänderungen wie in Österreich ab, die keine Erfolge zeigten. Der Germanist Karl-Heinz Göttert nannte die Aufregung über Anglizismen „komisch“: „Sie machen weniger als zwei Prozent des deutschen Wörterschatzes aus. Da gab und gibt es ganz andere Fremdwortschwemmen. Das Englische selbst hat im Mittelalter ein Drittel aus dem Französischen entlehnt. Und die japanische Sprache hat aus dem Chinesischen 50 Prozent übernommen.“ Sie seien „ein Beweis dafür, dass Nehmersprachen kreativ und nicht knechtisch mit dem Einfluss der Gebersprachen umgehen.“ Er wandte sich gegen eine „Leitkultur“ und kritisierte den Sprachpurismus mit den Worten: „Schon Jakob Grimm hat sich deshalb gegen den ärgerlichen Purismus gewendet. Es wäre besser, der Verein Deutsche Sprache würde sich auf die Grimm'sche Tradition besinnen, statt einen Grimm-Preis für Verdienste beim Anglizismen-Kampf zu vergeben.“\n\nAuch rechtsextreme Organisationen wie die NPD stören sich oft an Anglizismen und versuchen beispielsweise das nicht allgemein anerkannte Wort „Weltnetz“ statt „Internet“ zu etablieren.\n\n\nKulturpolitische Diskussion.\nDie Entwicklung des Englischen zur lingua franca im 20.&nbsp;Jahrhundert beeinflusst die meisten Sprachen der Welt. Mitunter werden einzelne Wörter ersetzt oder bei Neuerscheinungen ohne eigene Übersetzung übernommen. Diese Entwicklung wird vor allem dann skeptisch betrachtet, wenn es genügend Synonyme in der Landessprache gibt. Kritiker merken auch an, es handle sich häufig (beispielsweise bei \"Handy\" im Deutschen) um Scheinanglizismen.\n\nIn Frankreich gab es eine kulturpolitische Diskussion, die 1994 in ein „Gesetz betreffend den Gebrauch der französischen Sprache“ (Loi Toubon) führte.\n\n\n\n"}
{"id": "120", "url": "https://de.wikipedia.org/wiki?curid=120", "title": "Astronom", "text": "Astronom\n\nEin Astronom (von \"ástron\" ‚Stern, Gestirn‘ und νόμος \"nómos\" ‚Gesetz‘) ist eine (meist akademisch gebildete) Person, die sich wissenschaftlich mit der Astronomie beschäftigt.\n\n\nHaupttätigkeit der Astronomen.\nBeschränkt man den Begriff Astronom auf jene Wissenschaftler, die sich hauptberuflich der Astronomie widmen, dann sind meist zwei der folgenden Tätigkeiten Gegenstand des Berufs:\n\nDer Beruf des Fachastronomen setzt im Regelfall ein Hochschulstudium der Astronomie oder verwandter Naturwissenschaften voraus, etwa ein Diplom der Physik oder Astronomie (nur in Österreich), manchmal auch Studienabschlüsse aus Mathematik, Geodäsie, Aeronautik und anderen. Das Verfassen einer Dissertation schließt sich in den meisten Fällen an, denn die abgeschlossene Promotion gilt oft als Einstellungsvoraussetzung.\n\n\nGewandeltes Berufsbild.\nDas Berufsbild des Astronomen hat sich in den letzten Jahrzehnten stark gewandelt. In der Vergangenheit beobachteten Astronomen überwiegend den Himmel mittels optischer Teleskope an Sternwarten. Heute arbeiten die meisten Astronomen an sehr spezialisierten Fragestellungen und überwiegend am Computer. Sie verwenden elektromagnetische Signale aus allen Wellenlängenbereichen, von der kurzwelligen Gammastrahlung bis zu den längsten Radiowellen. Viele Messdaten werden auch über das Internet verbreitet – insbesondere bei regelmäßigen internationalen Messkampagnen wie im IVS – beziehungsweise vom Netz übernommen.\n\nDaher arbeiten Astronomen heute kaum mehr am Fernrohr selbst, sondern nur einen vergleichsweise kurzen Teil ihrer Arbeitszeit in den Kontrollräumen der Sternwarten. Die dort gewonnenen Daten werden meist außerhalb der Nachtdienste ausgewertet und aufbereitet. Immer mehr gewinnt das so genannte „service mode observing“ (Beobachtung auf Abruf) an Bedeutung: es werden nur Beobachtungsziel und -art angegeben werden, während die Beobachtungen unabhängig oder automatisiert an den Teleskopen beziehungsweise von Erdsatelliten durchgeführt werden.\n\n\nFach- und Amateurastronomen.\nDa viele Studenten des Faches später auf anderen Gebieten arbeiten, hängt es von ihrem Selbstverständnis ab, ob sie sich auch weiterhin als Astronom bezeichnen. Inwieweit wissenschaftlich tätige Amateurastronomen als Astronomen im eigentlichen Sinn zu nennen sind, ist ebenfalls offen. Besonders in früheren Jahrhunderten ist eine Trennung zwischen Fachastronom und Amateur wenig zweckmäßig, wie etwa das Beispiel von Wilhelm Olbers zeigt.\n\nDa die Astronomie nach wie vor eine Wissenschaft ist, die auch im professionellen Bereich von einzelnen und kleinen Forschungsgruppen geprägt ist, haben auch Amateure mit der entsprechenden Begabung und Ausrüstung die Möglichkeit, mitzuwirken. Amateure sind oft dort erfolgreich, wo eine kontinuierliche Beobachtung notwendig ist, aber wegen der Kosten durch Großteleskope kaum professionell machbar ist, etwa die Asteroiden- und Kometenüberwachung oder auf dem Gebiet veränderlicher Sterne sowie der Astrometrie.\n\n\nDienstzeit.\nDie Zeiten des „astronomischen Schlafmangels“, über die auch berühmte Astronomen manchmal in ihren Briefen oder Berichten geklagt haben, sind größtenteils vorüber. Moderne Sternwarten sind meistens mit Technologien ausgerüstet, die ein gewisses Maß an Fernbedienung erlauben oder sogar international anbieten, wie z.&nbsp;B. einige Observatorien auf Hawaii oder ESO-Sternwarten wie in Chile. Da visuelle Messungen oder Kontrollen nun selten erforderlich sind und elektro-optische Sensoren grundsätzlich auch eine Funktionskontrolle über EDV oder über das Internet erlauben, werden durchgehend nächtliche Arbeitszeiten zunehmend seltener.\n\n\n"}
{"id": "121", "url": "https://de.wikipedia.org/wiki?curid=121", "title": "Alan Turing", "text": "Alan Turing\n\nAlan Mathison Turing OBE, FRS [] (*&nbsp;23. Juni 1912 in London; † 7. Juni 1954 in Wilmslow, Cheshire) war ein britischer Logiker, Mathematiker, Kryptoanalytiker und Informatiker. Er gilt heute als einer der einflussreichsten Theoretiker der frühen Computerentwicklung und Informatik. Turing schuf einen großen Teil der theoretischen Grundlagen für die moderne Informations- und Computertechnologie. Als richtungsweisend erwiesen sich auch seine Beiträge zur theoretischen Biologie.\n\nDas von ihm entwickelte Berechenbarkeitsmodell der Turingmaschine bildet eines der Fundamente der Theoretischen Informatik. Während des Zweiten Weltkrieges war er maßgeblich an der Entzifferung der mit der „Enigma“ verschlüsselten deutschen Funksprüche beteiligt. Der Großteil seiner Arbeiten blieb auch nach Kriegsende unter Verschluss.\n\nTuring entwickelte 1953 eines der ersten Schachprogramme, dessen Berechnungen er mangels Hardware selbst vornahm. Nach ihm benannt sind der Turing Award, die bedeutendste Auszeichnung in der Informatik, sowie der Turing-Test zum Überprüfen des Vorhandenseins von künstlicher Intelligenz.\n\nIm März 1952 wurde Turing wegen seiner Homosexualität, die damals noch als Straftat verfolgt wurde, zur chemischen Kastration verurteilt. Turing erkrankte in Folge der Hormonbehandlung an einer Depression und starb etwa zwei Jahre später durch Suizid. Im Jahr 2009 sprach der damalige britische Premierminister Gordon Brown eine offizielle Entschuldigung im Namen der Regierung für die „entsetzliche Behandlung“ Turings aus und würdigte dessen „außerordentliche Verdienste“ während des Krieges; eine Begnadigung wurde aber noch 2011 trotz einer Petition abgelehnt. Am Weihnachtsabend, dem 24. Dezember 2013, sprach Königin Elisabeth&nbsp;II. postum ein „Royal Pardon“ (Königliche Begnadigung) aus.\n\n\nLeben und Wirken.\n\nKindheit und Jugend.\nAlan Turings Vater, Julius Mathison Turing, war britischer Beamter beim Indian Civil Service. Er und seine Frau Ethel Sara (geborene Stoney) wünschten, dass ihre Kinder in Großbritannien aufwüchsen. Deshalb kehrte die Familie vor Alans Geburt aus Chatrapur, damals Britisch-Indien, nach London-Paddington zurück, wo Alan Turing am 23. Juni 1912 zur Welt kam. Da der Staatsdienst seines Vaters noch nicht beendet war, reiste dieser im Frühjahr 1913 erneut nach Indien, wohin ihm seine Frau im Herbst folgte. Turing und sein älterer Bruder John wurden nach St. Leonards-on-the-Sea, Hastings, in die Familie eines pensionierten Obersts und dessen Frau in Pflege gegeben. In der Folgezeit pendelten die Eltern zwischen England und Indien, bis sich Turings Mutter 1916 entschied, längere Zeit in England zu bleiben, und die Söhne wieder zu sich nahm.\n\nSchon in früher Kindheit zeigte sich die hohe Begabung und Intelligenz Turings. Es wird berichtet, dass er sich innerhalb von drei Wochen selbst das Lesen beibrachte und sich schon früh zu Zahlen und Rätseln hingezogen fühlte.\n\nIm Alter von sechs Jahren wurde Turing auf die private Tagesschule St. Michael’s in St. Leonards-on-the-Sea geschickt, wo die Schulleiterin frühzeitig seine Begabung bemerkte. 1926, im Alter von 14 Jahren, wechselte er auf die Sherborne School in Dorset. Sein erster Schultag dort fiel auf einen Generalstreik in England. Turing war jedoch so motiviert, dass er die 100 Kilometer von Southampton zur Schule allein auf dem Fahrrad zurücklegte und dabei nur einmal in der Nacht an einer Gaststätte Halt machte; so berichtete jedenfalls die Lokalpresse.\n\nTurings Drang zur Naturwissenschaft traf bei seinen Lehrern in Sherborne auf wenig Gegenliebe; sie setzten eher auf Geistes- als auf Naturwissenschaften. Trotzdem zeigte Turing auch weiterhin bemerkenswerte Fähigkeiten in den von ihm geliebten Bereichen. So löste er für sein Alter fortgeschrittene Aufgabenstellungen, ohne zuvor irgendwelche Kenntnisse der elementaren Infinitesimalrechnung erworben zu haben.\n\nIm Jahr 1928 stieß Turing auf die Arbeiten Albert Einsteins. Er verstand sie nicht nur, sondern entnahm einem Text selbständig Einsteins Bewegungsgesetz, obwohl dieses nicht explizit erwähnt wurde.\n\n\nCollegezeit und theoretische Arbeiten.\nTurings Widerstreben, für Geisteswissenschaften genauso hart wie für Naturwissenschaften zu arbeiten, hatte zur Folge, dass er einige Male durch die Prüfungen fiel. Weil dies seinen Notendurchschnitt verschlechterte, musste er 1931 auf ein College zweiter Wahl gehen, das King’s College, Cambridge, entgegen seinem Wunsch, am Trinity College zu studieren. Er studierte von 1931 bis 1934 unter Godfrey Harold Hardy (1877–1947), einem respektierten Mathematiker, der den Sadleirian Chair in Cambridge innehatte, das zu der Zeit ein Zentrum der mathematischen Forschung war.\n\nIn seiner für diesen Zweig der Mathematik grundlegenden Arbeit \"On Computable Numbers, with an Application to the „Entscheidungsproblem“\" (28. Mai 1936) formulierte Turing die Ergebnisse Kurt Gödels von 1931 neu. Er ersetzte dabei Gödels universelle, arithmetisch-basierte formale Sprache durch einen einfachen gedanklichen Mechanismus, eine abstrakt-formale Zeichenketten verarbeitende mathematische Maschine, die heute unter dem Namen Turingmaschine bekannt ist. („Entscheidungsproblem“ verweist auf eines der 23 wichtigsten offenen Probleme der Mathematik des 20. Jahrhunderts, vorgestellt von David Hilbert 1900 auf dem 2. Internationalen Mathematiker-Kongress in Paris [„Hilbertsche Probleme“].) Turing bewies, dass solch ein Gerät in der Lage ist, „jedes vorstellbare mathematische Problem zu lösen, sofern dieses auch durch einen Algorithmus gelöst werden kann“.\n\nTuringmaschinen sind bis zum heutigen Tag einer der Schwerpunkte der Theoretischen Informatik, nämlich der Berechenbarkeitstheorie. Mit Hilfe der Turingmaschine gelang Turing der Beweis, dass es keine Lösung für das Entscheidungsproblem gibt. Er zeigte, dass die Mathematik in gewissem Sinne unvollständig ist, weil es allgemein keine Möglichkeit gibt, festzustellen, ob eine beliebige, syntaktisch korrekt gebildete mathematische Aussage beweisbar oder widerlegbar ist. Dazu bewies er, dass das Halteproblem für Turingmaschinen nicht lösbar ist, d.&nbsp;h., dass es nicht möglich ist, algorithmisch zu entscheiden, ob eine Turingmaschine, angesetzt auf eine Eingabe (initiale Bandbelegung), jemals zum Stillstand kommen wird, das heißt die Berechnung terminiert. Turings Beweis wurde erst nach dem von Alonzo Church (1903–1995) mit Hilfe des Lambda-Kalküls geführten Beweis veröffentlicht; unabhängig davon ist Turings Arbeit beträchtlich einfacher und intuitiv zugänglich. Auch war der Begriff der „Universellen (Turing-) Maschine“ neu, einer Maschine, welche jede beliebige andere Turing-Maschine simulieren kann. Die Eingabe für diese Maschine ist also ein verschlüsseltes Programm, das von der universellen Maschine interpretiert wird, und der Startwert, auf den es angewendet werden soll.\n\nAlle bis heute definierten Berechenbarkeitsbegriffe haben sich (bis auf die Abbildung von Worten auf Zahlen und umgekehrt) als äquivalent erwiesen.\n\n1938 und 1939 verbrachte Turing zumeist an der Princeton University und studierte dort unter Alonzo Church. 1938 erwarb er den Doktorgrad in Princeton. Seine Doktorarbeit führte den Begriff der „Hypercomputation“ ein, bei der Turingmaschinen zu sogenannten Orakel-Maschinen erweitert werden. So wurde das Studium von nicht-deterministisch lösbaren Problemen ermöglicht.\n\n\nKarriere und Forschung.\nNach seiner Rückkehr nach Cambridge im Jahr 1939 besuchte Turing Vorlesungen des österreichisch-britischen Philosophen Ludwig Wittgenstein (1889–1951) über die Grundlagen der Mathematik. Die Vorlesungen wurden Wort für Wort aus den Notizen der Studenten rekonstruiert, einschließlich Zwischenrufe von Turing und anderen Studenten. Die beiden diskutierten und stritten vehement: Turing verteidigte den mathematischen Formalismus, während Wittgenstein der Meinung war, dass Mathematik überbewertet sei und keine absolute Wahrheit zutage bringen könne.\n\n\nKryptoanalyse.\nWährend des Zweiten Weltkriegs war Turing einer der herausragenden Wissenschaftler bei den erfolgreichen Versuchen in Bletchley Park, verschlüsselte deutsche Funksprüche zu entziffern. Er steuerte einige mathematische Modelle bei, um sowohl die Enigma (siehe auch: Letchworth-Enigma) als auch die Lorenz-Schlüsselmaschine (siehe auch: \"Turingery\") zu brechen. Die Einblicke, die Turing bei der Kryptoanalyse der \"Fish\"-Verschlüsselungen gewann, halfen später bei der Entwicklung des ersten digitalen, programmierbaren elektronischen Röhrencomputers ENIAC. Konstruiert von Max Newman und seinem Team und gebaut in der \"Post Office Research Station\" in Dollis Hill von einem von Tommy Flowers angeführten Team im Jahr 1943, entzifferte Colossus die Lorenz-Maschine. Auch konzipierte Turing die nach ihm benannten \"Bombes\". Sie waren Nachfolgerinnen der von dem Polen Marian Rejewski entwickelten \"Bomba\" und dienten zur Ermittlung der Schlüssel von Enigma-Nachrichten. Dabei handelte es sich um ein elektromechanisches Gerät, das im Prinzip mehrere Enigma-Maschinen beinhaltete und so in der Lage war, viele mögliche Schlüsseleinstellungen der Enigma-Nachrichten durchzutesten und zu eliminieren, bis eine mögliche Lösung gefunden war (\"Reductio ad absurdum\"; ).\n\nTurings Mitwirkung als einer der wichtigsten Codeknacker bei der Entzifferung der Enigma war bis in die 1970er Jahre geheim; nicht einmal seine engsten Freunde wussten davon. Die Entzifferung geheimer deutscher Funksprüche war eine kriegsentscheidende Komponente für den Sieg der Alliierten im U-Boot-Krieg und im Afrikafeldzug.\n\n\nArbeit an frühen Computern – Der Turing-Test.\nVon 1945 bis 1948 war Turing im \"National Physical Laboratory\" in Teddington tätig, wo er am Design der ACE (\"Automatic Computing Engine\") arbeitete. Der Name der Maschine ist abgeleitet von der Analytical Engine des Mathematikers Charles Babbage, dessen Werk Turing zeitlebens bewunderte.\n\nAb 1948 lehrte Turing an der Universität Manchester und wurde im Jahr 1949 stellvertretender Direktor der Computerabteilung. Hier arbeitete er an der Software für einen der ersten echten Computer, den Manchester Mark I und gleichzeitig weiterhin verschiedenen theoretischen Arbeiten. In \"Computing machinery and intelligence\" (\"Mind\", Oktober 1950) griff Turing die Problematik der künstlichen Intelligenz auf und schlug den Turing-Test als Kriterium vor, ob eine Maschine dem Menschen vergleichbar denkfähig ist. Da der Denkvorgang nicht formalisierbar ist, betrachtet der Test nur die Antworten einer Maschine im Dialog mit einem Menschen, d.&nbsp;h. das kommunikative Verhalten der Maschine. Wenn dieses von einem menschlichen Verhalten nicht unterscheidbar erscheint, soll von maschineller Intelligenz gesprochen werden. Er beeinflusste durch die Veröffentlichung die Entwicklung der Künstlichen Intelligenz maßgeblich.\n\n1952 schrieb er das Schachprogramm Turochamp. Da es keine Computer mit ausreichender Leistung gab, um es auszuführen, übernahm Turing dessen Funktion und berechnete jeden Zug selbst. Dies dauerte bis zu 30 Minuten pro Zug. Das einzige schriftlich dokumentierte Spiel verlor er gegen einen Kollegen.\n\n\nArbeit an mathematischen Problemen der Biologie.\nVon 1952 bis zu seinem Tod 1954 arbeitete Turing an mathematischen Problemen der theoretischen Biologie. Er veröffentlichte 1952 eine Arbeit zum Thema \"The Chemical Basis of Morphogenesis\". In diesem Artikel wurde erstmals ein Mechanismus beschrieben, wie Reaktions-Diffusions-Systeme spontan Strukturen entwickeln können. Dieser als Turing-Mechanismus bekannte Prozess steht noch heute im Mittelpunkt vieler chemisch-biologischer Strukturbildungstheorien. Turings weiteres Interesse galt dem Vorkommen der Fibonacci-Zahlen in der Struktur von Pflanzen. Spätere Arbeiten blieben bis zur Veröffentlichung seiner gesammelten Werke 1992 unveröffentlicht.\n\n\nVerfolgung wegen Homosexualität und Turings Tod.\n1952 half der 19-jährige Arnold Murray, zu dem Turing eine gleichgeschlechtliche Beziehung hatte, einem Komplizen dabei, in Turings Haus einzubrechen. Turing meldete daraufhin einen Diebstahl bei der Polizei, die ihm als Folge der Ermittlungen eine sexuelle Beziehung zu Murray vorwarf. Da homosexuelle Handlungen zu dieser Zeit in England – wie in den meisten anderen Ländern – strafbar waren, wurde Turing wegen „grober Unzucht und sexueller Perversion“ angeklagt. Turing sah keinen Anlass, sich wegen dieser Vorwürfe zu rechtfertigen.\n\nNach seiner Verurteilung zu einer Gefängnisstrafe wurde er vor die Wahl gestellt, die Haftstrafe anzutreten oder – da zu seiner Zeit Homosexualität von weiten Teilen der Psychiatrie als Krankheit angesehen wurde – sich behandeln zu lassen. Er entschied sich für die ärztliche Behandlung, zu der auch eine medikamentöse Behandlung mit dem Hormon Östrogen gehörte. Diesem wurde eine triebhemmende Wirkung zugeschrieben. Die Behandlung dauerte ein Jahr und führte zu Nebenwirkungen wie der Vergrößerung der Brustdrüse. Auch wenn er seine körperlichen Veränderungen mit Humor kommentierte, muss die Verweiblichung seiner Konturen den sportlichen Läufer und Tennisspieler schwer getroffen haben. Turing erkrankte an einer Depression. Im Herbst 1952 begann Turing seine Therapie bei dem aus Berlin stammenden und seit 1939 in Manchester lebenden Psychoanalytiker Franz Greenbaum. Dieser war ein Anhänger C.G. Jungs und war ihm von Freunden als für seinen Fall verständnisvoll empfohlen worden. Turing entwickelte auch ein freundschaftliches Verhältnis zur Familie Greenbaum, die er auch privat besuchte.\n\n1954 starb Turing, wahrscheinlich entsprechend der offiziellen Feststellung durch Suizid, an einer Cyanidvergiftung, dem Anschein nach von einem vergifteten Apfel herrührend, den man halb aufgegessen neben ihm auffand. Die Ermittler versäumten es jedoch, den Apfel auf Gift untersuchen zu lassen. Es wird berichtet, dass Turing seit 1938, nachdem er den Film „Schneewittchen und die sieben Zwerge“ gesehen hatte, immer wieder die Verse \"Dip the apple in the brew / Let the sleeping death seep through\" („Tauch den Apfel tief hinein / bis das Gift wird in ihm sein“, in der deutschen Version des Films: „Apfel färbt sich strahlend rot / lockt Schneewittchen in den Tod“) sang. Der These, dass Turings Tod ein Unfall im Zusammenhang mit einem chemischen Versuch war, wird von Andrew Hodges, einem seiner Biographen, entschieden widersprochen. Unter seinen Biographen ist die Annahme verbreitet, die Auswirkungen der Hormonbehandlung seien die Hauptursache für den Suizid gewesen.\n\n\nOffizielle Entschuldigung, Danksagung und Rehabilitierung.\nAb etwa den späten 2000er Jahren unternahmen britische Bürger eine Reihe von öffentlichkeitswirksamen Aktivitäten, um das von Turing erlittene Unrecht bekannt zu machen und seine formale Rehabilitierung zu erreichen, also einen Widerruf oder eine Aufhebung des damaligen Urteils. Dies führte im Jahr 2013 zum Erfolg.\n\nIm Jahr 2009 unterzeichneten rund 30.000 Briten eine bei der Regierung eingereichte Online-Petition, in der eine posthume Entschuldigung von der britischen Regierung gefordert wurde. Der Initiator der Kampagne, der britische Programmierer John Graham-Cumming, regte an, Alan Turing eine Ritterwürde zu verleihen. Am 10. September 2009 veröffentlichte der damalige britische Premierminister Gordon Brown eine Erklärung, in der er im Namen der britischen Regierung die Verfolgung Turings bedauerte und seinen außerordentlichen Beitrag während des Zweiten Weltkriegs würdigte. Dabei spielte er auch auf den strategischen Vorteil der Alliierten durch die Entschlüsselung der „Enigma“ an und unterstrich deren Bedeutung:\n\nDa die Strafverfolgung seiner sexuellen Ausrichtung damals gesetzeskonform war, wurde eine nachträgliche Aufhebung der Verurteilung Turings zunächst von offizieller Seite als unmöglich dargestellt. Noch 2012 weigerte sich die Regierung von Browns Nachfolger David Cameron, 49.000 Homosexuelle, die nach dem \"Criminal Law Amendment Act\" von 1885 verurteilt worden waren, postum zu rehabilitieren.\n\nIm Jahr 2013 wurde bekannt, dass die britische Regierung nun doch die Absicht habe, Turing zu rehabilitieren. Das Oberhausmitglied John Sharkey, Baron Sharkey beantragte dies. Das konservative Mitglied des Oberhauses Tariq Ahmad, Baron Ahmad of Wimbledon kündigte die Zustimmung der Regierung an. Der Liberaldemokrat Sharkey hatte in den 1960er Jahren in Manchester Mathematik bei Turings einzigem Doktoranden Robin Gandy studiert. Eine dritte Lesung des Antrags beraumte die Regierung für Ende Oktober an.\n\nAm 24. Dezember 2013 wurde Alan Turing durch ein allein dem Monarchen zustehendes besonderes Gnadenrecht begnadigt, dem sogenannten \"Royal Pardon\". Justizminister Chris Grayling hatte diese Begnadigung bei Elisabeth&nbsp;II. beantragt. Turing gilt damit auch als offiziell rehabilitiert.\n\nIm April 2016 entschuldigte sich Robert Hannigan, der damalige Leiter des britischen Geheimdienstes GCHQ, für die Behandlung von Homosexuellen durch seine Institution und bezog dies ausdrücklich auf Alan Turing.\n\n\nNachwirkungen der Rehabilitierung.\nAnfang 2015 verlangten Mitglieder der Familie Alan Turings unter weiterer, teils prominenter Unterstützung (Stephen Fry, Turing-Darsteller Benedict Cumberbatch) in einer Petition an das britische Parlament die Rehabilitierung auch aller anderen der in England unter den Homosexuellen-Gesetzen Verurteilten. Die Petition wurde von ca. 500.000 Personen unterschrieben und sollte von Turings Großneffen Nevil Hunt und der Großnichte Rachel Barns überreicht werden.\n\nAm 21. Oktober 2016 lehnte das britische Parlament einen Gesetzesentwurf ab, der eine Rehabilitierung in Form einer generellen Rehabilitierung aller lebenden, früher für Homosexualität verurteilten Personen vorsah. Dieser Gesetzesentwurf ging einigen zu weit, anderen nicht weit genug. Am 31. Januar 2017 wurde von Königin Elisabeth&nbsp;II. ein Gesetz in Kraft gesetzt, das aufbauend auf der Begnadigung von Turing allen Männern die Strafe aufhebt, falls zu dem Zeitpunkt beide über 16 Jahre alt waren, als sie den geahndeten Akt in gegenseitigem Einvernehmen vollzogen. Ausgenommen sind weiterhin Verurteilungen wegen homosexueller Handlungen in öffentlichen Toiletten. Das Gesetz schließt auch bereits verstorbene Personen ein. Ein noch lebender Betroffener kann beantragen, dass die Strafe aus seiner polizeilichen Führungsakte gestrichen wird, und Historiker können darauf hinweisen, dass eine Verurteilung verstorbener Personen nach geltendem Recht ungültig ist. Das Gesetz, das von Justizminister Sam Gyimah als „Turings Gesetz“ bezeichnet wurde, ist eine Ergänzung zum \"Policing and Crime Act\" und nimmt keinen Bezug auf andere Gesetze, unter denen homosexuelle Handlungen verfolgt werden konnten. Von Michael Cashman, einem der Initiatoren des Gesetzes, wurden jedoch weitere Vereinbarungen abgesichert, die einen entsprechend umfassenden Straferlass für alle homosexuellen Handlungen ermöglichen.\n\n\nPostume Ehrungen.\nAm 23. Juni 1998, der Turings 86. Geburtstag gewesen wäre, wurde eine Blaue Plakette (\"English Heritage Blue Plaque\") an seinem Geburtshaus in Warrington Crescent, London, enthüllt.\n\nAm 2. März 1999 wurde der Asteroid (10204) Turing nach ihm benannt.\n\nEine Turing-Statue wurde am 23. Juni 2001 in Manchester enthüllt. Sie steht im Sackville Park, zwischen den wissenschaftlichen Gebäuden der Universität Manchester und dem bei Homosexuellen beliebten Viertel der Canal Street.\n\nAn seinem 50. Todestag, dem 7. Juni 2004, wurde zum Gedenken an Turings frühzeitigen Tod eine Tafel an seinem früheren Haus „Hollymeade“ in Wilmslow enthüllt.\n\nDer Turing Award wird jährlich von der \"Association for Computing Machinery\" an Personen verliehen, die bedeutende Beiträge zur Informatik geleistet haben. Er wird weithin als „Nobelpreis“ der Informatik angesehen.\n\nDer \"Bletchley Park Trust\" hat am 19. Juni 2007 eine Statue Turings in Bletchley Park enthüllt. Die Skulptur wurde von Stephen Kettle gestaltet, der als Material für sein Kunstwerk walisischen Schiefer verwendete.\n\nIm „Turing-Jahr 2012“ fanden zu Alan Turings hundertstem Geburtstag weltweit Veranstaltungen zur Würdigung seiner Leistungen und zum Gedenken daran statt.\n\nIm Jahr 2014 wurde er in die \"Hall of Honor\" (Ehrenhalle) des US-Geheimdienstes NSA (\"National Security Agency\") aufgenommen.\n\n\n\nWerke.\n\nVeröffentlichungen.\nWichtige Veröffentlichungen\n\nEnglische Ausgaben\n\nDeutsche Ausgabe und Übersetzungen\n\n\n\n\n\n\n\n\n\n\n\n\nWeblinks.\nArtikel\n"}
{"id": "123", "url": "https://de.wikipedia.org/wiki?curid=123", "title": "Archäologie", "text": "Archäologie\n\nDie Archäologie ( und λόγος \"lógos\" ‚Lehre‘; wörtlich also „Lehre von den Altertümern“) ist eine Wissenschaft, die mit naturwissenschaftlichen und geisteswissenschaftlichen Methoden die kulturelle Entwicklung der Menschheit erforscht. Sie hat sich weltweit zu einem Verbund unterschiedlichster theoretischer und praktischer Fachrichtungen entwickelt.\n\nDie Archäologie interessiert sich ausschließlich für den Menschen und seine materiellen Hinterlassenschaften, wie etwa Gebäude, Werkzeuge und Kunstwerke. Sie umfasst einen Zeitraum von den ersten Steinwerkzeugen vor etwa 2,5&nbsp;Millionen Jahren bis in die nähere Gegenwart. Aufgrund neuer Funde in Afrika, die etwa 3,3&nbsp;Millionen Jahre alt sind, wird auch ein deutlich früherer Beginn der Werkzeugherstellung in Betracht gezogen. Materielle Hinterlassenschaften der jüngsten Geschichte (beispielsweise Konzentrationslager und Bunkerlinien aus dem Zweiten Weltkrieg) werden heute ebenfalls mit archäologischen Methoden ausgewertet, auch wenn dieser Ansatz einer „zeitgeschichtlichen“ Archäologie fachintern umstritten ist.\n\nObwohl die Archäologie eine verhältnismäßig junge Wissenschaft ist, ist es kaum mehr möglich, alle Zeiträume zu überblicken, so dass sich verschiedene Fachrichtungen herausbildeten. Dabei können die Epochen regional unterschiedlich datiert sein, teilweise sind sie nicht überall dokumentierbar. Neben der Orientierung an Epochen (z.&nbsp;B. Mittelalterarchäologie) oder Regionen (z.&nbsp;B. Vorderasiatische Archäologie) gibt es auch die Spezialisierung auf bestimmte Themengebiete (z.&nbsp;B. Christliche Archäologie, Rechtsarchäologie, Industriearchäologie).\n\nObwohl die Methodik sich großteils ähnelt, können die Quellen unterschiedlich sein. In der Vor- und Frühgeschichte hat man es hauptsächlich mit materieller Kultur zu tun, in der Frühgeschichte kann dabei teils auf Schriftquellen zurückgegriffen werden. Diese stehen für Archäologen im Gegensatz zu Wissenschaftlern anderer Teildisziplinen der Geschichtswissenschaft aber nicht im Mittelpunkt.\nErkenntnisse zu Umwelt, Klima, Ernährung oder zum Alter von Funden tragen zur Rekonstruktion vergangener Kulturen bei.\n\n\nForschungsgeschichte.\n\nAnfänge der Altertumsforschung in Europa.\nIn Europa entwickelte sich die Archäologie um 1450, weil man Zeugnisse für die in den Quellen der Antike geschilderten Ereignisse finden wollte. Cyriacus von Ancona (* um 1391; † um 1455), ein italienischer Kaufmann und Humanist, gilt als einer der Gründungsväter der modernen Klassischen Archäologie.\n\nDie in der Renaissance einsetzende Wiedergeburt klassisch-antiker Gelehrsamkeit führte im 15. und 16. Jahrhundert zu einem gesteigerten Interesse an griechischen und römischen Altertümern und zu einer Welle der Sammelleidenschaft antiker Kunstgegenstände. Doch auch weniger reisefreudige Gelehrte begannen, sich für die vorhandenen Zeugnisse vergangener Zeiten zu interessieren.\n\nAb Mitte des 16. Jahrhunderts trat an die Stelle der Sammelleidenschaft die akribische Erfassung der Denkmäler. In dieser Zeit wurden zahlreiche Enzyklopädien und Kataloge veröffentlicht, im späten 16. Jahrhundert vielfach mit Kupferstichen und Holzschnitten illustriert. In England veröffentlichte William Camden (1551–1632) im Jahre 1586 seine \"Britannia\", einen Katalog der sichtbaren Altertümer. Bemerkenswert ist, dass er bereits Bewuchsmerkmale in Kornfeldern bemerkte und als solche interpretierte.\n\nMichele Mercati (1541–1593) gilt als der erste europäische Gelehrte, der Steinwerkzeuge als solche einstufte; sein Werk wurde jedoch erst 1717 veröffentlicht. Trotz großer Popularität hatte die Archäologie als Wissenschaft noch keinen Stellenwert, denn es herrschte die Ansicht vor, dass ausschließlich historische Quellen und die Bibel zur Interpretation der Vergangenheit geeignet seien. So galt es noch lange als ein Faktum, dass –&nbsp;wie James Ussher aus der Bibel ableitete&nbsp;– die Menschheit im Oktober 4004 v. Chr. entstand. 1655 wagte es Isaac de La Peyrère, die sogenannten Donnerkeile (Steinzeitartefakte) Menschen zuzuordnen, die vor Adam lebten (Präadamiten-Hypothese). Nach einer Intervention der Inquisition widerrief er seine Theorie.\n\nIn Skandinavien wurden Bodendenkmäler schon früh beachtet. Bereits 1588 grub man einen Dolmen bei Roskilde aus. Im Jahre 1662 erhielt Uppsala einen Lehrstuhl für Altertumskunde. 1685 wurde in Houlbec-Cocherel in Nordfrankreich eine neolithische Grabkammer ausgegraben. Sie gilt als die älteste archäologische Grabung, weil hier 1722 der erste erhaltene Grabungsbericht erstellt wurde. Der Kieler Professor Johann Daniel Major führte um 1690 umfangreiche Ausgrabungen in Jütland durch und ließ zahlreiche Hügelgräber öffnen. Sein Ziel war es, die Herkunft der Einwohner der Halbinsel mit archäologischen Methoden zu klären.\n\nBernard de Montfaucons \"L’Antiquité expliquée\" erschien ab 1719. In zehn Bänden stellte er Kunstgegenstände aus dem Mittelmeerraum dar. Montfaucons Werk blieb für lange Zeit das Standardwerk.\n\n\nMitte des 18. bis Mitte des 19. Jahrhunderts.\nArchäologische Forschungsmethoden setzten sich nun sukzessiv durch. Oftmals trafen einzelne Gelehrte schon früh bahnbrechende Schlussfolgerungen, die aber oft – da noch nicht zeitgemäß – unbeachtet blieben. Einer der Bahnbrecher war der französische Amateurarchäologe Jacques Boucher de Perthes, der als Erster prähistorische Artefakte richtig zuordnete, wofür ihm aber erst mehr als 20 Jahre später, durch die Bestätigung Charles Lyells (1797–1875), Anerkennung zuteilwurde. Eine wichtige Erkenntnis war die Entdeckung des stratigraphischen Prinzips. Bereits lange vorher war die Zusammengehörigkeit und somit Gleichaltrigkeit von Funden, die sich in einer Schicht befanden (beispielsweise ein Steinartefakt im Fundzusammenhang mit einer ausgestorbenen Tierart), immer wieder diskutiert, aber nicht allgemein akzeptiert worden.\n\nEin Modell, das in seinen Grundzügen noch heute gilt, wurde von 1836 von Christian Jürgensen Thomsen veröffentlicht. Er war Kurator in Kopenhagen und erfand das „Dreiperiodensystem“, das die Vorgeschichte der Menschheit in drei Phasen einteilt: die Steinzeit, die Bronzezeit und die Eisenzeit. Etwa 30 Jahre später, um 1865, unterschied J. Lubbock die Steinzeit noch in die des geschlagenen und die des geschliffenen Steins. Die Begriffe „Paläolithikum“ (Altsteinzeit) und „Neolithikum“ („Neusteinzeit“/Jungsteinzeit) waren geboren. Die Epochen sind in sich vielfach untergliedert, aber die damals gefundene Unterteilung gilt – mit Einschränkungen – bis heute.\nDie ersten großen Ausgrabungen fanden in den antiken Städten Pompeji und Herculaneum statt. Beide waren laut einem Bericht des römischen Schriftstellers Plinius des Jüngeren am 24. August 79 n. Chr. durch den Ausbruch des Vesuvs ausgelöscht worden. Pompeji wurde Ende des 16. Jahrhunderts beim Bau einer Wasserleitung wiederentdeckt. 1748 begannen die Grabungen. In Herculaneum wurde erstmals 1709 gegraben, 1738 ließ Karl&nbsp;III. von Neapel die Stadt gezielt ausgraben. 1768 konnte das Theater, die Basilika und die Villa dei Papiri freigelegt werden.\n\nMit seinem \"Sendschreiben von den Herculanischen Entdeckungen\", der ersten archäologischen Publikation, begründete Johann Joachim Winckelmann 1762 die neue Wissenschaft der Archäologie und gilt seither als \"Vater der (Klassischen) Archäologie\". Winckelmann war auch der Erste, der eine Periodisierung und geschichtliche Einordnung der griechischen Kunst versuchte. Seine Entwicklungsstufen (alter Stil – hoher Stil – schöner Stil – Stil der Nachahmer – Verfall der Kunst) sind durch die enthaltene Wertung jedoch überholt. Für die Verbreitung seiner Forschung und deren Rezeption in der zeitgenössischen Literatur und Kunst war der Göttinger Professor Christian Gottlob Heyne entscheidend, der mit Winckelmann korrespondierte, seine Schriften rezensierte und bekanntmachte und in seinen Vorlesungen verwendete. 1802 wurde an der Christian-Albrechts-Universität zu Kiel der erste Lehrstuhl für klassische Archäologie eingerichtet.\n\nDie ägyptischen Baudenkmäler, allen voran die Pyramiden, waren bereits im Altertum beliebte Reiseziele (siehe Weltwunder). Im 17. Jahrhundert hatte sich die Erkenntnis durchgesetzt, dass es sich hierbei um Königsgräber handelt. Die Ägyptologie nahm mit Napoléon Bonapartes Ägypten-Feldzug 1798 ihren Anfang. In Begleitung des Heeres befanden sich auch Wissenschaftler. Von besonderer Bedeutung war der Fund des Steins von Rosetta, der 1822 Jean-François Champollion die Entzifferung der Hieroglyphen ermöglichte.\n\nVon besonderer Bedeutung für die ägyptische Archäologie ist Auguste Mariette (1821–1881), der ab 1858 als Direktor des ägyptischen Altertümerdienstes mehr als dreißig Fundstätten ausgrub. Seine Methoden waren brachial (beispielsweise Sprengladungen). Die Feststellung der Fundumstände und wissenschaftliche Auswertungen waren damals noch nicht festgelegt, aber er beendete die Ära der reinen Schatzsucher (so Giovanni Battista Belzoni, 1778–1823), die zuvor zahllose Funde nach Europa geschafft hatten. Mariette selbst hat seit 1850 rund 7000 Objekte nach Paris (Louvre) gebracht. Nun setzte er sich jedoch vehement dafür ein, dass Ägyptens Altertümer nicht mehr außer Landes verschleppt wurden. Zur Aufbewahrung der Funde gründete Mariette den Vorläufer des Ägyptischen Nationalmuseums in Kairo. Karl Richard Lepsius (1810–1884) erstellte zwischen 1842 und 1845 eine umfassende Aufnahme ägyptischer und nubischer Denkmäler. 1859 wurde das Ergebnis in den zwölf Bänden der \"Denkmaeler aus Aegypten und Aethiopien\" veröffentlicht, welche allein 894 Farbtafeln enthalten. Um die archäologische Erforschung Griechenlands machte sich um 1840 besonders Ludwig Ross verdient, der als erster systematische Ausgrabungen auf der Akropolis von Athen durchführte.\n\n\nAb Mitte des 19. Jahrhunderts.\nMitte des 19. Jahrhunderts entwickelte sich die Archäologie zunehmend zur Wissenschaft. Unterschieden sich die Ausgräber bisher nur unwesentlich von Schatzsuchern und Grabräubern, wurden nun die Grabungstechniken verfeinert, eine gute Dokumentation und exakte Einordnung der Funde wurden immer wichtiger.\n\nErst ab 1859 wurde das hohe Alter der Menschheit allgemein anerkannt. Im selben Jahr erschien Darwins \"Über die Entstehung der Arten\". Der bereits 1856 entdeckte Fund des Neandertalers, der von Johann Carl Fuhlrott und Hermann Schaaffhausen vergeblich als eiszeitlich eingestuft wurde, konnte sich als solcher in Deutschland erst ab 1902 durchsetzen, als Rudolf Virchow starb, der als pathologische Autorität jede weiterführende Diskussion unterbunden hatte.\n\nIn Schweden entwickelte Oscar Montelius (1843–1921) ein System der differenzierten Typologie zur Einordnung (Periodisierung) von Fundstücken und schafft die Grundlage einer relativen Chronologie.\n\n1853/54 wurden aufgrund eines ungewöhnlich niedrigen Wasserstandes bei Obermeilen am Zürichsee hölzerne Pfeiler, Steinbeile und Keramik entdeckt. Die Siedlung wurde von Ferdinand Keller untersucht. Lange Zeit glaubt man, bei diesen Feuchtbodensiedlungen habe es sich um Pfahlbauten im Wasser gehandelt. Ab den 1920er Jahren entspann sich eine heftige Diskussion um die Lage der Pfahlbauten. Es konkurrierten Ufer- und Wasserpfahlbauten. Heute weiß man, dass es Land- und Wasserpfahlbauten gab. Die neuen Untersuchungen in Hornstaad am Bodensee belegen Pfahlbauten im Wasser, bis zu 5 Meter vom Seeboden abgehoben. Rekonstruktionen (beispielsweise in Unteruhldingen am Bodensee) zeigen nicht nur die verschiedenen Lösungsvorschläge der Archäologie, sondern auch den aktuellen Forschungsstand nach den Befunden der Unterwasserarchäologie (Pfahlbaumuseum Unteruhldingen).\n\n1846 beginnen die Ausgrabungen in Hallstatt. Die archäologische Erforschung der Kelten begann 1858, als Oberst Schwab die ersten Ausgrabungen in La Tène am Neuenburgersee (Schweiz) durchführte. 1872 wurde die Eisenzeit Europas erstmals in eine ältere Phase (Hallstattzeit) und einer jüngeren (Latènezeit) unterteilt.\n\nÉdouard Lartet (1801–1871) untersuchte 1860 eine Fundstätte in den Pyrenäen (Massat) und fand dabei auch eine Geweihspitze mit eingraviertem Bärenkopf, der erste Fund jungpaläolithischer Kunst. Später grub er mehrere französische Höhlenfundplätze (Gorge d’Enfer, Laugerie-Haute, La Madeleine und Le Moustier) aus. Besondere Aufmerksamkeit erlangten die großartigen Höhlenmalereien, die 1879 in der Höhle von Altamira entdeckt wurden.\n\nDie Entwicklung der Klassischen Archäologie in der zweiten Hälfte des 19. Jahrhunderts wurde von Heinrich Schliemann (1822–1890) dominiert. Der Geschäftsmann und „Hobbyarchäologe“ Schliemann gilt als Begründer der Vorgeschichtsarchäologie Griechenlands und des ägäischen Raumes. 1869 grub er auf Ithaka und 1871 begann er in Hissarlik zu graben. Dort vermutet er das Troja Homers und wird recht behalten, obwohl er sich in der Bauperiode selbst täuschte. Seine Ausgrabungsmethoden waren sehr umstritten, so mancher Fachmann hielt von Schliemanns Fähigkeiten nichts. Sein Ruhm stützt sich vor allem auf die wertvollen Funde (beispielsweise „Schatz des Priamos“). Seine Entdeckung prähistorischer (vorhomerischer) Kulturen und Siedlungen löste zahlreiche weitere Grabungen im ägäischen Raum aus. Lange unterschätzt wurden die durch ihn bewirkten methodischen Fortschritte, wie die Betonung der Stratigraphie oder der Einsatz der Fotografie als Mittel der archäologischen Dokumentation.\n\n1892 erhielt der Gründer des Instituts für Ur- und Frühgeschichte an der Universität Wien, Moritz Hoernes, die erste das Gesamtgebiet der Prähistorischen Archäologie umfassende Lehrbefugnis Europas.\n\n\n20. und 21. Jahrhundert.\nIn Ägypten leistete ab 1880 Sir William Matthew Flinders Petrie (1853–1942) als Forscher und Ausgräber Pionierarbeit. Ein Meilenstein der archäologischen Forschung sind seine \"Methoden und Ziele der Archäologie\", die er 1904 veröffentlichte. Darin legte Flinders Petrie vier Prinzipien dar:\n\n1913 erschien der erste Band des \"Handbuchs der Archäologie\", Herausgeber war Heinrich Bulle (1867–1945). Als vorbildliche Grabung dieser Zeit galt die 1922 begonnene Ausgrabung des Gräberfeldes von Assini (Argolis), die von schwedischen Archäologen vorgenommen wurde. Der gesamte Aushub wurde gesiebt und eine erstklassige Grabungsdokumentation erstellt. Der berühmteste archäologische Fund des 20. Jahrhunderts gelang Howard Carter (1873–1939) im selben Jahr. Er fand nach sechsjähriger Suche das Grab des Tut-anch-Amun.\n\nPionier der Luftbildarchäologie war nach dem Ersten Weltkrieg der britische Pilot Osbert G. S. Crawford, er fotografierte vom Flugzeug aus archäologische Fundstätten in England.\n\nGustaf Kossinna (1858–1931) stellte 1920 seine siedlungsarchäologischen Methoden vor. Seine Interpretationen, die den Germanen eine überragende kulturelle Bedeutung zuschrieben, dienten dem Nationalsozialismus als Beweis für die Überlegenheit der Germanen und der arischen Rasse. Die Diskreditierung in der Nachkriegszeit führte dazu, dass auf Jahrzehnte eine Anbindung archäologischer Funde an ethnische Gruppen obsolet war.\n\nDie erste ordentliche Professur wurde 1927 in Marburg geschaffen und im folgenden Jahr mit Gero Merhart von Bernegg aus Bregenz besetzt. Er hatte sich 1924 mit \"Die Bronzezeit am Jenissei\" habilitiert. Bei ihm promovierten bis zu seiner Zwangspensionierung durch die Nationalsozialisten im Jahr 1942 29 Studenten, nach dem Krieg kamen fünf weitere hinzu. Ab 1950 dominierte in Deutschland die \"Marburger Schule\", die diese Akademiker bildeten. Gero von Merhart, wie er meist genannt wird, legte das Fach auf strenge Erfassung, Systematisierung und Katalogisierung fest und mied weitgehend die kulturgeschichtliche Deutung.\n\nThor Heyerdahl fuhr 1947 mit einem Floß von Südamerika nach Polynesien und kann als einer der Begründer der Experimentellen Archäologie betrachtet werden.\n\nSeit dem 20. Jahrhundert greift die Archäologie vermehrt auf Techniken anderer Wissenschaften zurück. Als Beispiele seien die 1949 entwickelte C-Datierung zur Datierung von organischen Stoffen und die Strontiumisotopenanalyse zur Erforschung der Wanderbewegungen der ur- und frühzeitlichen Menschen genannt. Die Archäologie hat sich zur Verbundwissenschaft entwickelt. Die Erforschung der 1991 in den Ötztaler Alpen gefundenen vorgeschichtlichen Leiche (Similaun-Mann/Ötzi) ist hierfür beispielhaft. Mit Hilfe der DNA-Analyse konnten weltweit erstmals die Verwandtschaftsbeziehungen von 40 Individuen aus einer bronzezeitlichen Begräbnisstätte in der Lichtensteinhöhle rekonstruiert werden.\n\nDie New Archaeology der 1960er Jahre brachte die Forderung, Erkenntnisse aus Lebenswissenschaften in die Archäologie einzuführen. Das Konzept der Tragfähigkeit stammt aus der Ökologie und wurde angewandt, um Fragen von Bevölkerungsdichte und Siedlungsentwicklung zu untersuchen. Optimal Foraging konnte Reaktionen auf Klimaveränderungen gleichermaßen erklären, wie jahreszeitliche Wanderungen und Landnutzungsformen. Zudem wurden mathematische Simulationen und Modellbildungen und computergestützte Geoinformationssysteme als Methoden in die Archäologie eingebracht. Die New Archaeology war besonders im angelsächsischen Kulturraum stark entwickelt und konnte sich im deutschen Sprachraum nie durchsetzen. Als Grund gilt, dass in der angelsächsischen Tradition die Archäologie traditionell zur Anthropologie gehört, nicht zu den Geschichts- oder Kulturwissenschaften.\n\nAls Antwort auf die New Archaeology entstand in den 1980er Jahren die Postprozessuale Archäologie, die Methoden aus den Kultur- und Sozialwissenschaften stärker in den Vordergrund rückte. Ein Kernbegriff ist die aus der Soziologie stammende Agency, die Handlungsmotive und -optionen betrachtet. Berücksichtigt man zusätzlich die inhärente Subjektivität jeglicher Interpretation einer Kultur, von der nur die materiellen Artefakte erhalten sind, setzen postprozessuale Archäologen auf hermeneutische einerseits und selbstreflektierende Praktiken andererseits. Heutige Nachfahren der zu untersuchenden Kulturen werden gleichermaßen in die Arbeit der Archäologen einbezogen, wie bislang vernachlässigte soziale Perspektiven.\n\nZusammen mit anderen Gedächtnisinstitutionen sind archäologische Funde und Ausgrabungsstätten das besonders sensible kulturelle Gedächtnis und oft wirtschaftliche Grundlage (z.&nbsp;B. Tourismus) eines Staates, einer Kommune oder einer Region. Gerade archäologische Funde und Ausgrabungsstätten haben auch politische Brisanz und sind in vielen bewaffneten modernen Konflikten des 21. Jahrhunderts als Teil des kulturellen Erbes eines der Primärziele und damit von Zerstörung und Plünderung bedroht. Oft soll dabei das kulturelle Erbe des Gegner nachhaltig beschädigt oder gar vernichtet werden beziehungsweise werden dabei archäologische Funde gestohlen und verbracht. Internationale und nationale Koordinationen hinsichtlich militärischer und ziviler Strukturen zum Schutz von archäologische Funde und Ausgrabungsstätten betreibt das Internationale Komitee vom Blauen Schild (Association of the National Committees of the Blue Shield, ANCBS) mit Sitz in Den Haag. Umfangreiche Missionen dazu gab es zum Beispiel 2011 in Agypten und in Libyen, 2013 in Syrien, 2014 in Mali bzw. im Irak und seit 2015 im Jemen.\n\n\nFachgebiete.\nArchäologie ist ein Sammelbegriff vieler archäologischer Disziplinen, welche meist bestimmte Zeitabschnitte oder Regionen bezeichnen. Die einzelnen Disziplinen unterscheiden sich nicht nur im behandelten Forschungsgegenstand, sondern auch in den verwendeten Methoden, z.&nbsp;B. bei der Unterwasserarchäologie. Daneben bilden archäologische Methoden einen Teilaspekt einer eigenständigen Wissenschaft, beispielsweise in der Forensik. In Fächern wie der Altamerikanistik oder auch der Klassischen Archäologie können die inhaltlichen Schwerpunkte nicht-archäologischer Natur sein.\n\n\nNach Epochen und Regionen.\nDie Disziplinen der Archäologie unterscheiden sich thematisch, zeitlich und räumlich. Dementsprechend unterschiedlich sind die Quellen derer sie sich bedienen. Während in der Prähistorischen Archäologie keine oder sehr spärlich schriftliche Quellen vorliegen und man sich vorwiegend auf die materiellen Hinterlassenschaften dieses Zeitabschnitts beruft, können andere archäologische Fachrichtungen zusätzlich Schriftquellen auswerten.\n\n\n\n\n\n\n\nDie nachfolgenden Disziplinen stellen geografische Schwerpunkte dar:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForschungsmethoden.\nArchäologische Forschungsmethoden gliedern sich in solche der Quellenerschließung und solche der Interpretation.\nIn der Öffentlichkeit wird meist nur die Erschließung der Quellen zur Kenntnis genommen.\nZur Quellenerschließung zählt auch die typologische und chronologische Auswertung.\nErst nach der Quellenerschließung und Aufbereitung folgt die historische Interpretation.\n\n\nQuellenerschließung.\nDie Ausgrabung ist zwar die bekannteste Forschungsmethode, jedoch nur ein kleiner Teilbereich der archäologischen Arbeit. Die Dokumentation, Auswertung, Konservierung und Archivierung der Funde stellt den weitaus größten Teil der archäologischen Tätigkeit dar. Außerdem muss die Grabung sorgfältig vorbereitet werden.\n\n\nProspektion und Voruntersuchungen.\nDie Prospektion umfasst zerstörungsfreie Methoden, mit deren Hilfe eine Untersuchung potenzieller oder bekannter Fundplätze ermöglicht wird. Dazu gehören die Geländebegehung (Survey), die Luftbildarchäologie und geophysikalische Methoden (Geoelektrik, elektromagnetische Induktion, geomagnetische Kartierung sowie Bodenradar und LIDAR). Ebenfalls prospektiv einsetzen lässt sich die Phosphatanalyse.\n\nEingeleitet wird eine Ausgrabung durch archäologische Voruntersuchungen. Zum Einsatz kommen hier Suchgräben, magnetische Sondierung, Bodenwiderstandsmessung, Luftbilder und andere Methoden der Bodenforschung. Die Voruntersuchungen dienen dazu, sich ein Bild der potenziellen Grabungsstelle zu machen, um die eigentliche Grabung besser planen zu können.\n\n\nAusgrabung.\nDie meisten Fundplätze werden heute durch Baumaßnahmen entdeckt. Über Notgrabungen, auch Rettungsgrabungen genannt, versucht die archäologische Denkmalpflege diese Befunde vor ihrer endgültigen Zerstörung auszuwerten.\nSeltener sind Forschungsgrabungen, bei denen unter primär wissenschaftlichen Interessen Fundplätze zur Grabung ausgewählt und ohne äußeren Zeitdruck untersucht werden können.\n\nBei der Grabung werden verschiedene Grabungstechniken angewandt. Eine moderne Grabung ist befundorientiert, d.&nbsp;h. die Funde werden in ihrer räumlichen und zeitlichen Einbettung auf Befunde bezogen.\n\nDa jede Ausgrabung zur Zerstörung eines Befundes führt, soll eine exakte Dokumentation den Fundplatz, zumindest auf dem Papier, auch später bis ins Detail rekonstruierbar machen.\nDie wichtigsten Arbeitsmittel der Ausgrabung sind deshalb, neben der Kelle, „Papier und Buntstift“.\n\n\nBauforschung.\nDie Bauforschung ist ein wesentlicher Teil sowohl der klassischen Archäologie als auch der Archäologie des Mittelalters; wohingegen sie in der Ur- und Frühgeschichte mangels aufgehend erhaltener Bauwerke nur eine untergeordnete Rolle spielt.\nEine der Dokumentationsmethoden ist die Photogrammetrie.\n\n\nAuswertung.\nGerade am sehr populären Beispiel der Gletschermumie Ötzi ist zu erkennen, dass die Ausgrabung nur einen Bruchteil der archäologischen Arbeit darstellt. Der 1991 entdeckte Fund wird bis heute wissenschaftlich untersucht.\n\n\nTypologie.\nDie Typologie ist die Klassifikation von Objekten nach Kriterien von Form und Material. Sie ist grundlegend für die Einordnung des Fundmaterials, da sie Vergleiche mit Fundsituationen an anderen Fundplätzen ermöglicht und zur Grundlage von Kombinationsanalysen (zur relativchronologischen Datierung wie zur sozioökonomischen Einordnung) und Verbreitungsanalysen wird.\n\n\nMaterialbestimmungen.\nWie bei der Prospektion und der Altersbestimmung werden auch für Materialbestimmungen moderne naturwissenschaftliche Techniken eingesetzt (siehe Archäometrie).\nZur Identifikation und Detailuntersuchung von Artefakten dienen u.&nbsp;a. die Mikroskopie, Infrarot- und Ultraschallaufnahmen, Röntgen, chemische Analysen, Spektralanalysen und Laserscans.\n\n\nAltersbestimmung.\nEin Schwerpunkt der Fundanalyse ist die Datierung der Befunde (z.&nbsp;B. Grab) anhand der Funde (z.&nbsp;B. Grabbeigabe). Bei der Altersbestimmung wird zwischen absoluter Chronologie und relativer Chronologie unterschieden.\n\nDie relative Chronologie setzt einen Fund dabei in Bezug zu einem anderen. Ist er jünger, älter oder gar gleichzeitig? J.J. Winckelmanns „vergleichendes Sehen“ ist eine der ersten Methoden zur relativen Chronologie.\n\nBei der absoluten Chronologie wird einem Fund ein absolutes Datum (Jahr, Jahrhundert) zugeordnet\n\n\nInterpretation.\nDie Methoden der Interpretation sind in der Regel eher geisteswissenschaftlich. Für die \"prähistorische\" Archäologie ist der Analogieschluss die wesentliche Möglichkeit der Interpretation. In der \"historischen\" Archäologie (z.&nbsp;B. Klassische Archäologie oder Archäologie des Mittelalters) ist es der Vergleich mit Informationen aus anderen Quellen, wie schriftlicher oder bildlicher Überlieferung.\n\n\nArchäologie in Deutschland.\nIn Deutschland gehört die Archäologie zu den Aufgaben der Bundesländer (Landesarchäologe), meist als Bereich des Denkmalamtes als Bodendenkmalpflege organisiert. Größere Städte haben oft eine eigene Stadtarchäologie. Mehrere Institutionen fördern Forscher und Projekte durch Archäologiepreise.\n\nDeutsche Grabungen im Ausland werden hingegen im Rahmen von Forschungsprojekten der Universitäten, des Deutschen Archäologischen Instituts oder des Römisch-Germanischen Zentralmuseums durchgeführt.\n\n\nArchäologie außerhalb Europas.\n\nArchäologie in Amerika.\nDie Archäologie gehört in Amerika zur Anthropologie (Völkerkunde) und hat aus diesem Grund eine völlig andere Ausrichtung als die europäische Forschung. Dies folgt vor allem aus dem Umstand, dass zum Zeitpunkt der Besiedlung der neuen Welt zuerst ethnographische Untersuchungen an noch existierenden Ureinwohnern stattfanden. Die eher spärlichen präkolumbischen Funde sind ein weiterer Grund für den in der Erforschung kultureller Prozesse liegenden Schwerpunkt amerikanischer Archäologie.\n\nAls Pionier der amerikanischen Archäologie gilt Thomas Jefferson (1743–1826), welcher ab 1784 einige Grabhügel untersucht, um ihr Alter zu bestimmen. Jefferson setzt dabei erstmals eine Methode ein, die als Vorläufer der Dendrochronologie angesehen werden kann: er zählt die Jahresringe der auf den Grabhügeln stehenden Bäume.\n\nDie ersten großen Ausgrabungen in Mittelamerika werden Ende des 19. Jahrhunderts im Mayazentrum Copán durchgeführt. 1911 entdeckt Hiram Bingham die Inkastadt Machu Picchu.\n\nIm Jahre 1990 fanden Archäologen in der Nähe von Mexiko-Stadt über 10.000 Artefakte aus der Zeit der spanischen Eroberung des Landes. Man fand nicht nur menschliche Knochen, sondern auch Waffen, Kleidung, Haushaltsgeräte und Gegenstände aus dem persönlichen Besitz von Hernán Cortés. Die Fundstelle Tecoaque (vorspanischer Name: Zultepec) wurde als UNESCO-Welterbe vorgeschlagen.\n\n\nArchäologie in Indien und China.\n1863 wird in Indien die Archaeological Survey of India gegründet. 1921/1922 entdeckt man eine der ältesten Hochkulturen der Menschheit, die Indus-Kultur. Ausgegraben werden u.&nbsp;a. die Städte Harappa und Mohenjo-Daro.\n\nArchäologie in China beginnt mit dem schwedischen Geologen J. Gunnar Andersson (1874–1960), der 1921 bei Yang Shao Tsun in Honan eine neolithische Wohnhöhle entdeckt und damit beweist, dass China in vorgeschichtlicher Zeit bewohnt war. 1928 wird Anyang ausgegraben, die Hauptstadt der Shang-Dynastie des 2. Jahrtausends v. Chr.\n\n1974 wird die Terrakottaarmee rund um das Grab des chinesischen Kaisers Qin Shihuangdi bei Xi’an entdeckt.\n\nArchäologie in Afrika.\nAfrika ist nicht nur in paläoanthropologischer Hinsicht die Wiege der Menschheit, sondern auch die unserer Kultur. Nur in Afrika kommen Steingeräte vor, die 2,5 Millionen Jahre alt sind und deren Herstellung mit den ersten Homo-Arten unserer Spezies in Verbindung gebracht wird. Die betreffenden Werkzeuge – einfache Geröllgeräte vom Oldowan-Typ, später die Faustkeile, um die Leitformen zu nennen – kommen auch in anderen Teilen der Welt vor, nur sind sie dort deutlich jünger. In Europa datieren die ältesten Stellen auf eine Million Jahre. Neue, etwa 3,3&nbsp;Millionen Jahre alte Funde in Lomekwi 3, Kenia, werden als Beleg für eine eigenständige archäologische Kultur interpretiert, vorschlagsweise \"Lomekwian\" genannt.\n\nBereits seit dem 17. Jahrhundert ist der Nordosten Afrikas Gegenstand intensiver Forschungen durch die Ägyptologie und Koptologie. Diese Region des Kontinents ist auch im internationalen Vergleich hervorragend dokumentiert. Da jedoch die ältesten Schriftquellen im subsaharischen Afrika nicht weiter als 600 Jahre zurückreichen, kommt der Archäologie gerade hier eine besondere Bedeutung zu. Aufgrund der kurzen Forschungstradition im Vergleich zu Mitteleuropa steht man hier allerdings noch vielfach am Anfang.\n\n\nAufbereitung für die Öffentlichkeit und Schutz.\nDie Vermittlung archäologischer Forschungsergebnisse erfolgt auf verschiedene Weise:\n\nZunehmend wird international auch der Schutz der archäologischen Funde für die Öffentlichkeit im Hinblick auf Katastrophen, Kriege und bewaffnete Auseinandersetzungen durchgesetzt. Das geschieht einerseits durch internationale Abkommen und andererseits durch Organisationen die den Schutz überwachen beziehungsweise durchsetzen. Als weltweites Beispiel gilt Blue Shield International mit seinen Archäologen und lokalen Partnerorganisationen. Die Wichtigkeit der archäologischen Funde im Bezug auf Identität, Tourismus und nachhaltiges Wirtschaftswachstum werden immer wieder betont. So wurde auch vom Präsident von Blue Shield International, Karl von Habsburg, bei einem Kulturgutschutz-Einsatz im April 2019 im Libanon mit der United Nations Interim Force in Lebanon erläuterte: „Kulturgüter sind ein Teil der Identität der Menschen, die an einem bestimmten Ort leben. Zerstört man ihre Kultur, so zerstört man damit auch ihre Identität. Viele Menschen werden entwurzelt, haben oft keine Perspektiven mehr und flüchten in der Folge aus ihrer Heimat.“\n\n\n\n\n\n\n\n\n\n\n\n\nWeblinks.\nVereine und Organisationen\n\n"}
{"id": "124", "url": "https://de.wikipedia.org/wiki?curid=124", "title": "American Standard Code for Information Interchange", "text": "American Standard Code for Information Interchange\n\nDer (ASCII, alternativ US-ASCII, oft [] ausgesprochen, ) ist eine 7-Bit-Zeichenkodierung; sie entspricht der US-Variante von ISO 646 und dient als Grundlage für spätere, auf mehr Bits basierende Kodierungen für Zeichensätze.\n\nDer ASCII-Code wurde zuerst am 17.&nbsp;Juni 1963 von der American Standards Association (ASA) als Standard ASA X3.4-1963 gebilligt und 1967/1968 wesentlich sowie zuletzt im Jahr 1986 (ANSI X3.4-1986) von ihren Nachfolgeinstitutionen aktualisiert und wird bis heute noch benutzt. Die Zeichenkodierung definiert 128 Zeichen, bestehend aus 33 nicht druckbaren sowie den folgenden 95 druckbaren Zeichen, beginnend mit dem Leerzeichen:\n\nDie druckbaren Zeichen umfassen das lateinische Alphabet in Groß- und Kleinschreibung, die zehn arabischen Ziffern sowie einige Interpunktionszeichen (Satzzeichen, Wortzeichen) und andere Sonderzeichen. Der Zeichenvorrat entspricht weitgehend dem einer Tastatur oder Schreibmaschine für die englische Sprache. In Computern und anderen elektronischen Geräten, die Text darstellen, wird dieser in der Regel gemäß ASCII oder abwärtskompatibel (ISO 8859, Unicode) dazu gespeichert.\n\nDie nicht druckbaren Steuerzeichen enthalten Ausgabezeichen wie Zeilenvorschub oder Tabulatorzeichen, Protokollzeichen wie Übertragungsende oder Bestätigung und Trennzeichen wie Datensatztrennzeichen.\n\n\nKodierung.\nJedem Zeichen wird ein Bitmuster aus 7&nbsp;Bit zugeordnet. Da jedes Bit zwei Werte annehmen kann, gibt es 2 = 128 verschiedene Bitmuster, die auch als die ganzen Zahlen 0–127 (hexadezimal 00h–7Fh) interpretiert werden können.\n\nDas für ASCII nicht benutzte achte Bit kann für Fehlerkorrekturzwecke (Paritätsbit) auf den Kommunikationsleitungen oder für andere Steuerungsaufgaben verwendet werden. Heute wird es aber fast immer zur Erweiterung von ASCII auf einen 8-Bit-Code verwendet. Diese Erweiterungen sind mit dem ursprünglichen ASCII weitgehend kompatibel, so dass alle im ASCII definierten Zeichen auch in den verschiedenen Erweiterungen durch die gleichen Bitmuster kodiert werden. Die einfachsten Erweiterungen sind Kodierungen mit sprachspezifischen Zeichen, die nicht im lateinischen Grundalphabet enthalten sind, vgl. unten.\n\n\nZusammensetzung.\nDie ersten 32 ASCII-Zeichencodes (von 00 bis 1F) sind für Steuerzeichen \"(control character)\" reserviert; siehe dort für die Erklärung der Abkürzungen in obiger Tabelle. Diese Zeichen stellen keine Schriftzeichen dar, sondern dienen (oder dienten) zur Steuerung von solchen Geräten, die den ASCII verwenden (etwa Drucker). Steuerzeichen sind beispielsweise der Wagenrücklauf für den Zeilenumbruch oder \"Bell\" (die Glocke); ihre Definition ist historisch begründet.\n\nCode 20 \"(SP)\" ist das Leerzeichen (engl. \"space\" oder \"blank\"), das in einem Text als Leer- und Trennzeichen zwischen Wörtern verwendet und auf der Tastatur durch die Leertaste erzeugt wird.\n\nDie Codes 21 bis 7E stehen für druckbare Zeichen, die Buchstaben, Ziffern und Interpunktionszeichen (Satzzeichen, Wortzeichen) umfassen. Die Buchstaben sind lediglich Klein- und Großbuchstaben des lateinischen Alphabets. In nicht-englischen Sprachen verwendete Buchstabenvarianten – beispielsweise die deutschen Umlaute – sind im ASCII-Zeichensatz nicht enthalten. Ebenso fehlen typografisch korrekte Gedankenstriche und Anführungszeichen, die Typografie beschränkt sich auf den Schreibmaschinensatz. Der Zweck war \"Informationsaustausch\", nicht Drucksatz.\n\nCode 7F (alle sieben Bits auf eins gesetzt) ist ein Sonderzeichen, das auch als \"Löschzeichen\" bezeichnet wird \"(DEL)\". Dieser Code wurde früher wie ein Steuerzeichen verwendet, um auf Lochstreifen oder Lochkarten ein bereits gelochtes Zeichen nachträglich durch das Setzen aller Bits, das heißt durch Auslochen aller sieben Markierungen, löschen zu können. Dies war die einzige Möglichkeit zum Löschen, da einmal vorhandene Löcher nicht mehr rückgängig gemacht werden können.\nBereiche ohne Löcher (also mit dem Code 00) fanden sich vor allem am Anfang und Ende eines Lochstreifens \"(NUL)\".\n\nAus diesem Grund gehörten zum eigentlichen ASCII nur 126 Zeichen, denn den Bitmustern 0 (0000000) und 127 (1111111) entsprachen keine Zeichencodes. Der Code 0 wurde später in der Programmiersprache C als „Ende der Zeichenkette“ interpretiert; dem Zeichen 127 wurden verschiedene grafische Symbole zugeordnet.\n\n\nGeschichte.\n\nFernschreiber.\nEine frühe Form der Zeichenkodierung war der Morsecode. Er wurde mit der Einführung von Fernschreibern aus den Telegrafennetzen verdrängt und durch den Baudot-Code und Murray-Code ersetzt. Vom 5-Bit-Murray-Code zum 7-Bit-ASCII war es dann nur noch ein kleiner Schritt – auch ASCII wurde zuerst für bestimmte amerikanische Fernschreibermodelle, wie den Teletype ASR33, eingesetzt.\nDie erste Version, noch ohne Kleinbuchstaben und mit kleinen Abweichungen vom heutigen ASCII bei den Steuer- und Sonderzeichen, entstand im Jahr 1963.\n\nIm Jahr 1965 folgt die zweite Form des ASCII-Standards. Obwohl die Norm genehmigt wurde, wurde sie nie veröffentlicht und fand daher auch nie Anwendung. Der Grund dafür war, dass der ASA gemeldet wurde, dass die ISO (die International Standards Organization) einen Zeichensatz standardisieren würde, der ähnlich wie dieser Norm war, aber leicht im Widerspruch zu dieser stünde.\n\n1968 wurde dann die bis heute gültige Fassung des ASCII-Standards festgelegt.\nDiese Fassung gebar die Caesar-Verschlüsselung ROT47 als Erweiterung von ROT13. Während ROT13 nur das lateinische Alphabet um dessen halbe Länge rotiert, rotiert ROT47 alle ASCII-Zeichen zwischen 33 (codice_1) und 126 (codice_2).\n\n\nComputer.\nIn den Anfängen des Computerzeitalters entwickelte sich ASCII zum Standard-Code für Schriftzeichen. Zum Beispiel wurden viele Terminals (VT100) und Drucker nur mit ASCII angesteuert.\n\nFür die Kodierung lateinischer Zeichen wird fast nur bei Großrechnern die zu ASCII inkompatible 8-Bit-Kodierung EBCDIC verwendet, die IBM parallel zu ASCII für sein System/360 entwickelte, damals ein ernsthafter Konkurrent. Die Handhabung des Alphabets ist in EBCDIC schwieriger, denn es ist dort auf zwei auseinander liegende Codebereiche verteilt. IBM selbst verwendete ASCII für interne Dokumente. ASCII wurde durch Präsident Lyndon B. Johnsons Anordnung 1968 gestützt, es in den Regierungsbüros zu verwenden.\n\n\nVerwendung für andere Sprachen.\nMit dem Internationalen Alphabet 5 (IA5) wurde 1963 eine 7-Bit-Codierung auf Basis des ASCII als ISO 646 normiert. Die Referenzversion (ISO 646-IRV) entspricht dabei bis auf eine Position dem ASCII. Um Buchstaben und Sonderzeichen verschiedener Sprachen darstellen zu können (beispielsweise die deutschen Umlaute), wurden 12 Zeichenpositionen zur Umdefinition vorgesehen (<code>#$@[\\]^`\n\n\nEponyme.\nDer 1936 entdeckte Asteroid (3568) ASCII wurde 1988 nach der Zeichenkodierung benannt.\n\n\n\n\n"}
{"id": "130", "url": "https://de.wikipedia.org/wiki?curid=130", "title": "Außenbandruptur des oberen Sprunggelenkes", "text": "Außenbandruptur des oberen Sprunggelenkes\n\nDas Außenband des oberen Sprunggelenkes setzt sich zusammen aus drei Bändern (der „laterale Bandapparat“): \"Ligamentum fibulotalare anterius\" und \"posterius\" sowie \"Ligamentum fibulocalcaneare\". Beim Umknicken nach außen (Supinationstrauma) kommt es meist zur Zerrung oder zu einem Riss (Ruptur) des \"Lig. fibulotalare anterius\" oder/und des \"Lig. calcaneofibulare\", seltener ist die komplette Ruptur aller drei Bänder.\n\nEine Ruptur von mindestens einem dieser drei Bänder nennt man auch eine fibulare Bandruptur.\n\nDas \"Ligamentum fibulotalare anterius\" reißt am ehesten dann, wenn der Fuß zugleich gestreckt (Plantarflexion) und verdreht (Inversion) ist. Ist der Fuß bei einer Verdrehung angewinkelt (Dorsalextension), bleibt dieses Band zumeist intakt.\n\nBei Bänderrissen können die enormen Kräfte, die zugleich auf umliegende Weichteile und Knochen einwirken, zu Malleolarfrakturen oder zu knöchernen Bandausrissen (Abrissfrakturen) führen.\n\n\nErste Hilfe.\nAls Erste-Hilfe-Maßnahme bei einem vermuteten Bänderriss wird das Gelenk ruhiggestellt, gekühlt, vorsichtig bandagiert und hochgelagert (PECH-Regel: „Pause, Eis, Compression, Hochlagerung“).\n\nDie PECH-Regel ist generell als geeignete Behandlungsmaßnahme akzeptiert, wenn auch die empirischen Belege für ihre Wirksamkeit unzureichend sind.\n\nVermeintlich „leichte“ Verletzungen werden von dem Betroffenen ebenso wie vom behandelnden Arzt oft unterschätzt. Wenn sie nicht angemessen diagnostiziert und behandelt werden, kann es zu wiederholten Verletzungen oder auch zu chronischer Gelenkinstabilität kommen, woraus sich weitere Schäden ergeben können.\n\nLaut der Leitlinien der DGU, ÖGU und DGOOC wird geschätzt, dass nur etwa die Hälfte der Patienten mit akuter Außenbandverletzung ärztliche Hilfe in Anspruch nimmt und somit adäquat behandelt wird.\n\n\nDiagnostik.\nBei der Diagnostik wird der Patient im Rahmen der Anamnese nach dem Hergang der Verletzung (Unfallmechanismus, Sturz, Höhe, mechanische Kraft und Bewegungsrichtung des Fußes), nach der bisherigen Behandlung und nach früheren Verletzungen befragt, das Fußgelenk nach Schwellungen und Hämatomen untersucht, das Gangbild beurteilt und eine Untersuchung durch Palpation sowie spezifische Funktions- und Schmerztests durchgeführt. Hinzu kommen Röntgenaufnahmen und gegebenenfalls der Einsatz weiterer bildgebender Verfahren.\n\n\nSchubladentest.\nBei einer Verletzung der Außenbänder des oberen Sprunggelenks ist wegen der Wahl der richtigen Therapie vor allem die Frage wichtig, ob es sich um eine Bänderdehnung oder einen Bänderriss handelt. Geübten Untersuchern gelingt die Unterscheidung zwischen Bänderdehnung und Bänderriss in der Regel alleine mit dem Schubladentest, also ohne die Anfertigung von Röntgen-Bildern oder den Einsatz anderer gerätemedizinischer Untersuchungsmethoden: Für den vorderen Schubladentest liegt der Patient in Rückenlage. Der Untersucher umgreift mit einer Hand von unten die Ferse, mit der anderen Hand wird gefühlvoll von oben gegen das Schienbein gedrückt. Liegt lediglich eine Zerrung des vorderen Außenbandes vor, ist keine Schubladenbewegung möglich. Dagegen kann der Fuß bei einem Riss deutlich gegenüber dem Schien- und Wadenbein nach vorne (bei liegendem Patienten nach oben) aus dem Gelenk geschoben werden („Talus&shy;vorschub“). Da sich die normale Schubladenbewegung im oberen Sprunggelenk im gesunden Zustand von Mensch zu Mensch stark unterscheidet, ist es wichtig, die Untersuchung zuvor am gesunden Sprunggelenk des anderen Beins durchzuführen. Auf diese Weise lässt sich herausfinden, welches Ausmaß der Schubladenbewegung beim betroffenen Menschen noch als nicht gerissen anzusehen ist.\n\nDie Stabilität der fibularen Seitenbänder wird auch durch die vertikalen Aufklappbarkeit getestet. Hierbei wird der Rückfuß gegen den fixierten Unterschenkel maximal nach innen gedrückt.\n\nManche empfehlen, den Schubladentest beim Verdacht auf einen Außenbandriss nur innerhalb der ersten 48 Stunden nach der Verletzung durchzuführen, um nicht zu riskieren, dass sich im Falle eines Bänderrisses die frühen Verklebungen der Bänder wieder lösen. Sind mehr als 48 Stunden vergangen, solle man stattdessen von einem Riss ausgehen.\n\n\nBildgebende Verfahren.\nZusätzlich können bildgebende Verfahren wie Röntgen sinnvoll sein, um einen Bruch (Fraktur) der angrenzenden Knochen auszuschließen. In seltenen Fällen kann zudem eine Magnetresonanztomographie (MRT) sinnvoll sein; diese macht zugleich etwaige weitere Verletzungen (etwa Kapselrisse, Gelenkergüsse oder Knochenprellungen) sichtbar. Wie die Deutsche Gesellschaft für Ultraschall in der Medizin hervorhebt, kann ergänzend oder alternativ zur MRT die Sonografie eingesetzt werden, die es dem entsprechend qualifizierten Arzt erlaubt, Instabilitäten und Bänderrisse durch eine dynamische Untersuchung aufzuzeigen.\n\nSehr in die Kritik geraten sind bei der Diagnosestellung die bis vor einiger Zeit üblichen sogenannten „gehaltenen Röntgen-Aufnahmen“. Dabei wird auf einem Röntgenbild festgehalten, wie weit sich das Gelenk mit einer fest definierten Kraft aufklappen lässt. Aus dem Aufklappwinkel, der im Röntgenbild eingezeichnet werden kann, wurde dann auf den Verletzungsgrad geschlossen. Der Grund für die Kritik ist, dass sich mit solchen gehaltenen Aufnahmen vor allem das mittlere Außenband überprüfen lässt, das allerdings nur sehr selten isoliert reißt, sondern fast immer nur in Kombination mit dem vorderen Außenband. Da für die Auswahl der Therapie vor allem die Frage wichtig ist, ob es sich um einen Riss oder eine Zerrung handelt, nicht aber ob ein oder zwei Bänder gerissen sind, reicht der Schubladentest, der das vordere Außenband überprüft, in den meisten Fällen als alleinige Untersuchung aus.\n\n\nBegleiterscheinungen.\nDurch die enormen Kräfte, die beim Bänderriss auf umliegende Weichteile und Knochen einwirken, kann es zugleich zu Malleolarfrakturen oder zu knöchernen Bandausrissen (Abrissfrakturen) kommen, was zu einem freien Gelenkkörper und mitunter zu einem knöchernen Impingement des Gelenks führen kann. Außerdem können Außenbandrupturen ein Weichteil-Impingement am oberen Sprunggelenk verursachen, mit einer Einklemmung weichteiliger Strukturen (entzündliche Gelenkinnenhaut, Kapselbandgewebe, Narbengewebe) im Gelenk. Diese befinden sich zumeist in den vorderen und seitlichen Abschnitten des oberen Sprunggelenkes. Stören solche weichteiligen oder knöchernen Einklemmungen, werden sie eventuell arthroskopisch oder minimalinvasiv entfernt.\n\nZudem ist ein Inversionstrauma die häufigste Ursache für das Sinus-tarsi-Syndrom.\n\n\nBehandlung.\nWährend noch vor einigen Jahren die Außenbandruptur regelhaft genäht wurde, ist heute bei gleich guten Behandlungsergebnissen die konservative Behandlung Standard. Denn da das Außenband in eine Gelenkkapsel eingebettet ist, kann es nach einer Ruptur auch ohne Operation zusammenwachsen. Nur bei kompletter Zerreißung aller drei Bänder und Operationswunsch (z.&nbsp;B. Profisportler) wird noch eine operative Behandlung empfohlen. Sowohl bei konservativer als auch bei operativer Therapie einer fibularen Bänderruptur fällt typischerweise eine mehrwöchige Arbeitsunfähigkeit an.\n\n\nKonservative Behandlung.\nIn der Therapie des akuten Gelenktraumas können nicht-steroidale Entzündungshemmer (NSAR) eingesetzt und eine frühe Mobilisation des Gelenkes durchgeführt werden. Wie ein systematischer Review von 2008 zeigte, hat der Einsatz von NSAR, Beinwell-Salbe und manueller Therapie in einer frühen Phase nach der Verletzung zumindest kurzzeitig signifikant positive Auswirkungen auf die Funktionalität des Gelenkes. In Kombination mit anderen Therapien wie Krankengymnastik hat die manuelle Therapie auch längerfristig positive Auswirkungen auf die Heilung. Andererseits ist bekannt, dass NSAR auch Enzyme blockieren, die zur Heilung notwendig sind, und dass Patienten, deren Schmerzen unterdrückt werden, ihr Gelenk möglicherweise zu früh belasten. Krankengymnastik und manuelle Therapie sollen Schmerzen und Schwellungen verringern und die Funktion des Gelenkes wiederherstellen. Um einer chronischen Instabilität vorzubeugen, wird das Gelenk in der Akutphase mit Hilfe einer Schiene ruhiggestellt und später, in der Rehabilitationsphase, physiotherapeutisch beübt (Mobilisation des Gelenks, Balanceübungen). Ergänzend kann die Kryotherapie eingesetzt werden.\n\n\nEntzündliche, proliferative und remodellierende Phasen.\nBei der Behandlung in konservativer Therapie unterscheidet man mehrere Phasen der Heilung, deren Dauer von Fall zu Fall verschieden sein kann und die sich außerdem überlappen können: eine erste, mehrere Tage währende entzündliche Phase (Phase I), eine etwa 5–28-tägige reparative oder proliferative Phase der Primärheilung (Phase II), die durch Angiogenese, proliferierende Fibroblasten und Kollagenproduktion gekennzeichnet ist, und eine vier- bis sechswöchige Umbau- bzw. remodellierende Phase (Phase III) in der die Kollagenfibrillen und Zellen der Bänder reifen. In Phasen I und II ist ein Schutz vor zu viel Belastung wichtig, um eine überschießende Produktion des Kollagens Typ III, und somit der Bildung eines elongierten weichen Bandes vorzubeugen; in der Phase III hingegen ist eine allmähliche Zunahme der Belastung nötig, um die Bänder „auszuhärten“. Physiologische Belastungsreize während der Heilung eines Bänderrisses führen zu einer besseren Organisation des heilenden Gewebes und geringerer Narbenbildung.\n\nBei der konservativen Therapie ist das Anlegen einer Orthese über mindestens fünf Wochen Standard. Ist die Schwellung zu stark, um eine Orthese anlegen zu können, wird der Fuß zunächst kurzzeitig (zum Beispiel zwei bis vier Tage) ruhiggestellt bis die Schwellung etwas abgeklungen ist, beispielsweise mittels gespaltenem Spaltgips unter Entlastung an Unterarmgehstützen mit medikamentöser Thromboembolieprophylaxe. Während der ersten circa sechs Wochen finden Umbauvorgänge statt, die sich den ersten mechanischen Belastungen anpassen.\n\nEs ist auch erprobt worden, Bänderverletzungen ergänzend innerhalb der ersten 48 Stunden nach der Verletzung mit Hyaluronsäure-Spritzen zu behandeln; allerdings ist die empirische Datenlage zu dieser Methode noch gering (Stand: 2011).\n\nLiegt am Gelenk keine Schwellung (mehr) vor, wird normalerweise eine Orthese (z. B. Aircast-Schiene) eingesetzt. Sie stellt sicher, dass die gerissenen Bänder nicht belastet werden können und dass das Gelenk dennoch bewegt werden kann. Eine geeignete Orthese stabilisiert sowohl die Auswärtsdrehung (Supination) des Rückfußes als auch den Talusvorschub. Manche Orthesen verhindern nicht nur die Supination, sondern auch die Plantarflexion, was für die Anfangsphase als wichtig für die Heilung angesehen wird. Bei anderen wird die Beweglichkeit des Gelenks in horizontaler Richtung („rauf/runter“ = Flexion/Extension) kaum eingeschränkt, sodass zum Beispiel Spazieren oder Radfahren möglich sind. Sportmediziner raten dazu, spätestens 1–2 Tage nach dem Trauma (oder nach einer Operation) eine konsequente Schienung einzusetzen. Die Orthese wird zum Zweck der Stabilisierung Tag und Nacht getragen, da nachts der Muskeltonus nachlässt und der Fuß daher in eine ungünstige Haltung sacken könnte. Durch die nachlassende Spannung in der Nacht entstünde vor allem ein Zug auf das \"Ligamentum fibulotalare anterius\" und das \"Ligamentum fibulocalcaneare\". Die Bänder wachsen durch das Tragen der Orthese eher belastungsgerecht zusammen, Probleme mit einem versteiften Gelenk, die bei kompletter Fixierung zu erwarten wären, werden vermieden. Als besonders günstig haben sich hierbei sogenannte modulare Orthesen erwiesen, die eine Anpassung der Bewegungsfähigkeit mit Orthese an den Heilungsverlauf ermöglichen.\n\nAb der remodellierenden Phase kann der weitere Umbau von Bändern Monate bis Jahre dauern.\n\n\nPhysikalische Therapie.\nBei der Außenbandruptur ist die physikalische Therapie ein Teil der konservativen Therapie. Wie bei anderen Bänderrissen auch geht es dabei u.&nbsp;a. um die Resorption eventueller Ödeme, die Verbesserung der Durchblutung, die Lösung von Verklebungen und den Erhalt der Beweglichkeit unter Beachtung ärztlicher Vorgaben. Hinzu kommen angepasste Übungen der Muskulatur und ggf. die Anleitung zur Verwendung von Gehstützen oder anderer Hilfsmittel.\n\nDie Leitlinien der DGU, ÖGU und DGOOC sehen für die Behandlung der Außenbandruptur nach einer initialen Phase mit Hochlagerung, Kryotherapie und elastischer Wickelung isometrische Übungen in der Orthese vor. Im Verlauf der Behandlung wird häufig die manuelle Therapie eingesetzt, um die Beweglichkeit des Gelenkes zu verbessern. Die Leitlinien sprechen von limitierten, kurzzeitigen positiven Effekten der manuellen Therapie; andere Studien zeigen auf, dass eine Lymphdrainage und manuelle Therapie zu einer verringerten Schwellung und größeren Beweglichkeit des Gelenkes führen kann, die die Propriozeption verbessern und das Risiko einer Versteifung des Gelenkes verringern. Dehnungsübungen der Achillessehne werden eingesetzt, da diese sich andernfalls infolge der Verletzung verkürzen kann. Nach Abnahme der Orthese sind laut der Leitlinien Koordinationsschulung, Muskelkräftigung (Peroneusgruppe) und Eigenreflexschulung vorgesehen. Mit sensomotorischem Balancetraining, das der Patient unter Anleitung oder selbständig durchführen kann, sollen erneute Verletzungen und dadurch wiederkehrende Bänderinstabilitäten vermieden werden. Allerdings liegen zum propriozeptiven Training widersprüchliche Ergebnisse vor. Die Wirksamkeit zusätzlicher Ultraschall-, Laser- und Kurzwellentherapie ist für die Behandlung der Außenbandruptur nicht nachgewiesen.\n\nEin Review von 2014 kam zum Schluss, dass die manuelle Therapie und Gelenkmobilisierung sowohl bei akuten als auch bei subakuten oder chronischen Symptomen den Schmerz verringert und die Beweglichkeit verbessert.\n\n\nOperative Therapie.\nDie operative Therapie der Außenbandruptur, so die Leitlinien der DGU, ÖGU und DGOOC, „liefert bei gleicher frühfunktioneller Nachbehandlung eine der nichtoperativen Behandlung vergleichbare bis höhere Kapselbandstabilität bei einer nichtsignifikanten Tendenz zu höherer Steifigkeit und längerer Arbeitsunfähigkeit […] sowie einem leicht erhöhten Risiko für die Entwicklung einer posttraumatischen Arthrose“.\n\nBis in die späten 1980er wurde fast jeder akute Bänderriss am oberen Sprunggelenk operiert. Heute werden Bandrupturen am oberen Sprunggelenk meistens zunächst konservativ behandelt und eine Operation wird erst erwogen, wenn die konservative Therapie nach sechs oder mehr Monate nicht wirksam ist. Eine verbleibende chronische Instabilität kann mit Hilfe einer Außenbandplastik behoben werden. Bei Personen, bei denen im Alltag eine hohe Last auf das verletzte Gelenk einwirkt, vor allem bei Spitzensportlern, werden Bänderrisse auch direkt operiert. Auch im Falle eines knöchernen Bandausrisses kann eine Operation indiziert sein.\n\nWird direkt operiert, können innerhalb der ersten 14 Tage nach der Verletzung die Teile des gerissenen Bandes aneinandergelegt und operativ vernäht werden. Später ist dies nicht mehr möglich, da diese schon teilweise abgebaut worden sind. Eine allzu frühe Operation kann allerdings eine arthrofibrotische Reaktion in der Gelenkkapsel auslösen.\n\nNach einer operativen Bandrekonstruktion ist eine Prophylaxe der Schwellung, u.&nbsp;a. durch intermittierende Kühlung, durch den Einsatz von NSAR in den ersten Tagen nach der Operation und durch konsequente Hochlagerung des Fußes entscheidend. Im Folgenden können Warm-Kalt-Wechselduschen des Fußes und manuelle Therapie eingesetzt werden. Es kommt eine funktionelle Stufentherapie zum Einsatz, mit einer anfänglichen sechswöchigen Schienung, welche die Flexo-Extension des oberen Sprunggelenks begrenzt, sowie mit vorsichtigen frühfunktionellen, sensomotorischen Übungen ab dem Ende der zweiten Woche.\n\n\nEingeschränkte Fahrtauglichkeit während der Heilung.\nIst der Fuß noch nicht dauerhaft belastbar oder ist eine Orthese nötig, kann die Fahrtauglichkeit eingeschränkt sein (Fahruntüchtigkeit).\n\n\nKomplikationen.\nBei adäquater Behandlung, sei es konservativ oder durch Operation, heilt die Außenbandruptur in den meisten Fällen vollständig aus. Bei einem kleineren Teil der Behandelten bleiben jedoch chronische Symptome zurück. Laut einem Review von 1997 bleiben bei 10–30 % der Behandelten eine chronische Synovitis oder Tendinitis, Gelenksteife, Schwellung (bzw. Schwellneigung), Schmerz, Muskelschwäche oder Gelenkinstabilität zurück. Laut einem Review von 2018 hatten nach 1–4 Jahren 5 %–46 % der Behandelten noch Schmerzen, 3 %–34 % rezidivierende Umknickverletzungen und 33 %–55 % eine Gelenkinstabilität; 25 % berichteten über ein vorderes Impingement.\n\nWird ein Bänderriss in der Frühphase der Heilung nicht angemessen behandelt, etwa indem neu gebildetes Wundgewebe durch Retraumatisierung zerstört wird, verlängert sich die Entzündungsphase. Auch andere Faktoren wie ein hohes Alter, Durchblutungsstörungen oder Diabetes können die Heilung verzögern.\n\nWerden Verletzungen nicht ausreichend ausgeheilt, kann es zu wiederholten Verletzungen oder auch zu chronischer Gelenkinstabilität kommen, woraus sich weitere Schäden ergeben können. Des Weiteren deuten Studien darauf hin, dass Gelenkinstabilität mit einer verringerten Lebensqualität und einer verringerten körperlichen Aktivität der Betroffenen einhergehen kann.\n\nWachsen die Bänder unzureichend zusammen, kann es zur Abnutzung des Knorpels und somit zu einer Arthrose kommen. (Siehe auch: Posttraumatische Arthrose.)\n\nDurch Umknickverletzungen und Bänderrisse kann es durch posttraumatische Synovitis und durch Einklemmung von Narbengewebe zu einem Weichteil-Impingement am oberen Sprunggelenk und dadurch zu einer mit Schmerzen verbundenen Beschränkung des Bewegungsmaßes dieses Gelenks (Sprunggelenk-Impingement) kommen. Dies kann eine Indikation für einen operativen Eingriff darstellen, etwa durch eine Abtragung des einklemmenden Gewebes mittels Arthroskopie.\n\n"}
{"id": "132", "url": "https://de.wikipedia.org/wiki?curid=132", "title": "Alphabet", "text": "Alphabet\n\nEin Alphabet (frühneuhochdeutsch von kirchenlateinisch \"\", von \"alphábētos\") ist die Gesamtheit der kleinsten Schriftzeichen bzw. Buchstaben einer Sprache oder mehrerer Sprachen in einer festgelegten Reihenfolge. Die Buchstaben können über orthographische Regeln zu Wörtern verknüpft werden und damit die Sprache schriftlich darstellen. Die im Alphabet festgelegte Reihenfolge der Buchstaben erlaubt die alphabetische Sortierung von Wörtern und Namen beispielsweise in Wörterbüchern. Nach einigen Definitionen ist mit \"Alphabet\" nicht der Buchstabenbestand in seiner festgelegten Reihenfolge gemeint, sondern die Reihenfolge selbst.\n\nDie Bezeichnung \"Alphabet\" geht auf die ersten beiden Buchstaben des griechischen Alphabets zurück (Alpha – α, Beta – β). Ausgehend von den ersten drei Buchstaben des deutschen Alphabets (bzw. des lateinischen Alphabets) sagt man auch Abc (die Schreibweise Abece verdeutlicht die Aussprache, wird aber selten verwendet).\n\nAlphabetschriften gehören wie Silbenschriften zu den phonographischen Schriften und stehen damit im Gegensatz zu piktografischen oder logografischen Systemen, bei denen die Zeichen für Begriffe stehen (z.&nbsp;B. \"Rind, Sonnenaufgang, Freundschaft\"). Im Unterschied zu Silbenschriften bezeichnen alphabetische Buchstaben in der Regel jeweils nur \"einen\" Laut (Phonem). Damit wird die fürs Sprechenlernen schon erbrachte und unerlässliche Abstraktionsleistung hochgradig ins Schreiben hinübergerettet und das Erlernen völlig neuer Symbole für die Objekte des Alltags eingespart. Eine Zwischenform aus Alphabetschrift und Silbenschrift stellen die sogenannten Abugidas dar, zu denen die indischen Schriften gehören.\n\nDas Alphabet dient auch dem Erlernen des Lesens und des Schreibens; eine Merkhilfe dazu waren die Buchstabentafeln. Jemand, der lesen kann, wird fachsprachlich ebenfalls als \"Alphabet\" bezeichnet, das Gegenteil ist der Analphabet. Ein wichtiges Ziel von Kulturpolitik ist die Alphabetisierung der jeweiligen Bevölkerung – also die Beherrschung des Lesens und des Schreibens durch alle.\n\n\nDeutsches Alphabet.\nDas deutsche Alphabet ist eine Variante des lateinischen Alphabets. Von diesem stammen 26 Buchstaben:\n\nIm deutschen Alphabet kommen dazu noch die drei Umlaute (Ä/ä, Ö/ö, Ü/ü) sowie das Eszett (ẞ/ß).\n\n\nFunktionsweise.\nDie Buchstaben eines Alphabetes sind schriftliche Symbole für die kleinsten \"bedeutungsunterscheidenden\" lautlichen Einheiten der Sprache, die Phoneme; zum Beispiel unterscheiden und in und die Bedeutung der Wörter (siehe auch Minimalpaar und Allophon).\n\nIn einem \"idealen Alphabet\" entspricht jeder Buchstabe einem Phonem und umgekehrt. In der Praxis finden sich aber immer Abweichungen:\n\n\nDarüber hinaus geht die einmal festgelegte Korrespondenz von Phonem und Graphem auch durch den Sprachwandel verloren (vergleiche englisch und gegenüber lateinisch ).\n\nFehlen in einem Schriftsystem Zeichen für Phoneme, können sprachliche (inhaltliche) Unterschiede eventuell nicht schriftlich wiedergegeben werden. So bestanden einige Alphabete ursprünglich nur aus Konsonanten (Konsonantenschrift). Später wurden sie mit Zeichen für Vokale ergänzt, die als kleine Zusätze (z.&nbsp;B. Punkte, Striche) zu den Konsonanten gesetzt werden konnten (z.&nbsp;B. arabisches und hebräisches Alphabet).\n\nSind hingegen in einem Schriftsystem Zeichen für Phoneme im Übermaß vorhanden, können semantische (inhaltliche) Unterschiede selbst bei gleicher Lautung schriftlich ausgedrückt werden. Zum Beispiel im Deutschen und .\n\nDie Schriftsysteme für die meisten europäischen Sprachen nutzen Varianten des lateinischen Alphabets. Dabei wurden den Zeichen für lateinische Laute ähnliche Laute der jeweiligen Sprache zugeordnet. Dieselben Zeichen standen in den verschiedenen Sprachen für teilweise unterschiedliche Laute. Zudem ist es im Zuge der Sprachentwicklung zu weiteren Veränderungen der Aussprache gekommen (vgl. im Deutschen und Englischen).\n\nDa die Zahl und Art der Phoneme in den verschiedenen Sprachen unterschiedlich ist, genügte der Zeichenvorrat des lateinischen Alphabetes oft nicht. Deshalb wurden zur Darstellung der betreffenden Phoneme Buchstabenkombinationen (z.&nbsp;B. , , ) und diakritische Zeichen eingeführt (z.&nbsp;B. auf , ).\n\nDaneben wurden Varianten der ursprünglichen lateinischen Zeichen ( > , > ) und Ligaturen ( > , / > , / > ) zu eigenständigen Zeichen weiterentwickelt und gelegentlich auch Buchstaben aus anderen Alphabeten übernommen ().\n\n\nLautschrift.\nEin absolut \"phonetisches\" Alphabet wäre in der Praxis unbrauchbar, weil es aufgrund der mannigfaltigen Nuancen einer Sprache sehr viele Zeichen hätte. Ein in Bezug auf die phonetische Wiedergabe optimiertes Alphabet ist das IPA, welches möglichst vielen Lautnuancen ein grafisches Zeichen zuordnet.\n\nEine \"phonemische\" Schreibweise behandelt unterschiedliche Aussprachen desselben Phonems gleich. So wird beispielsweise in der deutschen Orthografie die regional unterschiedliche (phonetische) Aussprache des Phonems in als norddeutsch und hochdeutsch nicht berücksichtigt. Daneben sorgen morphemische Schreibungen für ein konstanteres Schriftbild bei der Flexion, z.&nbsp;B. schreibt man wegen des Plurals nicht *, sondern , und bei der Derivation, z.&nbsp;B. statt .\n\n\nBuchstabieren.\nWenn Menschen einander mündlich die korrekte Schreibweise eines Wortes mitteilen, indem sie nacheinander alle Buchstaben jenes Wortes nennen, so bezeichnet man diesen Vorgang als Buchstabieren (Verb: \"buchstabieren\"). Dabei werden Konsonantenbuchstaben meist mit Hilfe von zusätzlichen Vokalen ausgesprochen, im Deutschen zum Beispiel [beː] für B oder [kaː] für K (siehe Benennung der Buchstaben). Um Missverständnisse auszuschließen, können auch festgelegte Namen oder Wörter ausgesprochen werden, die mit dem betreffenden Buchstaben beginnen, zum Beispiel „Anton“ für A oder „Berta“ für B (siehe Buchstabiertafel).\n\n\nEntstehung und Entwicklung.\nAus den in Vorderasien gebräuchlichen Keilschriften entwickelten Händler in Ugarit um 1400 v.&nbsp;Chr. die erste alphabetische Schrift, die sogenannte ugaritische Schrift. Aus dieser Schrift hat sich um 1000 v.&nbsp;Chr. unter anderem das phönizische Alphabet entwickelt, das wiederum Ausgangspunkt für die heute gebräuchlichen Alphabete war. Die Phönizier verwendeten dabei Elemente vorhandener Bilderschriften. Sie lösten die Zeichen vollständig von ihrer bildlichen Bedeutung und wiesen ihnen Lautwerte zu. Die phönizische Schrift verlief von rechts nach links. Trotz der großen Unterschiede in der Gestalt der Zeichen lassen sich die Buchstaben der Phönizier mit den Keilschrift-Zeichen der ugaritischen Schrift in Verbindung bringen.\n\nDie phönizische Schrift war eine reine Konsonantenschrift. Dies entsprach der Struktur der semitischen Sprachen. Die hebräische und die arabische Schrift, die daraus entstanden, verzichten bis heute (weitgehend) auf Vokale. Als die Griechen etwa im 10. oder 9. Jahrhundert v.&nbsp;Chr. die phönizische Schrift übernahmen, benutzten sie Zeichen für bestimmte semitische Konsonanten, die in ihrer Sprache nicht vorkamen, zur Bezeichnung von Vokalen, z.&nbsp;B. wurde aus dem Zeichen H für einen rauen Hauchlaut im griechischen Alphabet ein Zeichen für einen Vokal (siehe Buchstabe Eta). Einige Zeichen für Konsonanten, die die phönizische Sprache nicht kannte, wurden neu geschaffen, z.&nbsp;B. das Psi. Im Jahre 403 v.&nbsp;Chr. wurde in Athen das Alphabet normiert. Es wurde so zum Schriftsystem für ganz Griechenland.\n\nAnfang des 4. Jahrhunderts v.&nbsp;Chr. brachten griechische Siedler das Alphabet nach Italien, wo die Etrusker (in der heutigen Toskana) es im Laufe des 4. Jahrhunderts übernahmen. Im 3. Jahrhundert v.&nbsp;Chr. orientierten sich die Römer an der griechisch-etruskischen Schrift und überlieferten sie im 1. Jahrhundert v.&nbsp;Chr. nach Mitteleuropa.\n\n\nHistorische Bedeutung.\nDurch das Alphabet entstand ein System mit vergleichsweise wenigen Zeichen. Um die Aufzeichnungen der alten Ägypter verstehen zu können, musste man Hunderte, später sogar Tausende Hieroglyphen lernen. Nun genügten zwei Dutzend Zeichen, um sämtliche Gedanken, die überhaupt formulierbar sind, zu notieren. Die Einfachheit dieses Systems begünstigte dessen Verbreitung über die halbe Welt.\n\n„Die menschlichen Sprechwerkzeuge können zwar eine riesige Zahl von Lauten erzeugen, doch beruhen fast alle Sprachen auf dem formalen Wiedererkennen von nur ungefähr vierzig dieser Laute durch die Mitglieder einer Gesellschaft.“ (Jack Goody).\n\nDie Reihenfolge des griechischen und lateinischen Alphabets folgt global (mit wenigen Ausnahmen) der Reihenfolge des phönizischen Alphabets, da die Zeichen auch mit einem Zahlwert gekoppelt waren.\n\n\nAlphabete im weiteren Sinn.\nDie Buchstaben (Schriftzeichen eines Alphabets) bestehen meist aus Linien und können beispielsweise auf Papier geschrieben werden. Das bestimmende Merkmal eines Buchstabens ist jedoch nicht die Form, sondern seine Funktion, einen Sprachlaut oder eine Lautverbindung zu repräsentieren. Deshalb spricht man im weiteren Sinn auch bei den folgenden Zeichensystemen von Alphabeten:\n\n\nDiese Zeichensysteme kodieren eigentlich Buchstaben – und nur indirekt Laute. Zudem enthalten sie auch Zeichen für Ziffern und teilweise weitere Zeichen (Satzzeichen, Steuerzeichen, Zeichen für Wörter).\n\nIn der Informatik werden die Begriffe \"Alphabet\" und \"Buchstabe\" in einem verallgemeinerten Sinn verwendet. Ein „Buchstabe“ kann hier auch eine Ziffer oder ein sonstiges Symbol sein – „Alphabete“ und „Wörter“ können solche beliebigen Symbole enthalten. Siehe hierzu Alphabet (Informatik) und formale Sprache.\n\n\n\n"}
{"id": "133", "url": "https://de.wikipedia.org/wiki?curid=133", "title": "Arthur Harris", "text": "Arthur Harris\n\nSir Arthur Travers Harris, 1. Baronet GCB OBE AFC, genannt \"Bomber-Harris\", (* 13. April 1892 in Cheltenham; † 5. April 1984 in Goring-on-Thames) war ein hochrangiger Offizier der Royal Air Force, zuletzt im Rang eines Marshal of the Royal Air Force. Während des Zweiten Weltkriegs war er ab Februar 1942 Oberbefehlshaber des RAF Bomber Command und gehört wegen der von ihm angeordneten Flächenbombardements deutscher Städte zu den umstrittensten Personen des Luftkriegs im Zweiten Weltkrieg.\n\n\nLeben.\nArthur Travers Harris wurde in Cheltenham während eines Urlaubs seiner Eltern geboren. Sein Vater war Angehöriger der britischen Beamtenschaft in Indien (Indian Civil Service (ICS)). Nach der Schulzeit in Dorset stand laut Biographie von Norman Longmate für Harris im Alter von 16 Jahren eine Entscheidung zwischen der Armee und den Kolonien an. Harris entschied sich 1908 für letztere. In Rhodesien war er nach eigener Auskunft mit Goldwaschen, Kutschfahrten und Landwirtschaft beschäftigt.\n\n\nErster Weltkrieg.\n1914 trat er als Trompeter in das 1st Rhodesian Regiment der Britischen Armee ein. Er diente für die Südafrikanische Union im Krieg in Deutsch-Südwestafrika (heute Namibia), bevor er 1915 nach England zurückkehrte und in das neu aufgestellte Royal Flying Corps eintrat. Harris war in Frankreich und England im Einsatz und errang auf den Doppeldeckern Sopwith 1½ Strutter und Sopwith Camel fünf Luftsiege, worauf ihm das Air Force Cross (AFC) verliehen wurde. Bei Kriegsende hatte er den Rang eines Majors.\n\n\nZwischenkriegszeit.\n1919 verblieb er bei der neu gegründeten Royal Air Force und wurde im April 1920 Staffelführer und Kommandant des Fliegerhorstes Digby und der No. 3 Flying Training School. Später diente er unter anderem in Britisch-Indien, in Mesopotamien und in Persien. In Mesopotamien kommandierte er ein Transportgeschwader Vickers Vernon.\n\n1922 war Harris Führer einer Lufttransportstaffel im Irak, wo die Briten versuchten, den Widerstand der Einheimischen durch Terrorangriffe auf Städte und Dörfer zu brechen, bevor der Einsatz von Infanterie notwendig wurde. Harris hatte die Idee, alle Transportflugzeuge zusätzlich mit Bombenträgern auszustatten, „kam es doch nicht auf Präzisionsangriffe an, sondern auf eine möglichst flächendeckende Terrorisierung der Bevölkerung.“\n\nVon 1930 an war Harris im Luftstab für den Nahen Osten tätig, wo er an der Niederschlagung verschiedener Aufstände der dortigen Bevölkerung gegen die britische Kolonialherrschaft beteiligt war. Er begründete dies damit, dass seiner Ansicht nach \"die Araber\" nur eine „Politik der harten Hand“ verstünden („The only thing the Arab understands is the heavy hand“).\n\n\nZweiter Weltkrieg.\nAm 14. Februar 1942 erfolgte die Anweisung „Area Bombing Directive“ des britischen Luftfahrtministeriums.\nHarris wurde im Februar 1942 zum Oberkommandierenden des Bomber Command der RAF ernannt. Basierend auf Vorlagen von Frederick Lindemann, einem engen Berater Churchills, von dem die Wortschöpfung \"Flächenbombardements\" (Carpet Bombing) stammt, war Harris der Ansicht, allein durch Flächenbombardierungen der Städte das Deutsche Reich zur Kapitulation zwingen zu können.\n\nHarris unterstützte maßgeblich die Entwicklung eines geplanten Feuersturms (Zitat A. Harris bei den Planungen des Luftangriffs auf Lübeck am 29. März 1942: „Historischer Stadtkern brennt gut“). In der ersten Welle wurden neben Spreng- und Brandbomben vor allem große Luftminen (Blockbuster – „Wohnblockknacker“) abgeworfen, die die Dächer abdeckten und Fenster zerstörten, um den Kamineffekt zu verstärken. In einer zweiten Welle wurden Brandbomben abgeworfen, die in kürzester Zeit einen Flächenbrand entstehen ließen. Dies gelang jedoch aufgrund meteorologischer und städtebaulicher Faktoren nicht immer.\n\nUm die deutsche Flugabwehr und die nach dem sog. „Himmelbett-Verfahren“ arbeitende Nachtjagd, z. B. entlang der Kammhuber-Linie durch lokale Überlastung zu überrumpeln, entwickelte er die Methode der Bomberströme, bei der möglichst viele Bomber auf demselben Kurs einfliegend in kurzer Zeit ein Ziel angriffen, statt einzeln und in breiter Front einzufliegen. Zur Demonstration der Wirksamkeit seines Konzeptes zog Harris im Frühjahr 1942 für die Operation Millennium alle verfügbaren Bomber zusammen, um Ende Mai mit 1047 Maschinen auf Köln den ersten „Tausend-Bomber-Angriff“ durchzuführen. Dieser Angriff war entscheidend, um die zahllosen britischen Skeptiker von der Wirksamkeit von Luftangriffen zu überzeugen und die betriebene Auflösung des Bomber Command zu verhindern.\n\nDie technischen Voraussetzungen für präzise Schläge gegen strategische Punkte wie Fabriken für Flugzeuge, Panzer und anderes Rüstungsmaterial befanden sich in der Mitte des Krieges noch in der Entwicklung. Die schweren Verluste der 8th Air Force bei ihren Angriffen 1943 und Anfang 1944 bestätigten sein Festhalten am Nachtangriff vorerst bis zum Einsatz von neuen amerikanischen Langstreckenbegleitjägern, wobei die Nachtangriffe der RAF durch die Schaffung der 24-Stunden-Bedrohung auch für den Erfolg der amerikanischen Tagesangriffe auf strategische Punktziele weiterhin bedeutend blieben.\n\nUnter seiner Führung wurden von der RAF zahlreiche deutsche Städte schwer zerstört, so bei der Operation Gomorrha gegen Hamburg im Juli/August 1943, Kassel (22. Oktober 1943), Leipzig (4. Dezember 1943), Frankfurt am Main (22. März 1944), Darmstadt (11. September 1944), Braunschweig (15. Oktober 1944), Nürnberg (2. Januar 1945), Magdeburg (16. Januar 1945), Dresden am 13./14. Februar 1945 sowie Pforzheim (23. Februar 1945), Mainz (27. Februar 1945), Würzburg (16. März 1945), Hanau (19. März 1945), Hildesheim (22. März 1945) und Nordhausen (3./4. April 1945).\n\nBei den Flächenbombardements wurden – neben den im Stadtgebiet befindlichen Industrieanlagen – die Zivilbevölkerung und die Infrastruktur der Stadt primäres Ziel der Angriffe. Seiner Meinung nach sollten ganz bewusst zivile Ziele angegriffen werden, um die Moral und den Widerstandswillen der deutschen Bevölkerung zu brechen (sogenanntes \"morale bombing\"). Zu Beginn der Bombardierungen äußerte sich Harris zu seiner Motivation: „\"Die Nazis starteten (‚entered‘) den Krieg mit der ziemlich kindischen Vorstellung, dass sie jeden anderen nach Belieben bombardieren könnten und niemand würde zurückbomben. In Rotterdam, London, Warschau und an beinahe einem halben Hundert anderer Stätten führten sie ihre ziemlich naive Theorie aus. Sie säten Wind und jetzt ernten sie Sturm“\". In seinen Memoiren schrieb er später: \"„Trotz all dem, was in Hamburg geschehen ist, bleibt das Bomben eine relativ humane Methode“\".\n\nNeben den Bombenangriffen auf Deutschland wurden insbesondere in Italien mehrere Großstädte bombardiert, was etwa in Mailand, Neapel und Palermo beträchtliche Schäden auch in Wohngebieten verursachte.\n\n\nNach dem Krieg.\nAm 15. September 1945 schied Harris im Streit mit dem neuen Premierminister Clement Attlee aus der Royal Air Force aus und zog sich verbittert nach Südafrika zurück. Seine Ehrungen durch die Ernennung zum erblichen Baronet of Chipping Wycombe in the County of Buckingham am 24. Januar 1953 (eine Erhebung zum Peer hatte er abgelehnt) sowie die Enthüllung eines von Veteranen finanzierten Denkmals 1992 vor der Kirche St Clement Danes in London durch die Königinmutter Elizabeth waren in der britischen Bevölkerung stark umstritten. Innerhalb von 24 Stunden wurde das Denkmal mit roter Farbe überschüttet und später noch mehrfach beschädigt, woraufhin es für mehrere Monate unter Bewachung stand. In diesem Zusammenhang ist auch von Bedeutung, dass seine Luftkriegs-Strategie für die Besatzungen der Flugzeuge verlustreich war. Nahezu 45 % kehrten nicht heim, insgesamt kamen 55.573 Flieger bei den Angriffen auf Deutschland um. Auch deswegen wurde Harris oft „Butcher“ (engl. für Metzger oder Schlächter) genannt.\n\n\nMilitärhistorische Wertung.\nDie historische wie rechtliche Qualifizierung der alliierten Luftkriegsstrategie und damit der Position Harris’ wird unterschiedlich bewertet. Nach sachlichen oder militärischen Kriterien war die gezielte Zerstörung von Wohngebieten und Innenstädten umstritten. Zwar waren militärisch gesehen die strategischen Folgen des Luftkriegs allgemein erheblich, da angesichts der Angriffe die deutsche Rüstungsproduktion zu umfangreichen produktionsbehindernden Verlagerungen gezwungen wurde – laut Albert Speer führten die alliierten Luftangriffe bei den Luftfahrzeugen zu einer Halbierung der möglichen Produktion. Über eine Million Soldaten wurden bei der Flakartillerie eingesetzt und fehlten dadurch an den Fronten, zusätzlich wurde eine halbe Million Behelfspersonal herangezogen, darunter viele Jugendliche als Flakhelfer.\n\nAll dies war aber in erster Linie auf die gegen die Rüstungsindustrie geführten Tagangriffe der USAAF und nicht auf die gegen die Zivilbevölkerung gerichteten und von Arthur Harris verantworteten Nachtangriffe der Royal Air Force zurückzuführen. Die bekanntesten Einsätze des Bomber Command außerhalb von Harris’ Strategie waren: Die Angriffe auf die Talsperren (Operation Chastise), die Versenkung des Schlachtschiffs \"Tirpitz\" (November 1944), die Bombardierung von U-Boot-Bunkern der Kriegsmarine an der französischen Atlantikküste und die Zerstörung von Anlagen des deutschen V-Waffen-Programms (Operation Hydra, Éperlecques, Mimoyecques) sowie die direkte taktische Unterstützung während der Landung alliierter Truppen in der Normandie (Operation Overlord).\n\nHarris hat seinen Standpunkt insbesondere in seinem Buch \"Bomber Offensive\" dargestellt, das seinen Lebensweg beschreibt. Er argumentiert, das nationalsozialistische Deutschland habe damit begonnen, die Zivilbevölkerung zum Objekt von Terrorangriffen zu machen (Guernica, Coventry, Rotterdam, Warschau, London). Aufgabe der britischen Verantwortlichen sei es gewesen, für ein schnelleres Ende des Krieges zu sorgen und eigene Opfer zu vermeiden, die etwa ein Landkrieg oder Stellungskrieg wie im Ersten Weltkrieg mit sich gebracht hätte. Vor dem Eintritt der Vereinigten Staaten in den Krieg (Dezember 1941) beziehungsweise vor dem D-Day, der alliierten Landung in der Normandie am 6.&nbsp;Juni 1944, hätte angesichts der Insellage Großbritanniens einzig die Offensivstrategie des Bomber Commands der Royal Air Force die Sicherheit des Vereinigten Königreichs garantieren können.\n\nDes Weiteren unterstreicht Harris die Bedeutung der Luftunterstützung für einen erfolgreichen Einsatz von Landtruppen. Er verweist zum Vergleich auf die deutsche Blitzkriegstrategie zu Beginn des Krieges, bei der das schnelle Vordringen des Heeres, insbesondere der Panzer, nur aufgrund massiver und rasch abrufbarer Luftunterstützung (Bomber und Jagdflieger) möglich gewesen sei. Die Tatsache, dass die deutsche Luftwaffe gegen Ende des Krieges zum großen Teil zerstört oder durch die Verteidigung des eigenen Territoriums gegen die alliierten Bomber gebunden waren, habe dazu geführt, dass dem deutschen Heer die notwendige Unterstützung durch die Luftwaffe fehlte. Die alliierte Luftüberlegenheit habe den britischen und US-amerikanischen Truppen sowie der Roten Armee entscheidend geholfen, die Deutschen zurückzudrängen.\n\n\nSonstiges.\nIn dem 1954 gedrehten britischen Spielfilm \"Mai 1943 – Die Zerstörung der Talsperren\" \"(The Dam Busters)\" von Michael Anderson wird Arthur T. Harris von Basil Sydney dargestellt.\n\n\n\n\n"}
{"id": "134", "url": "https://de.wikipedia.org/wiki?curid=134", "title": "Arthur Wellesley, 1. Duke of Wellington", "text": "Arthur Wellesley, 1. Duke of Wellington\n\nArthur Wellesley, 1. Duke of Wellington (* vermutlich 1. Mai 1769 in Dublin, Irland; † 14. September 1852 in Walmer Castle bei Deal, Kent, England), war Feldmarschall und der herausragendste britische Militärführer der napoleonischen Zeit sowie britischer Außen- und zweimal Premierminister. Er siegte über Napoleon in der Schlacht bei Waterloo.\n\n\nLeben.\n\nHerkunft und Kindheit.\nWellesley stammte aus englisch-irischem Adel und war der dritte überlebende Sohn von Garret Wesley, 1. Earl of Mornington und Anne, der Tochter von Arthur Hill-Trevor, 1. Viscount Dungannon. Der Tag seiner Geburt ist nicht sicher bekannt. Vermutlich wurde er in \"Mornington House\", \"24 Upper Merrion Street\" in Dublin geboren. Sein älterer Bruder Richard Colley Wellesley (1760–1842) folgte dem Vater als Earl of Mornington und wurde 1799 zum Marquess Wellesley erhoben.\n\nAls Kind kränklich und wenig ehrgeizig, aber musikalisch begabt (er spielte gerne und oft Violine), stand er ganz im Schatten seiner beiden älteren Brüder. Nach dem Besuch des Eton College von 1781 bis 1785, wo er sich wenig hervortat, sandten ihn seine Eltern zunächst zum 73. Infanterie-Regiment der British Army, in das er am 7. März 1787 eintrat. Danach besuchte er die Militärakademie in Angers (Frankreich). \n\n\nMilitärische Karriere.\nIm Jahre 1788 wurde Wellesley zum Leutnant befördert. Nach einigen Zwischenstationen bei der Kavallerie und den 12. und 18. leichten Dragonern wurde er 1793 Oberstleutnant des \"33rd Regiment of Foot\", ein schneller Aufstieg, der durch das damals übliche Kaufsystem ermöglicht wurde. Während der ganzen Zeit war er Adjutant des Vizekönigs von Irland und nebenbei von 1790 bis 1797 Abgeordneter für den Wahlbezirk Trim im County Meath (seinem Familiensitz) im House of Commons des irischen Parlaments.\n\nSeine aktive militärische Karriere begann 1794, als er im Ersten Koalitionskrieg mit dem Duke of York and Albany nach Flandern ging und dort am erfolglosen Feldzug gegen die Franzosen teilnahm. Er kommandierte beim Rückzug die Nachhut.\n\n1796 wurde Wellesley zum Oberst befördert und ging mit seinem Regiment nach Britisch-Indien, wo im Jahr darauf sein älterer Bruder Richard Generalgouverneur werden sollte. Als 1799 der Vierte Mysore-Krieg gegen den Sultan von Mysore, Tipu Sultan, ausbrach, kommandierte er seine erste Division. Er führte 1803 einen sehr erfolgreichen Feldzug im Zweiten Marathenkrieg und konnte dabei seine militärischen Fähigkeiten erheblich ausbauen. Er wurde Oberbefehlshaber der britischen Streitkräfte in Indien und zwang ganz Südindien unter britische Herrschaft. Am 11. August 1803 nahm er die Festung Ahmednagar und besiegte eine überlegene Streitmacht der Marathen in der Schlacht von Assaye. In den nächsten Wochen gelang es seinen Truppen, Burhanpur und die Festung Asirgarh einzunehmen. Er stieß auf Hyderabad vor, siegte am 29. November in der Schlacht von Argaon und stürmte die Festung Gawilgarh. In Anerkennung seiner Leistungen wurde er 1804 als Knight Companion des Order of the Bath zum Ritter geschlagen und kehrte 1805 als \"Major General Sir Arthur Wellesley\" nach Großbritannien zurück, gemeinsam mit Richard, dessen Amtszeit als Generalgouverneur ebenfalls abgelaufen war.\n\n1807 nahm Wellesley als Lieutenant General an einer Expedition nach Dänemark teil. Anfang August 1808 landete er mit 13.000 Mann in Portugal und besiegte zwei Wochen darauf die französischen Truppen in der Schlacht bei Vimeiro. Wellesleys Vorgesetzte Burrard und Dalrymple, die erst nach dem Ende der Schlacht in Portugal eingetroffen waren, schlossen Ende August die Konvention von Cintra ab, die den Franzosen nicht nur den freien Abzug gewährte, sondern ihnen auch den Erhalt der Kriegsbeute und den Rücktransport auf britischen Schiffen garantierte. Diese Bedingungen, wurden von der britischen Öffentlichkeit als Sieg für Frankreich empfunden; Wellesley, Burrard und Dalrymple wurden nach Großbritannien zurückbefohlen und vor einem Militärgericht angehört. Wellesley wurde entlastet und im Gegensatz zu den beiden anderen Beteiligten rehabilitiert. Am 27. Oktober 1807 vereinbarten Frankreich und Spanien im Geheimen im Vertrag von Fontainebleau (1807) die Eroberung und Teilung Portugals. Spanien gewährte den französischen Truppen den Durchmarsch durch spanisches Hoheitsgebiet.\n\nIm Frühjahr 1809 versuchten die Franzosen ein zweites Mal, Portugal zu erobern. Wellesley kehrte nach Portugal zurück und übernahm den Oberbefehl der dortigen britisch-portugiesischen Truppen. Am 12. Mai 1809 schlug er Marschall Nicolas Soult in der Zweiten Schlacht bei Oporto. Durch den Sieg in der Schlacht bei Talavera de la Reina am 28. Juli beendete Wellesley die französischen Ambitionen. In Anerkennung seiner Leistungen wurde er am 4. September 1809 mit den erblichen Titeln Viscount Wellington und Baron Douro zum Peer erhoben.\n\nWellesley gab zum Schutze Portugals am 20. Oktober 1809 beim britischen Ingenieur Richard Fletcher die Befestigung der Linien von Torres Vedras in Auftrag, unter dessen Leitung sie von portugiesischen Arbeitern und Soldaten errichtet wurden. Der Vormarsch der Franzosen unter Marschall Masséna erhielt am 27. September 1810 in der Schlacht von Buçaco einen empfindlichen Rückschlag, gleichwohl blieb die Hauptstadt Lissabon weiterhin bedroht. Am 3. April 1811 scheiterte mit der Schlacht von Sabugal der letzte Versuch Frankreichs, Portugal zu erobern. Danach schwenkte die Stimmung in Spanien auf die britische Seite um; Wellesley wurde auch Oberkommandierender der spanischen Streitkräfte. Lord Beresford erhielt den Oberbefehl über die reorganisierten portugiesischen Streitkräfte. Nachdem bekannt geworden war, dass die französischen Truppen im Westen Spaniens reduziert wurden, marschierte Wellington nach Ciudad Rodrigo und nahm diese Stadt nach kurzer Belagerung am 19. Januar 1812 ein, wofür er durch den Prinzregenten Georg am 28. Februar 1812 zum Earl of Wellington erhoben wurde. Ab 27. März 1812 begannen die Verbündeten die dritte Belagerung von Badajoz, Wellington nahm die Stadt nach drei Wochen unter Verlust von etwa 4.000 Mann auf britischer Seite am 7. April ein. Die Eroberung erlaubte es den Briten, eigene Operationen im zentralen Spanien einzuleiten. Während ein britisches Korps unter General Hill zwischen den französischen Armeen Marmont und Soult Richtung Tajo vorrückte, wandte sich die britische Hauptmacht nach León. Am 21. Juli erwarteten die Franzosen den Gegner am Tormes und in Stellungen auf den Arapilen, und am 22. Juli schlug Wellington sie in der Schlacht von Salamanca. Wellington konnte infolge dieser Kämpfe am 12. August Madrid besetzen, wurde aber kurz darauf wieder aus der Stadt vertrieben und musste die geplante Belagerung von Burgos aufgeben. Am 4. Oktober 1812 wurde ihm der erbliche Titel Marquess of Wellington verliehen.\n\nNach der Niederlage Napoleons in Russland und dem Beginn der Kämpfe in Deutschland erhielten die französischen Truppen in Spanien keine Verstärkung mehr. Wellington verbrachte den Winter damit, seine Armee zu reorganisieren, und plante, die iberische Halbinsel im Frühjahr 1813 gänzlich freizukämpfen. Im Mai 1813 begann Wellingtons abschließende Offensive von Portugal aus zwischen Duero und Tajo, in der er zunächst die nördlichen Provinzen Spaniens eroberte und sein Hauptquartier von Lissabon nach Santander verlegte. Wellingtons Truppen marschierten durch das kastilische Hochland in Richtung Kantabrien, um die französische Hauptmacht durch Abschneidung der Verbindungswege zum Rückzug aus Zentralspanien zu zwingen. Er griff die französische Hauptmacht unter Joseph Bonaparte am 21. Juni 1813 in der entscheidenden Schlacht von Vitoria mit drei Kolonnen an. Die Schlacht beendete Napoleons Herrschaft in Spanien. Am 7. Juli begann Wellington die Belagerung von San Sebastian. Im Herbst 1813 rang er, inzwischen zum Feldmarschall befördert, mit den Truppen des neuen französischen Oberbefehlshabers Soult auf breiter Front um die Übergänge in den Pyrenäen. In Südfrankreich eindringend, lieferte er sich mit Soult noch am 10. April 1814 die blutige Schlacht bei Toulouse, dann folgte mit der Abdankung Napoleons das Kriegsende.\n\nAm 11. Mai 1814 verlieh der Prinzregent stellvertretend für seinen Vater Georg III. ihm die erblichen Adelstitel Duke of Wellington und Marquess Douro.\n\nDer Duke nahm im Frühjahr 1815 unter Lord Castlereagh auch an mehreren Sitzungen des Wiener Kongress teil. Im Februar wurde Wellington nach dessen Abberufung nach England, Hauptbevollmächtigter in Wien, bevor er März 1815 nach der Rückkehr Napoleons aus Elba den Oberbefehl im neuen Krieg gegen Frankreich erhielt. Im Raum Brüssel sammelte Wellington das verbündete Heer gegen Napoleon, darunter nur etwa 35.000 Briten und wartete die geplante Vereinigung mit den Preußen ab. Am 18. Juni in der Schlacht von Waterloo (auch „Schlacht von Belle-Alliance“) von Napoleon angegriffen, hielten Wellingtons Truppen den französischen Angriffen solange erfolgreich stand, bis die Ankunft der Preußen den Sieg der Alliierten entschied. Das bekannte Zitat „Ich wollte, es wäre Nacht, oder die Preußen kämen“ wird Wellesley beim Warten auf die Ankunft Blüchers zugeschrieben, ist aber nicht verbürgt. Die Schlacht ging mit der Hilfe Blüchers zu Wellesleys Gunsten aus, Napoleon zog sich geschlagen zurück, für ihn bedeutete diese Schlacht das Ende seiner militärischen Karriere. Wellington hingegen wurde von den Briten als Held gefeiert, unter den Militärstrategen galt er fortan als Meister der Defensive.\n\n1827/28 und noch einmal von 1842 bis zu seinem Tod war Wellesley Oberbefehlshaber der britischen Armee. Ab 1829 hatte er auch das Amt des Lord Warden of the Cinque Ports inne. Nach seinem Tod wurde er am 18. November 1852 in einem Staatsbegräbnis in der Krypta der St Paul’s Cathedral beigesetzt. Im Hauptschiff der Kathedrale wurde ihm ein monumentales Grabdenkmal gesetzt.\n\n\nPolitisches Leben.\nIm Jahr 1806 zog er als Abgeordneter für den Wahlbezirk Rye in Sussex ins britische House of Commons ein. Im Parlament gehörte er den Torys an. 1807 wurde er Chief Secretary for Ireland, dieses Amt gab er jedoch noch im gleichen Jahr zugunsten seiner militärischen Karriere wieder auf. 1807 war er jeweils kurzzeitig Abgeordneter für die Wahlbezirke Tralee im irischen County Kerry und Mitchell in Cornwall und war anschließend 1807 bis 1809 Abgeordneter für den Wahlbezirk Newport auf der Isle of Wight. Als er 1809 zum Peer erhoben wurde, wurde er damit auch auf Lebenszeit Mitglied des britischen House of Lords und schied dafür aus dem House of Commons aus.\n\nNach dem Wiener Kongress wandte er sich wieder einer politischen Laufbahn zu und erhielt 1818 ein Amt in der Tory-Regierung unter Lord Liverpool. Am 17. August 1827 wurde er Oberkommandierender der britischen Armee, doch übernahm er 1828 nach dem Tod von Canning und dem Sturz von Lord Goderich widerstrebend das Amt des Premierministers. Er führte eine erzkonservative Regierung und eine isolationistische Politik. So beendete er trotz des Siegs in der Schlacht von Navarino die Unterstützung des griechischen Freiheitskampfes. Infolge dieser Politik lehnte Prinz Leopold die ihm angebotene griechische Krone ab. Innenpolitisch setzte Wellington gegen große innerparteiliche Widerstände 1829 das Wahlrecht für Katholiken durch. Gleichzeitig versuchte er eine weitere Wahlrechtsreform zu verhindern, weswegen er bei weiten Teilen der Bevölkerung höchst unpopulär wurde. Die Verzögerung der Wahlrechtsreform und seine Unbeliebtheit weiteten sich zu Unruhen aus. Dennoch erklärte er in völliger Verkennung der Lage bei der Parlamentseröffnung nach dem Tod Georgs IV., dass er eine Wahlrechtsreform weiter ablehne. Diese Erklärung führte zum Sturz seiner Regierung am 22. November 1830. Sein Nachfolger als Premierminister Earl Grey nahm sofort eine Wahlrechtsreform in Angriff und brachte am 1. März 1831 gegen den Widerstand Wellingtons den Reform Act ins Unterhaus ein. Nachdem das Gesetz das House of Commons passiert hatte, verweigerte das House of Lords am 8. Oktober 1831 seine Zustimmung. Trotz der Gefahr eines drohenden revolutionären Umsturzes blockierte das Oberhaus das Gesetz weiter. Am 9. Mai 1832 wurde Wellington erneut zum Regierungschef ernannt. Wilhelm IV. bewog ihn, ein gemäßigtes Kabinett zu bilden. Da zahlreiche Sparer aus Protest ihre Einlagen aus der Bank of England abzogen, drohte eine Finanzkrise, so dass Wellington schon am 15. Mai wieder aufgab. Sein Nachfolger wurde wieder Earl Grey, unter dem am 4. Juni 1832 die Wahlrechtsreform vom Oberhaus verabschiedet wurde.\n\nDie nächsten beiden Jahre verbrachte Wellington in der Opposition. Bei der Trauerfeier für Lord Althorp im Oberhaus entließ Wilhelm IV. unerwartet das Whig-Kabinett und beauftragte Wellington am 17. November 1834 mit der Bildung einer Minderheitsregierung. Dieser schlug schließlich Robert Peel, seinen langjährigen politischen Weggefährten, als Premierminister vor, während er das Amt des Außenministers übernahm. Dies war die letzte britische Regierung, die ein Monarch ohne Mehrheit im Unterhaus ernannte und sie scheiterte schon im April 1835. Peel wurde im September 1841 erneut zum Premierminister ernannt, und Wellington wurde in dieser Regierung als Oberkommandierender der Armee Minister ohne Geschäftsbereich sowie Leader des House of Lords. Als Peel 1846 zurücktrat, legte auch Wellington am 27. Juni sein Amt als Führer der Mehrheitsfraktion nieder und zog sich aus der Öffentlichkeit zurück. Das Amt des Oberbefehlshabers der Armee behielt er allerdings bis zu seinem Tod.\n\n\nPrivatleben.\nAm 10. April 1806 heiratete er Kitty Pakenham, die Tochter des 2. Baron Longford. Aus der Ehe gingen folgende Kinder hervor:\n\nKittys Bruder Ned war in Spanien einer von Wellingtons wichtigsten Generälen. Wellington war seit dem 7. Dezember 1790 ein Mitglied im Bund der Freimaurer (\"Trim No. 494\") und gehörte dem renommierten Londoner Travellers Club an.\n\n\nEhrungen und Auszeichnungen.\nEr war seit 1804 Knight Companion des Order of the Bath und wurde bei der Reform der Ordensstatuten 1815 zum Knight Grand Cross dieses Ordens erhoben. 1813 wurde er als Knight Companion in den Hosenbandorden aufgenommen und 1816 zum Großkreuzritter des Guelphen-Ordens geschlagen. 1807 wurde er ins Privy Council aufgenommen und 1847 zum Fellow der Royal Society ernannt.\n\nFür seine Verdienste wurden ihm auch von verbündeten ausländischen Monarchen Adelstitel verliehen, so wurde er in Portugal 1811 zum \"Conde do Vimeiro\" und 1812 zum \"Duque de Vitória\" und \"Marquês de Torres Vedras\", in Spanien 1812 zum \"Duque de Ciudad Rodrigo\" und in den Niederlanden 1815 zum \"Prins van Waterloo\" erhoben.\n\nDie HMS Duke of Wellington, ein 131-Kanonen-Schiff der britischen Royal Navy, wurde 1852 nach ihm benannt. Sie war das Flaggschiff von Sir Charles Napier, damals Konteradmiral der Blauen Flagge. Auch die HMS Iron Duke, das Flaggschiff der Grand Fleet im Ersten Weltkrieg war nach ihm benannt.\n\n\n\n\n"}
{"id": "135", "url": "https://de.wikipedia.org/wiki?curid=135", "title": "Astronomie", "text": "Astronomie\n\nDie Astronomie (griechisch für „Sternenkunde“, von \"ástron\" „Stern“ und \"nómos\" „Gesetz“) oder Sternkunde ist die Wissenschaft von den Gestirnen. Sie erforscht mit naturwissenschaftlichen Mitteln die Positionen, Bewegungen und Eigenschaften der Objekte im Universum, also der Himmelskörper (Planeten, Monde, Asteroiden, Sterne einschließlich der Sonne, Sternhaufen, Galaxien und Galaxienhaufen), der interstellaren Materie und der im Weltall auftretenden Strahlung. Darüber hinaus strebt sie nach einem Verständnis des Universums als Ganzes, seiner Entstehung und seines Aufbaus.\n\nObwohl die Astronomie nur an wenigen Schulen ein Unterrichtsfach ist, finden ihre Forschungsergebnisse in der Öffentlichkeit viel Interesse; als Amateurastronomie ist sie ein weit verbreitetes Hobby. Dies hängt einerseits mit dem „erhebenden“ Eindruck zusammen, den der Sternhimmel auch bei freisichtiger Beobachtung macht, andererseits mit ihrer thematischen Vielfalt, der Berührung philosophischer Fragen und der Verbindung zur Raumfahrt.\n\nIm Gegensatz zu früheren Zeiten wird die Astronomie als Naturwissenschaft heute streng abgegrenzt von der Astrologie, die aus Stellung und Lauf der Gestirne auf irdische Geschehnisse schließen will. Die Abgrenzung erfolgt auch, da die Astrologie eine Pseudowissenschaft ist – während die Astronomie auf empirischer Basis die Beschaffenheit, Bewegungen und Beziehungen von Himmelskörpern untersucht. Dennoch werden, wohl wegen der Ähnlichkeit beider Bezeichnungen, Astrologie und Astronomie von Laien nicht selten verwechselt.\n\nAn den Universitäten wurde die Astronomie um etwa 1800 zu einer eigenen Studienrichtung, wird aber heute zunehmend dem Physik-Studium zugeordnet. In der deutschen Hochschulpolitik wird sie gemeinsam mit der Astrophysik als Kleines Fach eingestuft.\n\n\nGeschichte der Astronomie.\nDie Astronomie gilt als eine der ältesten Wissenschaften. Ihre Anfänge liegen im Nachdenken über die Himmelserscheinungen, in der kultischen Verehrung der Gestirne und im Erarbeiten von Kalender bzw. Zeitbestimmung. In einem jahrtausendelangen Prozess – besonders gut erkennbar in der Himmelskunde Mesopotamiens und Griechenlands – trennten sich zunächst Astronomie und („Natur“)-Religion, später Astronomie und Meteorologie, in der Frühmoderne dann Astronomie und Astrologie. Wesentliche Meilensteine für unser Wissen über das Weltall waren die Erfindung des Fernrohrs vor etwa 400 Jahren, das die kopernikanische Wende vollendete, sowie später im 19. Jahrhundert die Einführung der Fotografie und Spektroskopie.\n\nSeit den 1960er-Jahren haben Astronomen mit der unbemannten und bemannten Raumfahrt die Möglichkeit, die Erdatmosphäre zu überwinden und ohne ihre Einschränkungen zu beobachten – also ohne Luftunruhe und in allen Bereichen des elektromagnetischen Spektrums. Dazu kommt erstmals die Möglichkeit, die untersuchten Objekte direkt zu besuchen und dort andere als nur rein beobachtende Messungen durchzuführen. Parallel dazu werden immer größere Teleskope für bodengebundene Beobachtungen gebaut.\n\n\nFachgebiete der Astronomie.\nDie astronomische Wissenschaft unterteilt sich allgemein nach den untersuchten Objekten, sowie danach, ob die Forschung theoretischer oder beobachtender Natur ist. Wichtige grundlegende Fachgebiete sind die \"beobachtende Astronomie\", die \"Astrophysik\", die \"Astrometrie\" und die \"Himmelsmechanik\", während die \"theoretische Astronomie\" analytische und numerisch-physikalische Modelle der Himmelskörper und Phänomene entwickelt.\n\nDie wichtigsten Untersuchungsgebiete der Himmelskunde sind die Physik des Sonnensystems, insbesondere die \"Planetologie\", die \"Galaktische Astronomie\", die die Milchstraße und ihr Zentrum erforscht, die \"Extragalaktische Astronomie\", die den Aufbau anderer Galaxien und ihrer aktiven Kerne, oder Gammablitze als die energiereichsten Vorgänge im Universum untersucht, sowie die \"relativistische Astrophysik\", die sich etwa mit Schwarzen Löchern beschäftigt. Die \"Stellarastronomie\" untersucht Geburt, Entwicklung und Tod der Sterne.\nDie \"Kosmologie\" hat die Geschichte und die Entstehung des Universums zum Gegenstand, während die \"Kosmogonie\" die Geschichte unseres eigenen Sonnensystems beschreibt. Sie erlebt derzeit eine Erweiterung durch das neueste Fachgebiet \"Exoplanetologie\".\n\nDie Integration vieler Messmethoden bringt es mit sich, dass man die \"Beobachtende Astronomie\" immer weniger nach benutzten Wellenlängenbereichen (\"Radioastronomie\", \"Infrarotastronomie\", \"Visuelle Astronomie\", \"Ultraviolettastronomie\", \"Röntgenastronomie\" und \"Gammaastronomie\") einteilt, weil die Forschergruppen und (im Idealfall) auch der einzelne Wissenschaftler Informationen aus allen diesen Quellen heranziehen kann.\n\nDie bis etwa 1900 vorherrschenden Methoden der \"klassischen Astronomie\" sind weiterhin als Basis für andere Teilgebiete unentbehrlich. Sie erforschen als \"Positionsastronomie\" mittels astrometrischer Verfahren, der Himmelsmechanik und \"Stellarstatistik\" den Aufbau des Weltalls und katalogisieren die Himmelskörper (v.&nbsp;a. durch Sternkataloge, Bahnbestimmungen und Ephemeriden). Im Gegensatz zu diesen überwiegend geometrischen Verfahren erforscht die \"Astrophysik\" mit ihren heute sehr vielfältigen Beobachtungstechniken die Physik der astronomischen Objekte und des ferneren Weltalls. Daneben kann die \"Raumfahrt\" als \"experimentelle Astronomie\" angesehen werden, und die Kosmologie als theoretische Disziplin.\n\n\nAstronomie und andere Wissenschaften.\nMit der Astronomie sehr eng verbunden sind die Physik und die Mathematik; die Fachgebiete haben sich vielfach befruchtet und sind auch im Astronomie-Studium als Einheit zu sehen. Das Universum erweist sich in vielen Fällen als Laboratorium der Physik, viele ihrer Theorien können nur in seinen Weiten und an heißen, energiereichen Objekten getestet werden. Nicht zuletzt waren die aufwändigen Berechnungen der Astronomie Triebfeder der modernen numerischen Mathematik und der Datenverarbeitung.\n\nTraditionell ist die Zusammenarbeit der Astronomie mit der Geodäsie (\"Astrogeodäsie\", Orts- und Zeitbestimmung, Bezugsysteme, Navigation), mit der Zeit- und Kalenderrechnung (\"Astronomische Chronologie\") sowie mit der Optik (Entwicklung \"astronomischer Instrumente und Sensoren\"). Instrumentell und methodisch sind auch starke Bezüge zur Technik, Raumfahrt und Mathematik gegeben (Messgeräte, Satellitentechnik, Modellierung von Bahnen und Himmelskörpern). Geodätische Methoden werden auch zur Bestimmung des Gravitationsfeldes sowie der Figur anderer Himmelskörper angewandt.\n\nIn den letzten Jahrzehnten ist auch die Zusammenarbeit der Astronomie mit der modernen Geologie und der Geophysik immer wichtiger geworden, da sich das Arbeitsgebiet der Geowissenschaften mit Teilen der \"Planetologie\" deckt. Die Mineralogie analysiert die Gesteine der Erde mit ähnlichen Methoden wie jene anderer Himmelskörper. Die \"Kosmochemie\" als Teil der Chemie untersucht die Entstehung und Verteilung der chemischen Elemente und Verbindungen im Universum und die chemische Evolution, die \"Astrobiologie\" die Umstände von Entstehung, Ursprung und Existenz von Leben im Universum.\n\nDes Weiteren kommt es zunehmend zu interdisziplinärer Forschung mit ursprünglich eher geisteswissenschaftlich ausgerichteten Disziplinen der Wissenschaft:\n\n\n\n\nLiteratur.\n\nPeriodika.\n\"Siehe auch: Abschnitt Literatur unter Amateurastronomie\"\n\n\n"}
